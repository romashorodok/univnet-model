{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_df = pd.read_csv(\n",
    "    \"./datasets_cache/LIBRITTS/LibriTTS/speakers.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"READER\", \"GENDER\", \"SUBSET\", \"NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Literal, Tuple, Union\n",
    "\n",
    "PreprocessLangType = Literal[\"english_only\", \"multilingual\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class STFTConfig:\n",
    "    filter_length: int\n",
    "    hop_length: int\n",
    "    win_length: int\n",
    "    n_mel_channels: int\n",
    "    mel_fmin: int\n",
    "    mel_fmax: int\n",
    "\n",
    "\n",
    "# Base class used with the Univnet vocoder\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    language: PreprocessLangType\n",
    "    stft: STFTConfig\n",
    "    sampling_rate: int = 22050\n",
    "    min_seconds: float = 0.5\n",
    "    max_seconds: float = 6.0\n",
    "    use_audio_normalization: bool = True\n",
    "    workers: int = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfigUnivNet(PreprocessingConfig):\n",
    "    stft: STFTConfig = field(\n",
    "        default_factory=lambda: STFTConfig(\n",
    "            filter_length=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "            n_mel_channels=100,  # univnet\n",
    "            mel_fmin=20,\n",
    "            mel_fmax=11025,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LangItem:\n",
    "    r\"\"\"A class for storing language information.\"\"\"\n",
    "\n",
    "    phonemizer: str\n",
    "    phonemizer_espeak: str\n",
    "    nemo: str\n",
    "    processing_lang_type: PreprocessLangType\n",
    "    \n",
    "langs_map: dict[str, LangItem] = {\n",
    "    \"en\": LangItem(\n",
    "        phonemizer=\"en_us\",\n",
    "        phonemizer_espeak=\"en-us\",\n",
    "        nemo=\"en\",\n",
    "        processing_lang_type=\"english_only\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "def get_lang_map(lang: str) -> LangItem:\n",
    "    r\"\"\"Returns a LangItem object for the given language.\n",
    "\n",
    "    Args:\n",
    "        lang (str): The language to get the LangItem for.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the language is not supported.\n",
    "\n",
    "    Returns:\n",
    "        LangItem: The LangItem object for the given language.\n",
    "    \"\"\"\n",
    "    if lang not in langs_map:\n",
    "        raise ValueError(f\"Language {lang} is not supported!\")\n",
    "    return langs_map[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_74617/2474197086.py\", line 5, in <module>\n",
      "    import torchaudio\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/__init__.py\", line 2, in <module>\n",
      "    from . import _extension  # noqa  # usort: skip\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/_extension/__init__.py\", line 5, in <module>\n",
      "    from torchaudio._internal.module_utils import fail_with_message, is_module_available, no_op\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/_internal/__init__.py\", line 4, in <module>\n",
      "    from torch.hub import download_url_to_file, load_state_dict_from_url\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from unidecode import unidecode\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "class NormalizeText:\n",
    "    r\"\"\"NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.\n",
    "\n",
    "    This class normalize the characters in the input text and normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "    Args:\n",
    "        lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "\n",
    "    Attributes:\n",
    "        lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "        model (Normalizer): The `nemo_text_processing` Normalizer model.\n",
    "\n",
    "    Methods:\n",
    "        byte_encode(word: str) -> list: Encode a word as a list of bytes.\n",
    "        normalize_chars(text: str) -> str: Normalize the characters in the input text.\n",
    "        __call__(text: str) -> str: Normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "    Examples:\n",
    "        >>> from training.preprocess.normilize_text import NormalizeText\n",
    "        >>> normilize_text = NormalizeText()\n",
    "        >>> normilize_text(\"It’s a beautiful day…\")\n",
    "        \"It's a beautiful day.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lang: str = \"en\"):\n",
    "        r\"\"\"Initialize a new instance of the NormalizeText class.\n",
    "\n",
    "        Args:\n",
    "            lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "\n",
    "        \"\"\"\n",
    "        self.lang = lang\n",
    "        self.processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()\n",
    "        # self.model = Normalizer(input_case=\"cased\", lang=lang)\n",
    "\n",
    "    def byte_encode(self, word: str) -> list:\n",
    "        r\"\"\"Encode a word as a list of bytes.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to encode.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of bytes representing the encoded word.\n",
    "\n",
    "        \"\"\"\n",
    "        text = word.strip()\n",
    "        return list(text.encode(\"utf-8\"))\n",
    "\n",
    "    def normalize_chars(self, text: str) -> str:\n",
    "        r\"\"\"Normalize the characters in the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "\n",
    "        Examples:\n",
    "            >>> normalize_chars(\"It’s a beautiful day…\")\n",
    "            \"It's a beautiful day.\"\n",
    "\n",
    "        \"\"\"\n",
    "        # Define the character mapping\n",
    "        char_mapping = {\n",
    "            ord(\"’\"): ord(\"'\"),\n",
    "            ord(\"”\"): ord(\"'\"),\n",
    "            ord(\"…\"): ord(\".\"),\n",
    "            ord(\"„\"): ord(\"'\"),\n",
    "            ord(\"“\"): ord(\"'\"),\n",
    "            ord('\"'): ord(\"'\"),\n",
    "            ord(\"–\"): ord(\"-\"),\n",
    "            ord(\"—\"): ord(\"-\"),\n",
    "            ord(\"«\"): ord(\"'\"),\n",
    "            ord(\"»\"): ord(\"'\"),\n",
    "        }\n",
    "\n",
    "        # Add unicode normalization as additional garanty and normalize the characters using translate() method\n",
    "        normalized_string = unidecode(text).translate(char_mapping)\n",
    "\n",
    "        # Remove redundant multiple characters\n",
    "        # TODO: Maybe there is some effect on duplication?\n",
    "        return re.sub(r\"(\\.|\\!|\\?|\\-)\\1+\", r\"\\1\", normalized_string)\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        r\"\"\"Normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "\n",
    "        \"\"\"\n",
    "        text = self.normalize_chars(text)\n",
    "        # return self.model.normalize(text)\n",
    "\n",
    "        # Split the text into lines\n",
    "        # lines = text.split(\"\\n\")\n",
    "        processed, lengths = self.processor(text)\n",
    "        normalized_lines = [self.processor.tokens[i] for i in processed[0, : lengths[0]]]\n",
    "        # normalized_lines = self.model.normalize_list(lines)\n",
    "\n",
    "        # TODO: check this!\n",
    "        # Join the normalized lines, replace \\n with . and return the result\n",
    "        result = \". \".join(normalized_lines)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for the backward comp.\n",
    "# Prepare the phonemes list and dictionary for the embedding\n",
    "phoneme_basic_symbols = [\n",
    "    # IPA symbols\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"æ\",\n",
    "    \"ç\",\n",
    "    \"ð\",\n",
    "    \"ø\",\n",
    "    \"ŋ\",\n",
    "    \"œ\",\n",
    "    \"ɐ\",\n",
    "    \"ɑ\",\n",
    "    \"ɔ\",\n",
    "    \"ə\",\n",
    "    \"ɛ\",\n",
    "    \"ɝ\",\n",
    "    \"ɹ\",\n",
    "    \"ɡ\",\n",
    "    \"ɪ\",\n",
    "    \"ʁ\",\n",
    "    \"ʃ\",\n",
    "    \"ʊ\",\n",
    "    \"ʌ\",\n",
    "    \"ʏ\",\n",
    "    \"ʒ\",\n",
    "    \"ʔ\",\n",
    "    \"ˈ\",\n",
    "    \"ˌ\",\n",
    "    \"ː\",\n",
    "    \"̃\",\n",
    "    \"̍\",\n",
    "    \"̥\",\n",
    "    \"̩\",\n",
    "    \"̯\",\n",
    "    \"͡\",\n",
    "    \"θ\",\n",
    "    # Punctuation\n",
    "    \"!\",\n",
    "    \"?\",\n",
    "    \",\",\n",
    "    \".\",\n",
    "    \"-\",\n",
    "    \":\",\n",
    "    \";\",\n",
    "    '\"',\n",
    "    \"'\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \" \",\n",
    "]\n",
    "\n",
    "# TODO: add support for other languages\n",
    "# _letters_accented = \"µßàáâäåæçèéêëìíîïñòóôöùúûüąćęłńœśşźżƒ\"\n",
    "# _letters_cyrilic = \"абвгдежзийклмнопрстуфхцчшщъыьэюяёєіїґӧ\"\n",
    "# _pad = \"$\"\n",
    "\n",
    "# This is the list of symbols from StyledTTS2\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“”'\n",
    "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "# Combine all symbols\n",
    "symbols = list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "# Add only unique symbols\n",
    "phones = phoneme_basic_symbols + [\n",
    "    symbol for symbol in symbols if symbol not in phoneme_basic_symbols\n",
    "]\n",
    "\n",
    "# TODO: Need to understand how to replace this\n",
    "# len(phones) == 184, leave it as is at this point\n",
    "symbols = [str(el) for el in range(256)]\n",
    "symbol2id = {s: i for i, s in enumerate(symbols)}\n",
    "id2symbol = {i: s for i, s in enumerate(symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import ERROR, Logger\n",
    "import os\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "# IPA Phonemizer: https://github.com/bootphon/phonemizer\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "# Create a Logger instance\n",
    "logger = Logger(\"my_logger\")\n",
    "# Set the level to ERROR\n",
    "logger.setLevel(ERROR)\n",
    "\n",
    "from dp.preprocessing.text import SequenceTokenizer\n",
    "\n",
    "# from models.config import get_lang_map\n",
    "# from models.config.symbols import phones\n",
    "\n",
    "# INFO: Fix for windows, used for local env\n",
    "if os.name == \"nt\":\n",
    "    ESPEAK_LIBRARY = os.getenv(\n",
    "        \"ESPEAK_LIBRARY\",\n",
    "        \"C:\\\\Program Files\\\\eSpeak NG\\\\libespeak-ng.dll\",\n",
    "    )\n",
    "    EspeakWrapper.set_library(ESPEAK_LIBRARY)\n",
    "\n",
    "\n",
    "class TokenizerIpaEspeak:\n",
    "    def __init__(self, lang: str = \"en\"):\n",
    "        lang_map = get_lang_map(lang)\n",
    "        self.lang = lang_map.phonemizer_espeak\n",
    "        self.lang_seq = lang_map.phonemizer\n",
    "\n",
    "        # NOTE: for backward compatibility with previous IPA tokenizer see the TokenizerIPA class\n",
    "        self.tokenizer = SequenceTokenizer(\n",
    "            phones,\n",
    "            languages=[\"de\", \"en_us\"],\n",
    "            lowercase=True,\n",
    "            char_repeats=1,\n",
    "            append_start_end=True,\n",
    "        )\n",
    "\n",
    "        self.phonemizer = EspeakBackend(\n",
    "            language=self.lang,\n",
    "            preserve_punctuation=True,\n",
    "            with_stress=True,\n",
    "            words_mismatch=\"ignore\",\n",
    "            logger=logger,\n",
    "        ).phonemize\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        r\"\"\"Converts the input text to phonemes and tokenizes them.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Union[str, List[str]], List[int]]: IPA phonemes and tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        phones_ipa = \"\".join(self.phonemizer([text]))\n",
    "\n",
    "        tokens = self.tokenizer(phones_ipa, language=self.lang_seq)\n",
    "\n",
    "        return phones_ipa, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VocoderBasicConfig:\n",
    "    segment_size: int = 16384\n",
    "    learning_rate: float = 0.0001\n",
    "    adam_b1: float = 0.5\n",
    "    adam_b2: float = 0.9\n",
    "    lr_decay: float = 0.995\n",
    "    synth_interval: int = 250\n",
    "    checkpoint_interval: int = 250\n",
    "    stft_lamb: float = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class TacotronSTFT(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        n_mel_channels: int,\n",
    "        sampling_rate: int,\n",
    "        center: bool,\n",
    "        mel_fmax: Optional[int],\n",
    "        mel_fmin: float = 0.0,\n",
    "    ):\n",
    "        r\"\"\"TacotronSTFT module that computes mel-spectrograms from a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            filter_length (int): Length of the filter window.\n",
    "            hop_length (int): Number of samples between successive frames.\n",
    "            win_length (int): Size of the STFT window.\n",
    "            n_mel_channels (int): Number of mel bins.\n",
    "            sampling_rate (int): Sampling rate of the input waveforms.\n",
    "            mel_fmin (int or None): Minimum frequency for the mel filter bank.\n",
    "            mel_fmax (int or None): Maximum frequency for the mel filter bank.\n",
    "            center (bool): Whether to pad the input signal on both sides.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_fft = filter_length\n",
    "        self.hop_size = hop_length\n",
    "        self.win_size = win_length\n",
    "        self.fmin = mel_fmin\n",
    "        self.fmax = mel_fmax\n",
    "        self.center = center\n",
    "\n",
    "        # Define the mel filterbank\n",
    "        mel = librosa.filters.mel(\n",
    "            sr=sampling_rate,\n",
    "            n_fft=filter_length,\n",
    "            n_mels=n_mel_channels,\n",
    "            fmin=mel_fmin,\n",
    "            fmax=mel_fmax,\n",
    "        )\n",
    "\n",
    "        mel_basis = torch.tensor(mel, dtype=float).float()\n",
    "\n",
    "        # Define the Hann window\n",
    "        hann_window = torch.hann_window(win_length)\n",
    "\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"hann_window\", hann_window)\n",
    "\n",
    "    def _spectrogram(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computes the linear spectrogram of a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): Input waveforms.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Linear spectrogram.\n",
    "        \"\"\"\n",
    "        assert torch.min(y.data) >= -1\n",
    "        assert torch.max(y.data) <= 1\n",
    "\n",
    "        y = torch.nn.functional.pad(\n",
    "            y.unsqueeze(1),\n",
    "            (\n",
    "                int((self.n_fft - self.hop_size) / 2),\n",
    "                int((self.n_fft - self.hop_size) / 2),\n",
    "            ),\n",
    "            mode=\"reflect\",\n",
    "        )\n",
    "        y = y.squeeze(1)\n",
    "        spec = torch.stft(\n",
    "            y,\n",
    "            self.n_fft,\n",
    "            hop_length=self.hop_size,\n",
    "            win_length=self.win_size,\n",
    "            window=self.hann_window,  # type: ignore\n",
    "            center=self.center,\n",
    "            pad_mode=\"reflect\",\n",
    "            normalized=False,\n",
    "            onesided=True,\n",
    "            return_complex=True,\n",
    "        )\n",
    "        return torch.view_as_real(spec)\n",
    "\n",
    "    def linear_spectrogram(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computes the linear spectrogram of a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): Input waveforms.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Linear spectrogram.\n",
    "        \"\"\"\n",
    "        spec = self._spectrogram(y)\n",
    "        return torch.norm(spec, p=2, dim=-1)\n",
    "\n",
    "    def forward(self, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Computes mel-spectrograms from a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.FloatTensor): Input waveforms with shape (B, T) in range [-1, 1]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Spectrogram of shape (B, n_spech_channels, T)\n",
    "            torch.FloatTensor: Mel-spectrogram of shape (B, n_mel_channels, T)\n",
    "        \"\"\"\n",
    "        spec = self._spectrogram(y)\n",
    "\n",
    "        spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n",
    "\n",
    "        mel = torch.matmul(self.mel_basis, spec)  # type: ignore\n",
    "        mel = self.spectral_normalize_torch(mel)\n",
    "\n",
    "        return spec, mel\n",
    "\n",
    "    def spectral_normalize_torch(self, magnitudes: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Applies dynamic range compression to magnitudes.\n",
    "\n",
    "        Args:\n",
    "            magnitudes (torch.Tensor): Input magnitudes.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output magnitudes.\n",
    "        \"\"\"\n",
    "        return self.dynamic_range_compression_torch(magnitudes)\n",
    "\n",
    "    def dynamic_range_compression_torch(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        C: int = 1,\n",
    "        clip_val: float = 1e-5,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Applies dynamic range compression to x.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            C (float): Compression factor.\n",
    "            clip_val (float): Clipping value.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "    # NOTE: audio np.ndarray changed to torch.FloatTensor!\n",
    "    def get_mel_from_wav(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        audio_tensor = audio.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, melspec = self.forward(audio_tensor)\n",
    "        return melspec.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa.filters import mel as librosa_mel_fn\n",
    "import torch\n",
    "\n",
    "\n",
    "class AudioProcessor:\n",
    "    r\"\"\"A class used to process audio signals and convert them into different representations.\n",
    "\n",
    "    Attributes:\n",
    "        hann_window (dict): A dictionary to store the Hann window for different configurations.\n",
    "        mel_basis (dict): A dictionary to store the Mel basis for different configurations.\n",
    "\n",
    "    Methods:\n",
    "        name_mel_basis(spec, n_fft, fmax): Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n",
    "        amp_to_db(magnitudes, C=1, clip_val=1e-5): Convert amplitude to decibels (dB).\n",
    "        db_to_amp(magnitudes, C=1): Convert decibels (dB) to amplitude.\n",
    "        wav_to_spec(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the magnitude.\n",
    "        wav_to_energy(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the energy.\n",
    "        spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax): Convert a spectrogram to a Mel spectrogram.\n",
    "        wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False): Convert a waveform to a Mel spectrogram.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hann_window = {}\n",
    "        self.mel_basis = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def name_mel_basis(spec: torch.Tensor, n_fft: int, fmax: int) -> str:\n",
    "        \"\"\"Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): The spectrogram tensor.\n",
    "            n_fft (int): The FFT size.\n",
    "            fmax (int): The maximum frequency.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated name for the Mel basis.\n",
    "        \"\"\"\n",
    "        n_fft_len = f\"{n_fft}_{fmax}_{spec.dtype}_{spec.device}\"\n",
    "        return n_fft_len\n",
    "\n",
    "    @staticmethod\n",
    "    def amp_to_db(magnitudes: torch.Tensor, C: int = 1, clip_val: float = 1e-5) -> torch.Tensor:\n",
    "        r\"\"\"Convert amplitude to decibels (dB).\n",
    "\n",
    "        Args:\n",
    "            magnitudes (Tensor): The amplitude magnitudes to convert.\n",
    "            C (int, optional): A constant value used in the conversion. Defaults to 1.\n",
    "            clip_val (float, optional): A value to clamp the magnitudes to avoid taking the log of zero. Defaults to 1e-5.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The converted magnitudes in dB.\n",
    "        \"\"\"\n",
    "        return torch.log(torch.clamp(magnitudes, min=clip_val) * C)\n",
    "\n",
    "    @staticmethod\n",
    "    def db_to_amp(magnitudes: torch.Tensor, C: int = 1) -> torch.Tensor:\n",
    "        r\"\"\"Convert decibels (dB) to amplitude.\n",
    "\n",
    "        Args:\n",
    "            magnitudes (Tensor): The dB magnitudes to convert.\n",
    "            C (int, optional): A constant value used in the conversion. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The converted magnitudes in amplitude.\n",
    "        \"\"\"\n",
    "        return torch.exp(magnitudes) / C\n",
    "\n",
    "    def wav_to_spec(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a spectrogram and compute the magnitude.\n",
    "\n",
    "        Args:\n",
    "            y (Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The magnitude of the computed spectrogram.\n",
    "        \"\"\"\n",
    "        y = y.squeeze(1)\n",
    "\n",
    "        dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "        wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n",
    "        if wnsize_dtype_device not in self.hann_window:\n",
    "            self.hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "        y = torch.nn.functional.pad(\n",
    "            y.unsqueeze(1),\n",
    "            (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n",
    "            mode=\"reflect\",\n",
    "        )\n",
    "        y = y.squeeze(1)\n",
    "\n",
    "        spec = torch.stft(\n",
    "            y,\n",
    "            n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=self.hann_window[wnsize_dtype_device],\n",
    "            center=center,\n",
    "            pad_mode=\"reflect\",\n",
    "            normalized=False,\n",
    "            onesided=True,\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        spec = torch.view_as_real(spec)\n",
    "\n",
    "        # Compute the magnitude\n",
    "        spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def wav_to_energy(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a spectrogram and compute the energy.\n",
    "\n",
    "        Args:\n",
    "            y (Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The energy of the computed spectrogram.\n",
    "        \"\"\"\n",
    "        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n",
    "        spec = torch.norm(spec, dim=1, keepdim=True).squeeze(0)\n",
    "\n",
    "        # Normalize the energy\n",
    "        return (spec - spec.mean()) / spec.std()\n",
    "\n",
    "    def spec_to_mel(\n",
    "            self,\n",
    "            spec: torch.Tensor,\n",
    "            n_fft: int,\n",
    "            num_mels: int,\n",
    "            sample_rate: int,\n",
    "            fmin: int,\n",
    "            fmax: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a spectrogram to a Mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): The input spectrogram of shape [B, C, T].\n",
    "            n_fft (int): The FFT size.\n",
    "            num_mels (int): The number of Mel bands.\n",
    "            sample_rate (int): The sample rate of the audio.\n",
    "            fmin (int): The minimum frequency.\n",
    "            fmax (int): The maximum frequency.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed Mel spectrogram of shape [B, C, T].\n",
    "        \"\"\"\n",
    "        mel_basis_key = self.name_mel_basis(spec, n_fft, fmax)\n",
    "\n",
    "        if mel_basis_key not in self.mel_basis:\n",
    "            mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "            self.mel_basis[mel_basis_key] = torch.tensor(mel).to(dtype=spec.dtype, device=spec.device)\n",
    "\n",
    "        mel = torch.matmul(self.mel_basis[mel_basis_key], spec)\n",
    "        mel = self.amp_to_db(mel)\n",
    "\n",
    "        return mel\n",
    "\n",
    "    def wav_to_mel(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        num_mels: int,\n",
    "        sample_rate: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        fmin: int,\n",
    "        fmax: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a Mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            num_mels (int): The number of Mel bands.\n",
    "            sample_rate (int): The sample rate of the audio.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            fmin (int): The minimum frequency.\n",
    "            fmax (int): The maximum frequency.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed Mel spectrogram.\n",
    "        \"\"\"\n",
    "        # Convert the waveform to a spectrogram\n",
    "        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n",
    "\n",
    "        # Convert the spectrogram to a Mel spectrogram\n",
    "        mel = self.spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax)\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def stereo_to_mono(audio: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Converts a stereo audio tensor to mono by taking the mean across channels.\n",
    "\n",
    "    Args:\n",
    "        audio (torch.Tensor): Input audio tensor of shape (channels, samples).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mono audio tensor of shape (1, samples).\n",
    "    \"\"\"\n",
    "    return torch.mean(audio, 0, True)\n",
    "\n",
    "\n",
    "def resample(wav: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
    "    r\"\"\"Resamples an audio waveform from the original sampling rate to the target sampling rate.\n",
    "\n",
    "    Args:\n",
    "        wav (np.ndarray): The audio waveform to be resampled.\n",
    "        orig_sr (int): The original sampling rate of the audio waveform.\n",
    "        target_sr (int): The target sampling rate to resample the audio waveform to.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resampled audio waveform.\n",
    "    \"\"\"\n",
    "    return librosa.resample(wav, orig_sr=orig_sr, target_sr=target_sr)\n",
    "\n",
    "\n",
    "def safe_load(path: str, sr: Union[int, None]) -> Tuple[np.ndarray, int]:\n",
    "    r\"\"\"Load an audio file from disk and return its content as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the audio file.\n",
    "        sr (int or None): The target sampling rate. If None, the original sampling rate is used.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, int]: A tuple containing the audio content as a numpy array and the actual sampling rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr_actual = torchaudio.load(path) # type: ignore\n",
    "        if audio.shape[0] > 0:\n",
    "            audio = stereo_to_mono(audio)\n",
    "        audio = audio.squeeze(0)\n",
    "        if sr_actual != sr and sr is not None:\n",
    "            audio = resample(audio.numpy(), orig_sr=sr_actual, target_sr=sr)\n",
    "            sr_actual = sr\n",
    "        else:\n",
    "            audio = audio.numpy()\n",
    "    except Exception as e:\n",
    "        raise type(e)(\n",
    "            f\"The following error happened loading the file {path} ... \\n\" + str(e),\n",
    "        ).with_traceback(sys.exc_info()[2])\n",
    "\n",
    "    return audio, sr_actual\n",
    "\n",
    "\n",
    "def preprocess_audio(\n",
    "    audio: torch.Tensor, sr_actual: int, sr: Union[int, None],\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    r\"\"\"Preprocesses audio by converting stereo to mono, resampling if necessary, and returning the audio tensor and sample rate.\n",
    "\n",
    "    Args:\n",
    "        audio (torch.Tensor): The audio tensor to preprocess.\n",
    "        sr_actual (int): The actual sample rate of the audio.\n",
    "        sr (Union[int, None]): The target sample rate to resample the audio to, if necessary.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, int]: The preprocessed audio tensor and sample rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if audio.shape[0] > 0:\n",
    "            audio = stereo_to_mono(audio)\n",
    "        audio = audio.squeeze(0)\n",
    "        if sr_actual != sr and sr is not None:\n",
    "            if isinstance(audio, torch.Tensor):\n",
    "                detach = audio.data.detach().tolist()\n",
    "                \n",
    "                audio = np.array(detach, dtype=float)\n",
    "\n",
    "            # audio\n",
    "            audio_np = resample(audio, orig_sr=sr_actual, target_sr=sr)\n",
    "            # Convert back to torch tensor\n",
    "            audio = torch.tensor(audio_np)\n",
    "            sr_actual = sr\n",
    "    except Exception as e:\n",
    "        raise type(e)(\n",
    "            f\"The following error happened while processing the audio ... \\n {e!s}\",\n",
    "        ).with_traceback(sys.exc_info()[2])\n",
    "\n",
    "    return audio, sr_actual\n",
    "\n",
    "\n",
    "def normalize_loudness(wav: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Normalize the loudness of an audio waveform.\n",
    "\n",
    "    Args:\n",
    "        wav (torch.Tensor): The input waveform.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The normalized waveform.\n",
    "\n",
    "    Examples:\n",
    "        >>> wav = np.array([1.0, 2.0, 3.0])\n",
    "        >>> normalize_loudness(wav)\n",
    "        tensor([0.33333333, 0.66666667, 1.  ])\n",
    "    \"\"\"\n",
    "    return wav / torch.max(torch.abs(wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: LOOK AT THIS ESTIMATION ALGO\n",
    "#######################################################################################\n",
    "# Original implementation from https://github.com/NVIDIA/mellotron/blob/master/yin.py #\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def differenceFunction(x: np.ndarray, N: int, tau_max: int) -> np.ndarray:\n",
    "    r\"\"\"Compute the difference function of an audio signal.\n",
    "\n",
    "    This function computes the difference function of an audio signal `x` using the algorithm described in equation (6) of [1]. The difference function is a measure of the similarity between the signal and a time-shifted version of itself, and is commonly used in pitch detection algorithms.\n",
    "\n",
    "    This implementation uses the NumPy FFT functions to compute the difference function efficiently.\n",
    "\n",
    "    Parameters\n",
    "        x (np.ndarray): The audio signal to compute the difference function for.\n",
    "        N (int): The length of the audio signal.\n",
    "        tau_max (int): The maximum integration window size to use.\n",
    "\n",
    "    Returns\n",
    "        np.ndarray: The difference function of the audio signal.\n",
    "\n",
    "    References\n",
    "        [1] A. de Cheveigné and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n",
    "    \"\"\"\n",
    "    x = np.array(x, np.float64)\n",
    "    w = x.size\n",
    "    tau_max = min(tau_max, w)\n",
    "    x_cumsum = np.concatenate((np.array([0.0]), (x * x).cumsum()))\n",
    "    size = w + tau_max\n",
    "    p2 = (size // 32).bit_length()\n",
    "    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n",
    "    size_pad = min(x * 2**p2 for x in nice_numbers if x * 2**p2 >= size)\n",
    "    fc = np.fft.rfft(x, size_pad)\n",
    "    conv = np.fft.irfft(fc * fc.conjugate())[:tau_max]\n",
    "    return x_cumsum[w : w - tau_max : -1] + x_cumsum[w] - x_cumsum[:tau_max] - 2 * conv\n",
    "\n",
    "\n",
    "def cumulativeMeanNormalizedDifferenceFunction(df: np.ndarray, N: int) -> np.ndarray:\n",
    "    r\"\"\"Compute the cumulative mean normalized difference function (CMND) of a difference function.\n",
    "\n",
    "    The CMND is defined as the element-wise product of the difference function with a range of values from 1 to N-1,\n",
    "    divided by the cumulative sum of the difference function up to that point, plus a small epsilon value to avoid\n",
    "    division by zero. The first element of the CMND is set to 1.\n",
    "\n",
    "    Args:\n",
    "        df (np.ndarray): The difference function.\n",
    "        N (int): The length of the data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The cumulative mean normalized difference function.\n",
    "\n",
    "    References:\n",
    "        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n",
    "            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n",
    "    \"\"\"\n",
    "    cmndf = (\n",
    "        df[1:] * range(1, N) / (np.cumsum(df[1:]).astype(float) + 1e-8)\n",
    "    )  # scipy method\n",
    "    return np.insert(cmndf, 0, 1)\n",
    "\n",
    "\n",
    "def getPitch(cmdf: np.ndarray, tau_min: int, tau_max: int, harmo_th: float=0.1) -> int:\n",
    "    r\"\"\"Compute the fundamental period of a frame based on the Cumulative Mean Normalized Difference function (CMND).\n",
    "\n",
    "    The CMND is a measure of the periodicity of a signal, and is computed as the cumulative mean normalized difference\n",
    "    function of the difference function of the signal. The fundamental period is the first value of the index `tau`\n",
    "    between `tau_min` and `tau_max` where the CMND is below the `harmo_th` threshold. If there are no such values, the\n",
    "    function returns 0 to indicate that the signal is unvoiced.\n",
    "\n",
    "    Args:\n",
    "        cmdf (np.ndarray): The Cumulative Mean Normalized Difference function of the signal.\n",
    "        tau_min (int): The minimum period for speech.\n",
    "        tau_max (int): The maximum period for speech.\n",
    "        harmo_th (float, optional): The harmonicity threshold to determine if it is necessary to compute pitch\n",
    "            frequency. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        int: The fundamental period of the signal, or 0 if the signal is unvoiced.\n",
    "\n",
    "    References:\n",
    "        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n",
    "            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n",
    "    \"\"\"\n",
    "    tau = tau_min\n",
    "    while tau < tau_max:\n",
    "        if cmdf[tau] < harmo_th:\n",
    "            while tau + 1 < tau_max and cmdf[tau + 1] < cmdf[tau]:\n",
    "                tau += 1\n",
    "            return tau\n",
    "        tau += 1\n",
    "\n",
    "    return 0  # if unvoiced\n",
    "\n",
    "\n",
    "def compute_yin(\n",
    "    sig_torch: torch.Tensor,\n",
    "    sr: int,\n",
    "    w_len: int = 512,\n",
    "    w_step: int = 256,\n",
    "    f0_min: int = 100,\n",
    "    f0_max: int = 500,\n",
    "    harmo_thresh: float = 0.1,\n",
    ") -> Tuple[np.ndarray, List[float], List[float], List[float]]:\n",
    "    r\"\"\"Compute the Yin Algorithm for pitch detection on an audio signal.\n",
    "\n",
    "    The Yin Algorithm is a widely used method for pitch detection in speech and music signals. It works by computing the\n",
    "    Cumulative Mean Normalized Difference function (CMND) of the difference function of the signal, and finding the first\n",
    "    minimum of the CMND below a given threshold. The fundamental period of the signal is then estimated as the inverse of\n",
    "    the lag corresponding to this minimum.\n",
    "\n",
    "    Args:\n",
    "        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n",
    "        sr (int): The sampling rate of the signal.\n",
    "        w_len (int, optional): The size of the analysis window in samples. Defaults to 512.\n",
    "        w_step (int, optional): The size of the lag between two consecutive windows in samples. Defaults to 256.\n",
    "        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz. Defaults to 100.\n",
    "        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz. Defaults to 500.\n",
    "        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND\n",
    "            function below this threshold. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, List[float], List[float], List[float]]: A tuple containing the following elements:\n",
    "            * pitches (np.ndarray): A 1D numpy array of fundamental frequencies estimated for each analysis window.\n",
    "            * harmonic_rates (List[float]): A list of harmonic rate values for each fundamental frequency value, which\n",
    "              can be interpreted as a confidence value.\n",
    "            * argmins (List[float]): A list of the minimums of the Cumulative Mean Normalized Difference Function for\n",
    "              each analysis window.\n",
    "            * times (List[float]): A list of the time of each estimation, in seconds.\n",
    "\n",
    "    References:\n",
    "        [1] A. K. Jain, Fundamentals of Digital Image Processing, Prentice Hall, 1989.\n",
    "        [2] A. de Cheveigné and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal\n",
    "            of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n",
    "    \"\"\"\n",
    "    sig_torch = sig_torch.view(1, 1, -1)\n",
    "    sig_torch = F.pad(\n",
    "        sig_torch.unsqueeze(1),\n",
    "        (int((w_len - w_step) / 2), int((w_len - w_step) / 2), 0, 0),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "    sig_torch_n: np.ndarray = sig_torch.view(-1).numpy()\n",
    "\n",
    "    tau_min = int(sr / f0_max)\n",
    "    tau_max = int(sr / f0_min)\n",
    "\n",
    "    timeScale = range(\n",
    "        0, len(sig_torch_n) - w_len, w_step,\n",
    "    )  # time values for each analysis window\n",
    "    times = [t / float(sr) for t in timeScale]\n",
    "    frames = [sig_torch_n[t : t + w_len] for t in timeScale]\n",
    "\n",
    "    pitches = [0.0] * len(timeScale)\n",
    "    harmonic_rates = [0.0] * len(timeScale)\n",
    "    argmins = [0.0] * len(timeScale)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Compute YIN\n",
    "        df = differenceFunction(frame, w_len, tau_max)\n",
    "        cmdf = cumulativeMeanNormalizedDifferenceFunction(df, tau_max)\n",
    "        p = getPitch(cmdf, tau_min, tau_max, harmo_thresh)\n",
    "\n",
    "        # Get results\n",
    "        if np.argmin(cmdf) > tau_min:\n",
    "            argmins[i] = float(sr / np.argmin(cmdf))\n",
    "        if p != 0:  # A pitch was found\n",
    "            pitches[i] = float(sr / p)\n",
    "            harmonic_rates[i] = cmdf[p]\n",
    "        else:  # No pitch, but we compute a value of the harmonic rate\n",
    "            harmonic_rates[i] = min(cmdf)\n",
    "\n",
    "    return np.array(pitches), harmonic_rates, argmins, times\n",
    "\n",
    "\n",
    "def norm_interp_f0(f0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    r\"\"\"Normalize and interpolate the fundamental frequency (f0) values.\n",
    "\n",
    "    Args:\n",
    "        f0 (np.ndarray): The input f0 values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: A tuple containing the normalized f0 values and a boolean array indicating which values were interpolated.\n",
    "\n",
    "    Examples:\n",
    "        >>> f0 = np.array([0, 100, 0, 200, 0])\n",
    "        >>> norm_interp_f0(f0)\n",
    "        (\n",
    "            np.array([100, 100, 150, 200, 200]),\n",
    "            np.array([True, False, True, False, True]),\n",
    "        )\n",
    "    \"\"\"\n",
    "    uv: np.ndarray = f0 == 0\n",
    "    if sum(uv) == len(f0):\n",
    "        f0[uv] = 0\n",
    "    elif sum(uv) > 0:\n",
    "        f0[uv] = np.interp(np.where(uv)[0], np.where(~uv)[0], f0[~uv])\n",
    "    return f0, uv\n",
    "\n",
    "\n",
    "def compute_pitch(\n",
    "    sig_torch: torch.Tensor,\n",
    "    sr: int,\n",
    "    w_len: int = 1024,\n",
    "    w_step: int = 256,\n",
    "    f0_min: int = 50,\n",
    "    f0_max: int = 1000,\n",
    "    harmo_thresh: float = 0.25,\n",
    "):\n",
    "    r\"\"\"Compute the pitch of an audio signal using the Yin algorithm.\n",
    "\n",
    "    The Yin algorithm is a widely used method for pitch detection in speech and music signals. This function uses the\n",
    "    Yin algorithm to compute the pitch of the input audio signal, and then normalizes and interpolates the pitch values.\n",
    "    Returns the normalized and interpolated pitch values.\n",
    "\n",
    "    Args:\n",
    "        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n",
    "        sr (int): The sampling rate of the signal.\n",
    "        w_len (int, optional): The size of the analysis window in samples.\n",
    "        w_step (int, optional): The size of the lag between two consecutive windows in samples.\n",
    "        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz.\n",
    "        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz.\n",
    "        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND function below this threshold.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized and interpolated pitch values of the audio signal.\n",
    "    \"\"\"\n",
    "    pitch, _, _, _ = compute_yin(\n",
    "        sig_torch,\n",
    "        sr=sr,\n",
    "        w_len=w_len,\n",
    "        w_step=w_step,\n",
    "        f0_min=f0_min,\n",
    "        f0_max=f0_max,\n",
    "        harmo_thresh=harmo_thresh,\n",
    "    )\n",
    "\n",
    "    pitch, _ = norm_interp_f0(pitch)\n",
    "\n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import random\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import betabinom\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from models.config import PreprocessingConfig, VocoderBasicConfig, get_lang_map\n",
    "\n",
    "# from .audio import normalize_loudness, preprocess_audio\n",
    "# from .audio_processor import AudioProcessor\n",
    "# from .compute_yin import compute_yin, norm_interp_f0\n",
    "# from .normalize_text import NormalizeText\n",
    "# from .tacotron_stft import TacotronSTFT\n",
    "# from .tokenizer_ipa_espeak import TokenizerIpaEspeak as TokenizerIPA\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessForAcousticResult:\n",
    "    wav: torch.Tensor\n",
    "    mel: torch.Tensor\n",
    "    pitch: torch.Tensor\n",
    "    phones_ipa: Union[str, List[str]]\n",
    "    phones: torch.Tensor\n",
    "    attn_prior: torch.Tensor\n",
    "    energy: torch.Tensor\n",
    "    raw_text: str\n",
    "    normalized_text: str\n",
    "    speaker_id: int\n",
    "    chapter_id: str | int\n",
    "    utterance_id: str\n",
    "    pitch_is_normalized: bool\n",
    "\n",
    "\n",
    "class PreprocessLibriTTS:\n",
    "    r\"\"\"Preprocessing PreprocessLibriTTS audio and text data for use with a TacotronSTFT model.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): The preprocessing configuration.\n",
    "        lang (str): The language of the input text.\n",
    "\n",
    "    Attributes:\n",
    "        min_seconds (float): The minimum duration of audio clips in seconds.\n",
    "        max_seconds (float): The maximum duration of audio clips in seconds.\n",
    "        hop_length (int): The hop length of the STFT.\n",
    "        sampling_rate (int): The sampling rate of the audio.\n",
    "        use_audio_normalization (bool): Whether to normalize the loudness of the audio.\n",
    "        tacotronSTFT (TacotronSTFT): The TacotronSTFT object used for computing mel spectrograms.\n",
    "        min_samples (int): The minimum number of audio samples in a clip.\n",
    "        max_samples (int): The maximum number of audio samples in a clip.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        lang: str = \"en\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        lang_map = get_lang_map(lang)\n",
    "\n",
    "        self.phonemizer_lang = lang_map.phonemizer\n",
    "        normilize_text_lang = lang_map.nemo\n",
    "\n",
    "        self.normilize_text = NormalizeText(normilize_text_lang)\n",
    "        self.tokenizer = TokenizerIpaEspeak(lang)\n",
    "        self.vocoder_train_config = VocoderBasicConfig()\n",
    "\n",
    "        self.preprocess_config = preprocess_config\n",
    "\n",
    "        self.sampling_rate = self.preprocess_config.sampling_rate\n",
    "        self.use_audio_normalization = self.preprocess_config.use_audio_normalization\n",
    "\n",
    "        self.hop_length = self.preprocess_config.stft.hop_length\n",
    "        self.filter_length = self.preprocess_config.stft.filter_length\n",
    "        self.mel_fmin = self.preprocess_config.stft.mel_fmin\n",
    "        self.win_length = self.preprocess_config.stft.win_length\n",
    "\n",
    "        self.tacotronSTFT = TacotronSTFT(\n",
    "            filter_length=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.preprocess_config.stft.win_length,\n",
    "            n_mel_channels=self.preprocess_config.stft.n_mel_channels,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            mel_fmin=self.mel_fmin,\n",
    "            mel_fmax=self.preprocess_config.stft.mel_fmax,\n",
    "            center=False,\n",
    "        )\n",
    "\n",
    "        min_seconds, max_seconds = (\n",
    "            self.preprocess_config.min_seconds,\n",
    "            self.preprocess_config.max_seconds,\n",
    "        )\n",
    "\n",
    "        self.min_samples = int(self.sampling_rate * min_seconds)\n",
    "        self.max_samples = int(self.sampling_rate * max_seconds)\n",
    "\n",
    "        self.audio_processor = AudioProcessor()\n",
    "\n",
    "    def beta_binomial_prior_distribution(\n",
    "        self,\n",
    "        phoneme_count: int,\n",
    "        mel_count: int,\n",
    "        scaling_factor: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Computes the beta-binomial prior distribution for the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            phoneme_count (int): Number of phonemes in the input text.\n",
    "            mel_count (int): Number of mel frames in the input mel-spectrogram.\n",
    "            scaling_factor (float, optional): Scaling factor for the beta distribution. Defaults to 1.0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A 2D tensor containing the prior distribution.\n",
    "        \"\"\"\n",
    "        P, M = phoneme_count, mel_count\n",
    "        x = np.arange(0, P)\n",
    "        mel_text_probs = []\n",
    "        for i in range(1, M + 1):\n",
    "            a, b = scaling_factor * i, scaling_factor * (M + 1 - i)\n",
    "            rv: Any = betabinom(P, a, b)\n",
    "            mel_i_prob = rv.pmf(x)\n",
    "            mel_text_probs.append(mel_i_prob)\n",
    "        return torch.tensor(np.array(mel_text_probs))\n",
    "\n",
    "    def acoustic(\n",
    "        self,\n",
    "        row: Tuple[torch.Tensor, int, str, str, int, str | int, str],\n",
    "    ) -> Union[None, PreprocessForAcousticResult]:\n",
    "        r\"\"\"Preprocesses audio and text data for use with a TacotronSTFT model.\n",
    "\n",
    "        Args:\n",
    "            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the preprocessed audio and text data.\n",
    "\n",
    "        Examples:\n",
    "            >>> preprocess_audio = PreprocessAudio(\"english_only\")\n",
    "            >>> audio = torch.randn(1, 44100)\n",
    "            >>> sr_actual = 44100\n",
    "            >>> raw_text = \"Hello, world!\"\n",
    "            >>> output = preprocess_audio(audio, sr_actual, raw_text)\n",
    "            >>> output.keys()\n",
    "            dict_keys(['wav', 'mel', 'pitch', 'phones', 'raw_text', 'normalized_text', 'speaker_id', 'chapter_id', 'utterance_id', 'pitch_is_normalized'])\n",
    "        \"\"\"\n",
    "        (\n",
    "            audio,\n",
    "            sr_actual,\n",
    "            raw_text,\n",
    "            normalized_text,\n",
    "            speaker_id,\n",
    "            chapter_id,\n",
    "            utterance_id,\n",
    "        ) = row\n",
    "\n",
    "        wav, sampling_rate = preprocess_audio(audio, sr_actual, self.sampling_rate)\n",
    "\n",
    "        # TODO: check this, maybe you need to move it to some other place\n",
    "        # TODO: maybe we can increate the max_samples ?\n",
    "        # if wav.shape[0] < self.min_samples or wav.shape[0] > self.max_samples:\n",
    "        #     return None\n",
    "\n",
    "        if self.use_audio_normalization:\n",
    "            wav = normalize_loudness(wav)\n",
    "\n",
    "        normalized_text = self.normilize_text(normalized_text)\n",
    "\n",
    "        # NOTE: fixed version of tokenizer with punctuation\n",
    "        phones_ipa, phones = self.tokenizer(normalized_text)\n",
    "\n",
    "        # Convert to tensor\n",
    "        phones = torch.Tensor(phones)\n",
    "\n",
    "        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n",
    "\n",
    "        # Skipping small sample due to the mel-spectrogram containing less than self.mel_fmin frames\n",
    "        # if mel_spectrogram.shape[1] < self.mel_fmin:\n",
    "        #     return None\n",
    "\n",
    "        # Text is longer than mel, will be skipped due to monotonic alignment search\n",
    "        if phones.shape[0] >= mel_spectrogram.shape[1]:\n",
    "            return None\n",
    "\n",
    "        pitch, _, _, _ = compute_yin(\n",
    "            wav,\n",
    "            sr=sampling_rate,\n",
    "            w_len=self.filter_length,\n",
    "            w_step=self.hop_length,\n",
    "            f0_min=50,\n",
    "            f0_max=1000,\n",
    "            harmo_thresh=0.25,\n",
    "        )\n",
    "\n",
    "        pitch, _ = norm_interp_f0(pitch)\n",
    "\n",
    "        if np.sum(pitch != 0) <= 1:\n",
    "            return None\n",
    "\n",
    "        pitch = torch.tensor(pitch)\n",
    "\n",
    "        # TODO this shouldnt be necessary, currently pitch sometimes has 1 less frame than spectrogram,\n",
    "        # We should find out why\n",
    "        mel_spectrogram = mel_spectrogram[:, : pitch.shape[0]]\n",
    "\n",
    "        attn_prior = self.beta_binomial_prior_distribution(\n",
    "            phones.shape[0],\n",
    "            mel_spectrogram.shape[1],\n",
    "        ).T\n",
    "\n",
    "        assert pitch.shape[0] == mel_spectrogram.shape[1], (\n",
    "            pitch.shape,\n",
    "            mel_spectrogram.shape[1],\n",
    "        )\n",
    "\n",
    "        energy = self.audio_processor.wav_to_energy(\n",
    "            wav.unsqueeze(0),\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "        )\n",
    "\n",
    "        return PreprocessForAcousticResult(\n",
    "            wav=wav,\n",
    "            mel=mel_spectrogram,\n",
    "            pitch=pitch,\n",
    "            attn_prior=attn_prior,\n",
    "            energy=energy,\n",
    "            phones_ipa=phones_ipa,\n",
    "            phones=phones,\n",
    "            raw_text=raw_text,\n",
    "            normalized_text=normalized_text,\n",
    "            speaker_id=speaker_id,\n",
    "            chapter_id=chapter_id,\n",
    "            utterance_id=utterance_id,\n",
    "            # TODO: check the pitch normalization process\n",
    "            pitch_is_normalized=False,\n",
    "        )\n",
    "\n",
    "    def univnet(self, row: Tuple[torch.Tensor, int, str, str, int, str | int, str]):\n",
    "        r\"\"\"Preprocesses audio data for use with a UnivNet model.\n",
    "\n",
    "        This method takes a row of data, extracts the audio and preprocesses it.\n",
    "        It then selects a random segment from the preprocessed audio and its corresponding mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, int]: A tuple containing the selected segment of the mel spectrogram, the corresponding audio segment, and the speaker ID.\n",
    "\n",
    "        Examples:\n",
    "            >>> preprocess = PreprocessLibriTTS()\n",
    "            >>> audio = torch.randn(1, 44100)\n",
    "            >>> sr_actual = 44100\n",
    "            >>> speaker_id = 0\n",
    "            >>> mel, audio_segment, speaker_id = preprocess.preprocess_univnet((audio, sr_actual, \"\", \"\", speaker_id, 0, \"\"))\n",
    "        \"\"\"\n",
    "        (\n",
    "            audio,\n",
    "            sr_actual,\n",
    "            _,\n",
    "            _,\n",
    "            speaker_id,\n",
    "            _,\n",
    "            _,\n",
    "        ) = row\n",
    "\n",
    "        segment_size = self.vocoder_train_config.segment_size\n",
    "        frames_per_seg = math.ceil(segment_size / self.hop_length)\n",
    "\n",
    "        wav, _ = preprocess_audio(audio, sr_actual, self.sampling_rate)\n",
    "\n",
    "        if self.use_audio_normalization:\n",
    "            wav = normalize_loudness(wav)\n",
    "\n",
    "        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n",
    "\n",
    "        if wav.shape[0] < segment_size:\n",
    "            wav = F.pad(\n",
    "                wav,\n",
    "                (0, segment_size - wav.shape[0]),\n",
    "                \"constant\",\n",
    "            )\n",
    "\n",
    "        if mel_spectrogram.shape[1] < frames_per_seg:\n",
    "            mel_spectrogram = F.pad(\n",
    "                mel_spectrogram,\n",
    "                (0, frames_per_seg - mel_spectrogram.shape[1]),\n",
    "                \"constant\",\n",
    "            )\n",
    "\n",
    "        from_frame = random.randint(0, mel_spectrogram.shape[1] - frames_per_seg)\n",
    "\n",
    "        # Skip last frame, otherwise errors are thrown, find out why\n",
    "        if from_frame > 0:\n",
    "            from_frame -= 1\n",
    "\n",
    "        till_frame = from_frame + frames_per_seg\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram[:, from_frame:till_frame]\n",
    "        wav = wav[from_frame * self.hop_length : till_frame * self.hop_length]\n",
    "\n",
    "        return mel_spectrogram, wav, speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "def pad_1D(inputs: List[Tensor], pad_value: float = 0.0) -> Tensor:\n",
    "    r\"\"\"Pad a list of 1D tensor list to the same length.\n",
    "\n",
    "    Args:\n",
    "        inputs (List[torch.Tensor]): List of 1D numpy arrays to pad.\n",
    "        pad_value (float): Value to use for padding. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 2D numpy array of shape (len(inputs), max_len), where max_len is the length of the longest input array.\n",
    "    \"\"\"\n",
    "    max_len = max(x.size(0) for x in inputs)\n",
    "    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(0)), value=pad_value) for x in inputs]\n",
    "    return torch.stack(padded_inputs)\n",
    "\n",
    "\n",
    "def pad_2D(\n",
    "    inputs: List[Tensor], maxlen: Union[int, None] = None, pad_value: float = 0.0,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Pad a list of 2D tensor arrays to the same length.\n",
    "\n",
    "    Args:\n",
    "        inputs (List[torch.Tensor]): List of 2D numpy arrays to pad.\n",
    "        maxlen (Union[int, None]): Maximum length to pad the arrays to. If None, pad to the length of the longest array. Default is None.\n",
    "        pad_value (float): Value to use for padding. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 3D numpy array of shape (len(inputs), max_len, input_dim), where max_len is the maximum length of the input arrays, and input_dim is the dimension of the input arrays.\n",
    "    \"\"\"\n",
    "    max_len = max(x.size(1) for x in inputs) if maxlen is None else maxlen\n",
    "\n",
    "    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(1), 0, 0), value=pad_value) for x in inputs]\n",
    "    return torch.stack(padded_inputs)\n",
    "\n",
    "\n",
    "def pad_3D(inputs: Union[Tensor, List[Tensor]], B: int, T: int, L: int) -> Tensor:\n",
    "    r\"\"\"Pad a 3D torch tensor to a specified shape.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): 3D numpy array to pad.\n",
    "        B (int): Batch size to pad the array to.\n",
    "        T (int): Time steps to pad the array to.\n",
    "        L (int): Length to pad the array to.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 3D numpy array of shape (B, T, L), where B is the batch size, T is the time steps, and L is the length.\n",
    "    \"\"\"\n",
    "    if isinstance(inputs, list):\n",
    "        inputs_padded = torch.zeros(B, T, L, dtype=inputs[0].dtype)\n",
    "        for i, input_ in enumerate(inputs):\n",
    "            inputs_padded[i, :input_.size(0), :input_.size(1)] = input_\n",
    "\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        inputs_padded = torch.zeros(B, T, L, dtype=inputs.dtype)\n",
    "        inputs_padded[:inputs.size(0), :inputs.size(1), :inputs.size(2)] = inputs\n",
    "\n",
    "    return inputs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import datasets\n",
    "\n",
    "# from models.config import PreprocessingConfigUnivNet, get_lang_map\n",
    "\n",
    "# from training.preprocess import PreprocessLibriTTS\n",
    "# from training.tools import pad_1D, pad_2D\n",
    "\n",
    "\n",
    "class LibriTTSDatasetVocoder(Dataset):\n",
    "    r\"\"\"Loading preprocessed univnet model data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        batch_size: int,\n",
    "        download: bool = True,\n",
    "        lang: str = \"en\",\n",
    "    ):\n",
    "        r\"\"\"A PyTorch dataset for loading preprocessed univnet data.\n",
    "\n",
    "        Args:\n",
    "            root (str): Path to the directory where the dataset is found or downloaded.\n",
    "            batch_size (int): Batch size for the dataset.\n",
    "            download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.LIBRITTS(root=root, download=download)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        lang_map = get_lang_map(lang)\n",
    "        self.preprocess_libtts = PreprocessLibriTTS(\n",
    "            PreprocessingConfigUnivNet(lang_map.processing_lang_type),\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        r\"\"\"Returns a sample from the dataset at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing the sample data.\n",
    "        \"\"\"\n",
    "        # Retrive the dataset row\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        data = self.preprocess_libtts.univnet(data)\n",
    "\n",
    "        if data is None:\n",
    "            # print(\"Skipping due to preprocessing error\")\n",
    "            rand_idx = np.random.randint(0, self.__len__())\n",
    "            return self.__getitem__(rand_idx)\n",
    "\n",
    "        mel, audio, speaker_id = data\n",
    "\n",
    "        return {\n",
    "            \"mel\": mel,\n",
    "            \"audio\": audio,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, data: List) -> List:\n",
    "        r\"\"\"Collates a batch of data samples.\n",
    "\n",
    "        Args:\n",
    "            data (List): A list of data samples.\n",
    "\n",
    "        Returns:\n",
    "            List: A list of reprocessed data batches.\n",
    "        \"\"\"\n",
    "        data_size = len(data)\n",
    "\n",
    "        idxs = list(range(data_size))\n",
    "\n",
    "        # Initialize empty lists to store extracted values\n",
    "        empty_lists: List[List] = [[] for _ in range(4)]\n",
    "        (\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            audios,\n",
    "            speaker_ids,\n",
    "        ) = empty_lists\n",
    "\n",
    "        # Extract fields from data dictionary and populate the lists\n",
    "        for idx in idxs:\n",
    "            data_entry = data[idx]\n",
    "\n",
    "            mels.append(data_entry[\"mel\"])\n",
    "            mel_lens.append(data_entry[\"mel\"].shape[1])\n",
    "            audios.append(data_entry[\"audio\"])\n",
    "            speaker_ids.append(data_entry[\"speaker_id\"])\n",
    "\n",
    "        mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n",
    "        mel_lens = torch.tensor(mel_lens, dtype=torch.int64)\n",
    "        audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n",
    "        speaker_ids = torch.tensor(speaker_ids, dtype=torch.int64)\n",
    "\n",
    "        return [\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            audios,\n",
    "            speaker_ids,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dataset = LibriTTSDatasetVocoder(\n",
    "            root=\"datasets_cache/LIBRITTS\",\n",
    "            batch_size=batch_size,\n",
    "            download=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dataset) == 33236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[0]\n",
    "\n",
    "assert sample[\"mel\"].shape == torch.Size([100, 64])\n",
    "assert sample[\"audio\"].shape == torch.Size([16384])\n",
    "assert sample[\"speaker_id\"] == 1034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_74617/3836305582.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n",
      "/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_74617/3836305582.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    dataset[0],\n",
    "    dataset[2],\n",
    "]\n",
    "result = dataset.collate_fn(data)\n",
    "assert len(result) == 4\n",
    "for batch in result:\n",
    "    assert len(batch) == batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_74617/3836305582.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n",
      "/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_74617/3836305582.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collate_fn,\n",
    ")\n",
    "\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "for _, items in enumerate([next(dataloader_iter), next(dataloader_iter)]):\n",
    "    assert len(items) == 4\n",
    "    for it in items:\n",
    "        assert len(it) == batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
