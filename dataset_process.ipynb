{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_df = pd.read_csv(\n",
    "    \"./datasets_cache/LIBRITTS/LibriTTS/speakers.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"READER\", \"GENDER\", \"SUBSET\", \"NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Literal, Tuple, Union\n",
    "\n",
    "PreprocessLangType = Literal[\"english_only\", \"multilingual\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class STFTConfig:\n",
    "    filter_length: int\n",
    "    hop_length: int\n",
    "    win_length: int\n",
    "    n_mel_channels: int\n",
    "    mel_fmin: int\n",
    "    mel_fmax: int\n",
    "\n",
    "\n",
    "# Base class used with the Univnet vocoder\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    language: PreprocessLangType\n",
    "    stft: STFTConfig\n",
    "    sampling_rate: int = 22050\n",
    "    min_seconds: float = 0.5\n",
    "    max_seconds: float = 6.0\n",
    "    use_audio_normalization: bool = True\n",
    "    workers: int = 8\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfigUnivNet(PreprocessingConfig):\n",
    "    stft: STFTConfig = field(\n",
    "        default_factory=lambda: STFTConfig(\n",
    "            filter_length=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "            n_mel_channels=100,  # univnet\n",
    "            mel_fmin=20,\n",
    "            mel_fmax=11025,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LangItem:\n",
    "    r\"\"\"A class for storing language information.\"\"\"\n",
    "\n",
    "    phonemizer: str\n",
    "    phonemizer_espeak: str\n",
    "    nemo: str\n",
    "    processing_lang_type: PreprocessLangType\n",
    "    \n",
    "langs_map: dict[str, LangItem] = {\n",
    "    \"en\": LangItem(\n",
    "        phonemizer=\"en_us\",\n",
    "        phonemizer_espeak=\"en-us\",\n",
    "        nemo=\"en\",\n",
    "        processing_lang_type=\"english_only\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "def get_lang_map(lang: str) -> LangItem:\n",
    "    r\"\"\"Returns a LangItem object for the given language.\n",
    "\n",
    "    Args:\n",
    "        lang (str): The language to get the LangItem for.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the language is not supported.\n",
    "\n",
    "    Returns:\n",
    "        LangItem: The LangItem object for the given language.\n",
    "    \"\"\"\n",
    "    if lang not in langs_map:\n",
    "        raise ValueError(f\"Language {lang} is not supported!\")\n",
    "    return langs_map[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_2780/2474197086.py\", line 5, in <module>\n",
      "    import torchaudio\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/__init__.py\", line 2, in <module>\n",
      "    from . import _extension  # noqa  # usort: skip\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/_extension/__init__.py\", line 5, in <module>\n",
      "    from torchaudio._internal.module_utils import fail_with_message, is_module_available, no_op\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/_internal/__init__.py\", line 4, in <module>\n",
      "    from torch.hub import download_url_to_file, load_state_dict_from_url\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from unidecode import unidecode\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "class NormalizeText:\n",
    "    r\"\"\"NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new conversational AI models.\n",
    "\n",
    "    This class normalize the characters in the input text and normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "    Args:\n",
    "        lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "\n",
    "    Attributes:\n",
    "        lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "        model (Normalizer): The `nemo_text_processing` Normalizer model.\n",
    "\n",
    "    Methods:\n",
    "        byte_encode(word: str) -> list: Encode a word as a list of bytes.\n",
    "        normalize_chars(text: str) -> str: Normalize the characters in the input text.\n",
    "        __call__(text: str) -> str: Normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "    Examples:\n",
    "        >>> from training.preprocess.normilize_text import NormalizeText\n",
    "        >>> normilize_text = NormalizeText()\n",
    "        >>> normilize_text(\"It’s a beautiful day…\")\n",
    "        \"It's a beautiful day.\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lang: str = \"en\"):\n",
    "        r\"\"\"Initialize a new instance of the NormalizeText class.\n",
    "\n",
    "        Args:\n",
    "            lang (str): The language code to use for normalization. Defaults to \"en\".\n",
    "\n",
    "        \"\"\"\n",
    "        self.lang = lang\n",
    "        self.processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()\n",
    "        # self.model = Normalizer(input_case=\"cased\", lang=lang)\n",
    "\n",
    "    def byte_encode(self, word: str) -> list:\n",
    "        r\"\"\"Encode a word as a list of bytes.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to encode.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of bytes representing the encoded word.\n",
    "\n",
    "        \"\"\"\n",
    "        text = word.strip()\n",
    "        return list(text.encode(\"utf-8\"))\n",
    "\n",
    "    def normalize_chars(self, text: str) -> str:\n",
    "        r\"\"\"Normalize the characters in the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "\n",
    "        Examples:\n",
    "            >>> normalize_chars(\"It’s a beautiful day…\")\n",
    "            \"It's a beautiful day.\"\n",
    "\n",
    "        \"\"\"\n",
    "        # Define the character mapping\n",
    "        char_mapping = {\n",
    "            ord(\"’\"): ord(\"'\"),\n",
    "            ord(\"”\"): ord(\"'\"),\n",
    "            ord(\"…\"): ord(\".\"),\n",
    "            ord(\"„\"): ord(\"'\"),\n",
    "            ord(\"“\"): ord(\"'\"),\n",
    "            ord('\"'): ord(\"'\"),\n",
    "            ord(\"–\"): ord(\"-\"),\n",
    "            ord(\"—\"): ord(\"-\"),\n",
    "            ord(\"«\"): ord(\"'\"),\n",
    "            ord(\"»\"): ord(\"'\"),\n",
    "        }\n",
    "\n",
    "        # Add unicode normalization as additional garanty and normalize the characters using translate() method\n",
    "        normalized_string = unidecode(text).translate(char_mapping)\n",
    "\n",
    "        # Remove redundant multiple characters\n",
    "        # TODO: Maybe there is some effect on duplication?\n",
    "        return re.sub(r\"(\\.|\\!|\\?|\\-)\\1+\", r\"\\1\", normalized_string)\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        r\"\"\"Normalize the input text with the `nemo_text_processing`.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "\n",
    "        \"\"\"\n",
    "        text = self.normalize_chars(text)\n",
    "        # return self.model.normalize(text)\n",
    "\n",
    "        # Split the text into lines\n",
    "        # lines = text.split(\"\\n\")\n",
    "        processed, lengths = self.processor(text)\n",
    "        normalized_lines = [self.processor.tokens[i] for i in processed[0, : lengths[0]]]\n",
    "        # normalized_lines = self.model.normalize_list(lines)\n",
    "\n",
    "        # TODO: check this!\n",
    "        # Join the normalized lines, replace \\n with . and return the result\n",
    "        result = \". \".join(normalized_lines)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: for the backward comp.\n",
    "# Prepare the phonemes list and dictionary for the embedding\n",
    "phoneme_basic_symbols = [\n",
    "    # IPA symbols\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"æ\",\n",
    "    \"ç\",\n",
    "    \"ð\",\n",
    "    \"ø\",\n",
    "    \"ŋ\",\n",
    "    \"œ\",\n",
    "    \"ɐ\",\n",
    "    \"ɑ\",\n",
    "    \"ɔ\",\n",
    "    \"ə\",\n",
    "    \"ɛ\",\n",
    "    \"ɝ\",\n",
    "    \"ɹ\",\n",
    "    \"ɡ\",\n",
    "    \"ɪ\",\n",
    "    \"ʁ\",\n",
    "    \"ʃ\",\n",
    "    \"ʊ\",\n",
    "    \"ʌ\",\n",
    "    \"ʏ\",\n",
    "    \"ʒ\",\n",
    "    \"ʔ\",\n",
    "    \"ˈ\",\n",
    "    \"ˌ\",\n",
    "    \"ː\",\n",
    "    \"̃\",\n",
    "    \"̍\",\n",
    "    \"̥\",\n",
    "    \"̩\",\n",
    "    \"̯\",\n",
    "    \"͡\",\n",
    "    \"θ\",\n",
    "    # Punctuation\n",
    "    \"!\",\n",
    "    \"?\",\n",
    "    \",\",\n",
    "    \".\",\n",
    "    \"-\",\n",
    "    \":\",\n",
    "    \";\",\n",
    "    '\"',\n",
    "    \"'\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \" \",\n",
    "]\n",
    "\n",
    "# TODO: add support for other languages\n",
    "# _letters_accented = \"µßàáâäåæçèéêëìíîïñòóôöùúûüąćęłńœśşźżƒ\"\n",
    "# _letters_cyrilic = \"абвгдежзийклмнопрстуфхцчшщъыьэюяёєіїґӧ\"\n",
    "# _pad = \"$\"\n",
    "\n",
    "# This is the list of symbols from StyledTTS2\n",
    "_punctuation = ';:,.!?¡¿—…\"«»“”'\n",
    "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "_letters_ipa = \"ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁǂǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ\"\n",
    "\n",
    "# Combine all symbols\n",
    "symbols = list(_punctuation) + list(_letters) + list(_letters_ipa)\n",
    "\n",
    "# Add only unique symbols\n",
    "phones = phoneme_basic_symbols + [\n",
    "    symbol for symbol in symbols if symbol not in phoneme_basic_symbols\n",
    "]\n",
    "\n",
    "# TODO: Need to understand how to replace this\n",
    "# len(phones) == 184, leave it as is at this point\n",
    "symbols = [str(el) for el in range(256)]\n",
    "symbol2id = {s: i for i, s in enumerate(symbols)}\n",
    "id2symbol = {i: s for i, s in enumerate(symbols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import ERROR, Logger\n",
    "import os\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "# IPA Phonemizer: https://github.com/bootphon/phonemizer\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "\n",
    "# Create a Logger instance\n",
    "logger = Logger(\"my_logger\")\n",
    "# Set the level to ERROR\n",
    "logger.setLevel(ERROR)\n",
    "\n",
    "from dp.preprocessing.text import SequenceTokenizer\n",
    "\n",
    "# from models.config import get_lang_map\n",
    "# from models.config.symbols import phones\n",
    "\n",
    "# INFO: Fix for windows, used for local env\n",
    "if os.name == \"nt\":\n",
    "    ESPEAK_LIBRARY = os.getenv(\n",
    "        \"ESPEAK_LIBRARY\",\n",
    "        \"C:\\\\Program Files\\\\eSpeak NG\\\\libespeak-ng.dll\",\n",
    "    )\n",
    "    EspeakWrapper.set_library(ESPEAK_LIBRARY)\n",
    "\n",
    "\n",
    "class TokenizerIpaEspeak:\n",
    "    def __init__(self, lang: str = \"en\"):\n",
    "        lang_map = get_lang_map(lang)\n",
    "        self.lang = lang_map.phonemizer_espeak\n",
    "        self.lang_seq = lang_map.phonemizer\n",
    "\n",
    "        # NOTE: for backward compatibility with previous IPA tokenizer see the TokenizerIPA class\n",
    "        self.tokenizer = SequenceTokenizer(\n",
    "            phones,\n",
    "            languages=[\"de\", \"en_us\"],\n",
    "            lowercase=True,\n",
    "            char_repeats=1,\n",
    "            append_start_end=True,\n",
    "        )\n",
    "\n",
    "        self.phonemizer = EspeakBackend(\n",
    "            language=self.lang,\n",
    "            preserve_punctuation=True,\n",
    "            with_stress=True,\n",
    "            words_mismatch=\"ignore\",\n",
    "            logger=logger,\n",
    "        ).phonemize\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        r\"\"\"Converts the input text to phonemes and tokenizes them.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Union[str, List[str]], List[int]]: IPA phonemes and tokens.\n",
    "\n",
    "        \"\"\"\n",
    "        phones_ipa = \"\".join(self.phonemizer([text]))\n",
    "\n",
    "        tokens = self.tokenizer(phones_ipa, language=self.lang_seq)\n",
    "\n",
    "        return phones_ipa, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VocoderBasicConfig:\n",
    "    segment_size: int = 16384\n",
    "    learning_rate: float = 0.0001\n",
    "    adam_b1: float = 0.5\n",
    "    adam_b2: float = 0.9\n",
    "    lr_decay: float = 0.995\n",
    "    synth_interval: int = 250\n",
    "    checkpoint_interval: int = 250\n",
    "    stft_lamb: float = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class TacotronSTFT(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_length: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        n_mel_channels: int,\n",
    "        sampling_rate: int,\n",
    "        center: bool,\n",
    "        mel_fmax: Optional[int],\n",
    "        mel_fmin: float = 0.0,\n",
    "    ):\n",
    "        r\"\"\"TacotronSTFT module that computes mel-spectrograms from a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            filter_length (int): Length of the filter window.\n",
    "            hop_length (int): Number of samples between successive frames.\n",
    "            win_length (int): Size of the STFT window.\n",
    "            n_mel_channels (int): Number of mel bins.\n",
    "            sampling_rate (int): Sampling rate of the input waveforms.\n",
    "            mel_fmin (int or None): Minimum frequency for the mel filter bank.\n",
    "            mel_fmax (int or None): Maximum frequency for the mel filter bank.\n",
    "            center (bool): Whether to pad the input signal on both sides.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.n_fft = filter_length\n",
    "        self.hop_size = hop_length\n",
    "        self.win_size = win_length\n",
    "        self.fmin = mel_fmin\n",
    "        self.fmax = mel_fmax\n",
    "        self.center = center\n",
    "\n",
    "        # Define the mel filterbank\n",
    "        mel = librosa.filters.mel(\n",
    "            sr=sampling_rate,\n",
    "            n_fft=filter_length,\n",
    "            n_mels=n_mel_channels,\n",
    "            fmin=mel_fmin,\n",
    "            fmax=mel_fmax,\n",
    "        )\n",
    "\n",
    "        mel_basis = torch.tensor(mel, dtype=float).float()\n",
    "\n",
    "        # Define the Hann window\n",
    "        hann_window = torch.hann_window(win_length)\n",
    "\n",
    "        self.register_buffer(\"mel_basis\", mel_basis)\n",
    "        self.register_buffer(\"hann_window\", hann_window)\n",
    "\n",
    "    def _spectrogram(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computes the linear spectrogram of a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): Input waveforms.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Linear spectrogram.\n",
    "        \"\"\"\n",
    "        assert torch.min(y.data) >= -1\n",
    "        assert torch.max(y.data) <= 1\n",
    "\n",
    "        y = torch.nn.functional.pad(\n",
    "            y.unsqueeze(1),\n",
    "            (\n",
    "                int((self.n_fft - self.hop_size) / 2),\n",
    "                int((self.n_fft - self.hop_size) / 2),\n",
    "            ),\n",
    "            mode=\"reflect\",\n",
    "        )\n",
    "        y = y.squeeze(1)\n",
    "        spec = torch.stft(\n",
    "            y,\n",
    "            self.n_fft,\n",
    "            hop_length=self.hop_size,\n",
    "            win_length=self.win_size,\n",
    "            window=self.hann_window,  # type: ignore\n",
    "            center=self.center,\n",
    "            pad_mode=\"reflect\",\n",
    "            normalized=False,\n",
    "            onesided=True,\n",
    "            return_complex=True,\n",
    "        )\n",
    "        return torch.view_as_real(spec)\n",
    "\n",
    "    def linear_spectrogram(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computes the linear spectrogram of a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): Input waveforms.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Linear spectrogram.\n",
    "        \"\"\"\n",
    "        spec = self._spectrogram(y)\n",
    "        return torch.norm(spec, p=2, dim=-1)\n",
    "\n",
    "    def forward(self, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Computes mel-spectrograms from a batch of waves.\n",
    "\n",
    "        Args:\n",
    "            y (torch.FloatTensor): Input waveforms with shape (B, T) in range [-1, 1]\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Spectrogram of shape (B, n_spech_channels, T)\n",
    "            torch.FloatTensor: Mel-spectrogram of shape (B, n_mel_channels, T)\n",
    "        \"\"\"\n",
    "        spec = self._spectrogram(y)\n",
    "\n",
    "        spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n",
    "\n",
    "        mel = torch.matmul(self.mel_basis, spec)  # type: ignore\n",
    "        mel = self.spectral_normalize_torch(mel)\n",
    "\n",
    "        return spec, mel\n",
    "\n",
    "    def spectral_normalize_torch(self, magnitudes: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Applies dynamic range compression to magnitudes.\n",
    "\n",
    "        Args:\n",
    "            magnitudes (torch.Tensor): Input magnitudes.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output magnitudes.\n",
    "        \"\"\"\n",
    "        return self.dynamic_range_compression_torch(magnitudes)\n",
    "\n",
    "    def dynamic_range_compression_torch(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        C: int = 1,\n",
    "        clip_val: float = 1e-5,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Applies dynamic range compression to x.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            C (float): Compression factor.\n",
    "            clip_val (float): Clipping value.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "    # NOTE: audio np.ndarray changed to torch.FloatTensor!\n",
    "    def get_mel_from_wav(self, audio: torch.Tensor) -> torch.Tensor:\n",
    "        audio_tensor = audio.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, melspec = self.forward(audio_tensor)\n",
    "        return melspec.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa.filters import mel as librosa_mel_fn\n",
    "import torch\n",
    "\n",
    "\n",
    "class AudioProcessor:\n",
    "    r\"\"\"A class used to process audio signals and convert them into different representations.\n",
    "\n",
    "    Attributes:\n",
    "        hann_window (dict): A dictionary to store the Hann window for different configurations.\n",
    "        mel_basis (dict): A dictionary to store the Mel basis for different configurations.\n",
    "\n",
    "    Methods:\n",
    "        name_mel_basis(spec, n_fft, fmax): Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n",
    "        amp_to_db(magnitudes, C=1, clip_val=1e-5): Convert amplitude to decibels (dB).\n",
    "        db_to_amp(magnitudes, C=1): Convert decibels (dB) to amplitude.\n",
    "        wav_to_spec(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the magnitude.\n",
    "        wav_to_energy(y, n_fft, hop_length, win_length, center=False): Convert a waveform to a spectrogram and compute the energy.\n",
    "        spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax): Convert a spectrogram to a Mel spectrogram.\n",
    "        wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False): Convert a waveform to a Mel spectrogram.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hann_window = {}\n",
    "        self.mel_basis = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def name_mel_basis(spec: torch.Tensor, n_fft: int, fmax: int) -> str:\n",
    "        \"\"\"Generate a name for the Mel basis based on the FFT size, maximum frequency, data type, and device.\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): The spectrogram tensor.\n",
    "            n_fft (int): The FFT size.\n",
    "            fmax (int): The maximum frequency.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated name for the Mel basis.\n",
    "        \"\"\"\n",
    "        n_fft_len = f\"{n_fft}_{fmax}_{spec.dtype}_{spec.device}\"\n",
    "        return n_fft_len\n",
    "\n",
    "    @staticmethod\n",
    "    def amp_to_db(magnitudes: torch.Tensor, C: int = 1, clip_val: float = 1e-5) -> torch.Tensor:\n",
    "        r\"\"\"Convert amplitude to decibels (dB).\n",
    "\n",
    "        Args:\n",
    "            magnitudes (Tensor): The amplitude magnitudes to convert.\n",
    "            C (int, optional): A constant value used in the conversion. Defaults to 1.\n",
    "            clip_val (float, optional): A value to clamp the magnitudes to avoid taking the log of zero. Defaults to 1e-5.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The converted magnitudes in dB.\n",
    "        \"\"\"\n",
    "        return torch.log(torch.clamp(magnitudes, min=clip_val) * C)\n",
    "\n",
    "    @staticmethod\n",
    "    def db_to_amp(magnitudes: torch.Tensor, C: int = 1) -> torch.Tensor:\n",
    "        r\"\"\"Convert decibels (dB) to amplitude.\n",
    "\n",
    "        Args:\n",
    "            magnitudes (Tensor): The dB magnitudes to convert.\n",
    "            C (int, optional): A constant value used in the conversion. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The converted magnitudes in amplitude.\n",
    "        \"\"\"\n",
    "        return torch.exp(magnitudes) / C\n",
    "\n",
    "    def wav_to_spec(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a spectrogram and compute the magnitude.\n",
    "\n",
    "        Args:\n",
    "            y (Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The magnitude of the computed spectrogram.\n",
    "        \"\"\"\n",
    "        y = y.squeeze(1)\n",
    "\n",
    "        dtype_device = str(y.dtype) + \"_\" + str(y.device)\n",
    "        wnsize_dtype_device = str(win_length) + \"_\" + dtype_device\n",
    "        if wnsize_dtype_device not in self.hann_window:\n",
    "            self.hann_window[wnsize_dtype_device] = torch.hann_window(win_length).to(dtype=y.dtype, device=y.device)\n",
    "\n",
    "        y = torch.nn.functional.pad(\n",
    "            y.unsqueeze(1),\n",
    "            (int((n_fft - hop_length) / 2), int((n_fft - hop_length) / 2)),\n",
    "            mode=\"reflect\",\n",
    "        )\n",
    "        y = y.squeeze(1)\n",
    "\n",
    "        spec = torch.stft(\n",
    "            y,\n",
    "            n_fft,\n",
    "            hop_length=hop_length,\n",
    "            win_length=win_length,\n",
    "            window=self.hann_window[wnsize_dtype_device],\n",
    "            center=center,\n",
    "            pad_mode=\"reflect\",\n",
    "            normalized=False,\n",
    "            onesided=True,\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        spec = torch.view_as_real(spec)\n",
    "\n",
    "        # Compute the magnitude\n",
    "        spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def wav_to_energy(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a spectrogram and compute the energy.\n",
    "\n",
    "        Args:\n",
    "            y (Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The energy of the computed spectrogram.\n",
    "        \"\"\"\n",
    "        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n",
    "        spec = torch.norm(spec, dim=1, keepdim=True).squeeze(0)\n",
    "\n",
    "        # Normalize the energy\n",
    "        return (spec - spec.mean()) / spec.std()\n",
    "\n",
    "    def spec_to_mel(\n",
    "            self,\n",
    "            spec: torch.Tensor,\n",
    "            n_fft: int,\n",
    "            num_mels: int,\n",
    "            sample_rate: int,\n",
    "            fmin: int,\n",
    "            fmax: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a spectrogram to a Mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            spec (torch.Tensor): The input spectrogram of shape [B, C, T].\n",
    "            n_fft (int): The FFT size.\n",
    "            num_mels (int): The number of Mel bands.\n",
    "            sample_rate (int): The sample rate of the audio.\n",
    "            fmin (int): The minimum frequency.\n",
    "            fmax (int): The maximum frequency.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed Mel spectrogram of shape [B, C, T].\n",
    "        \"\"\"\n",
    "        mel_basis_key = self.name_mel_basis(spec, n_fft, fmax)\n",
    "\n",
    "        if mel_basis_key not in self.mel_basis:\n",
    "            mel = librosa_mel_fn(sr=sample_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
    "            self.mel_basis[mel_basis_key] = torch.tensor(mel).to(dtype=spec.dtype, device=spec.device)\n",
    "\n",
    "        mel = torch.matmul(self.mel_basis[mel_basis_key], spec)\n",
    "        mel = self.amp_to_db(mel)\n",
    "\n",
    "        return mel\n",
    "\n",
    "    def wav_to_mel(\n",
    "        self,\n",
    "        y: torch.Tensor,\n",
    "        n_fft: int,\n",
    "        num_mels: int,\n",
    "        sample_rate: int,\n",
    "        hop_length: int,\n",
    "        win_length: int,\n",
    "        fmin: int,\n",
    "        fmax: int,\n",
    "        center: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert a waveform to a Mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            y (torch.Tensor): The input waveform.\n",
    "            n_fft (int): The FFT size.\n",
    "            num_mels (int): The number of Mel bands.\n",
    "            sample_rate (int): The sample rate of the audio.\n",
    "            hop_length (int): The hop (stride) size.\n",
    "            win_length (int): The window size.\n",
    "            fmin (int): The minimum frequency.\n",
    "            fmax (int): The maximum frequency.\n",
    "            center (bool, optional): Whether to pad `y` such that frames are centered. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed Mel spectrogram.\n",
    "        \"\"\"\n",
    "        # Convert the waveform to a spectrogram\n",
    "        spec = self.wav_to_spec(y, n_fft, hop_length, win_length, center=center)\n",
    "\n",
    "        # Convert the spectrogram to a Mel spectrogram\n",
    "        mel = self.spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax)\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "def stereo_to_mono(audio: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Converts a stereo audio tensor to mono by taking the mean across channels.\n",
    "\n",
    "    Args:\n",
    "        audio (torch.Tensor): Input audio tensor of shape (channels, samples).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mono audio tensor of shape (1, samples).\n",
    "    \"\"\"\n",
    "    return torch.mean(audio, 0, True)\n",
    "\n",
    "\n",
    "def resample(wav: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
    "    r\"\"\"Resamples an audio waveform from the original sampling rate to the target sampling rate.\n",
    "\n",
    "    Args:\n",
    "        wav (np.ndarray): The audio waveform to be resampled.\n",
    "        orig_sr (int): The original sampling rate of the audio waveform.\n",
    "        target_sr (int): The target sampling rate to resample the audio waveform to.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resampled audio waveform.\n",
    "    \"\"\"\n",
    "    return librosa.resample(wav, orig_sr=orig_sr, target_sr=target_sr)\n",
    "\n",
    "\n",
    "def safe_load(path: str, sr: Union[int, None]) -> Tuple[np.ndarray, int]:\n",
    "    r\"\"\"Load an audio file from disk and return its content as a numpy array.\n",
    "\n",
    "    Args:\n",
    "        path (str): The path to the audio file.\n",
    "        sr (int or None): The target sampling rate. If None, the original sampling rate is used.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, int]: A tuple containing the audio content as a numpy array and the actual sampling rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        audio, sr_actual = torchaudio.load(path) # type: ignore\n",
    "        if audio.shape[0] > 0:\n",
    "            audio = stereo_to_mono(audio)\n",
    "        audio = audio.squeeze(0)\n",
    "        if sr_actual != sr and sr is not None:\n",
    "            audio = resample(np.array(audio), orig_sr=sr_actual, target_sr=sr)\n",
    "            # audio = resample(audio.numpy(), orig_sr=sr_actual, target_sr=sr)\n",
    "            sr_actual = sr\n",
    "        else:\n",
    "            audio = np.array(audio)\n",
    "            # .numpy()\n",
    "            # audio = audio.numpy()\n",
    "    except Exception as e:\n",
    "        raise type(e)(\n",
    "            f\"The following error happened loading the file {path} ... \\n\" + str(e),\n",
    "        ).with_traceback(sys.exc_info()[2])\n",
    "\n",
    "    return audio, sr_actual\n",
    "\n",
    "\n",
    "def preprocess_audio(\n",
    "    audio: torch.Tensor, sr_actual: int, sr: Union[int, None],\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    r\"\"\"Preprocesses audio by converting stereo to mono, resampling if necessary, and returning the audio tensor and sample rate.\n",
    "\n",
    "    Args:\n",
    "        audio (torch.Tensor): The audio tensor to preprocess.\n",
    "        sr_actual (int): The actual sample rate of the audio.\n",
    "        sr (Union[int, None]): The target sample rate to resample the audio to, if necessary.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, int]: The preprocessed audio tensor and sample rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if audio.shape[0] > 0:\n",
    "            audio = stereo_to_mono(audio)\n",
    "        audio = audio.squeeze(0)\n",
    "        if sr_actual != sr and sr is not None:\n",
    "            if isinstance(audio, torch.Tensor):\n",
    "                detach = audio.data.detach().tolist()\n",
    "                \n",
    "                audio = np.array(detach, dtype=float)\n",
    "\n",
    "            # audio\n",
    "            audio_np = resample(audio, orig_sr=sr_actual, target_sr=sr)\n",
    "            # Convert back to torch tensor\n",
    "            audio = torch.tensor(audio_np)\n",
    "            sr_actual = sr\n",
    "    except Exception as e:\n",
    "        raise type(e)(\n",
    "            f\"The following error happened while processing the audio ... \\n {e!s}\",\n",
    "        ).with_traceback(sys.exc_info()[2])\n",
    "\n",
    "    return audio, sr_actual\n",
    "\n",
    "\n",
    "def normalize_loudness(wav: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Normalize the loudness of an audio waveform.\n",
    "\n",
    "    Args:\n",
    "        wav (torch.Tensor): The input waveform.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The normalized waveform.\n",
    "\n",
    "    Examples:\n",
    "        >>> wav = np.array([1.0, 2.0, 3.0])\n",
    "        >>> normalize_loudness(wav)\n",
    "        tensor([0.33333333, 0.66666667, 1.  ])\n",
    "    \"\"\"\n",
    "    return wav / torch.max(torch.abs(wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: LOOK AT THIS ESTIMATION ALGO\n",
    "#######################################################################################\n",
    "# Original implementation from https://github.com/NVIDIA/mellotron/blob/master/yin.py #\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def differenceFunction(x: np.ndarray, N: int, tau_max: int) -> np.ndarray:\n",
    "    r\"\"\"Compute the difference function of an audio signal.\n",
    "\n",
    "    This function computes the difference function of an audio signal `x` using the algorithm described in equation (6) of [1]. The difference function is a measure of the similarity between the signal and a time-shifted version of itself, and is commonly used in pitch detection algorithms.\n",
    "\n",
    "    This implementation uses the NumPy FFT functions to compute the difference function efficiently.\n",
    "\n",
    "    Parameters\n",
    "        x (np.ndarray): The audio signal to compute the difference function for.\n",
    "        N (int): The length of the audio signal.\n",
    "        tau_max (int): The maximum integration window size to use.\n",
    "\n",
    "    Returns\n",
    "        np.ndarray: The difference function of the audio signal.\n",
    "\n",
    "    References\n",
    "        [1] A. de Cheveigné and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n",
    "    \"\"\"\n",
    "    x = np.array(x, np.float64)\n",
    "    w = x.size\n",
    "    tau_max = min(tau_max, w)\n",
    "    x_cumsum = np.concatenate((np.array([0.0]), (x * x).cumsum()))\n",
    "    size = w + tau_max\n",
    "    p2 = (size // 32).bit_length()\n",
    "    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n",
    "    size_pad = min(x * 2**p2 for x in nice_numbers if x * 2**p2 >= size)\n",
    "    fc = np.fft.rfft(x, size_pad)\n",
    "    conv = np.fft.irfft(fc * fc.conjugate())[:tau_max]\n",
    "    return x_cumsum[w : w - tau_max : -1] + x_cumsum[w] - x_cumsum[:tau_max] - 2 * conv\n",
    "\n",
    "\n",
    "def cumulativeMeanNormalizedDifferenceFunction(df: np.ndarray, N: int) -> np.ndarray:\n",
    "    r\"\"\"Compute the cumulative mean normalized difference function (CMND) of a difference function.\n",
    "\n",
    "    The CMND is defined as the element-wise product of the difference function with a range of values from 1 to N-1,\n",
    "    divided by the cumulative sum of the difference function up to that point, plus a small epsilon value to avoid\n",
    "    division by zero. The first element of the CMND is set to 1.\n",
    "\n",
    "    Args:\n",
    "        df (np.ndarray): The difference function.\n",
    "        N (int): The length of the data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The cumulative mean normalized difference function.\n",
    "\n",
    "    References:\n",
    "        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n",
    "            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n",
    "    \"\"\"\n",
    "    cmndf = (\n",
    "        df[1:] * range(1, N) / (np.cumsum(df[1:]).astype(float) + 1e-8)\n",
    "    )  # scipy method\n",
    "    return np.insert(cmndf, 0, 1)\n",
    "\n",
    "\n",
    "def getPitch(cmdf: np.ndarray, tau_min: int, tau_max: int, harmo_th: float=0.1) -> int:\n",
    "    r\"\"\"Compute the fundamental period of a frame based on the Cumulative Mean Normalized Difference function (CMND).\n",
    "\n",
    "    The CMND is a measure of the periodicity of a signal, and is computed as the cumulative mean normalized difference\n",
    "    function of the difference function of the signal. The fundamental period is the first value of the index `tau`\n",
    "    between `tau_min` and `tau_max` where the CMND is below the `harmo_th` threshold. If there are no such values, the\n",
    "    function returns 0 to indicate that the signal is unvoiced.\n",
    "\n",
    "    Args:\n",
    "        cmdf (np.ndarray): The Cumulative Mean Normalized Difference function of the signal.\n",
    "        tau_min (int): The minimum period for speech.\n",
    "        tau_max (int): The maximum period for speech.\n",
    "        harmo_th (float, optional): The harmonicity threshold to determine if it is necessary to compute pitch\n",
    "            frequency. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        int: The fundamental period of the signal, or 0 if the signal is unvoiced.\n",
    "\n",
    "    References:\n",
    "        [1] K. K. Paliwal and R. P. Sharma, \"A robust algorithm for pitch detection in noisy speech signals,\"\n",
    "            Speech Communication, vol. 12, no. 3, pp. 249-263, 1993.\n",
    "    \"\"\"\n",
    "    tau = tau_min\n",
    "    while tau < tau_max:\n",
    "        if cmdf[tau] < harmo_th:\n",
    "            while tau + 1 < tau_max and cmdf[tau + 1] < cmdf[tau]:\n",
    "                tau += 1\n",
    "            return tau\n",
    "        tau += 1\n",
    "\n",
    "    return 0  # if unvoiced\n",
    "\n",
    "\n",
    "def compute_yin(\n",
    "    sig_torch: torch.Tensor,\n",
    "    sr: int,\n",
    "    w_len: int = 512,\n",
    "    w_step: int = 256,\n",
    "    f0_min: int = 100,\n",
    "    f0_max: int = 500,\n",
    "    harmo_thresh: float = 0.1,\n",
    ") -> Tuple[np.ndarray, List[float], List[float], List[float]]:\n",
    "    r\"\"\"Compute the Yin Algorithm for pitch detection on an audio signal.\n",
    "\n",
    "    The Yin Algorithm is a widely used method for pitch detection in speech and music signals. It works by computing the\n",
    "    Cumulative Mean Normalized Difference function (CMND) of the difference function of the signal, and finding the first\n",
    "    minimum of the CMND below a given threshold. The fundamental period of the signal is then estimated as the inverse of\n",
    "    the lag corresponding to this minimum.\n",
    "\n",
    "    Args:\n",
    "        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n",
    "        sr (int): The sampling rate of the signal.\n",
    "        w_len (int, optional): The size of the analysis window in samples. Defaults to 512.\n",
    "        w_step (int, optional): The size of the lag between two consecutive windows in samples. Defaults to 256.\n",
    "        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz. Defaults to 100.\n",
    "        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz. Defaults to 500.\n",
    "        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND\n",
    "            function below this threshold. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, List[float], List[float], List[float]]: A tuple containing the following elements:\n",
    "            * pitches (np.ndarray): A 1D numpy array of fundamental frequencies estimated for each analysis window.\n",
    "            * harmonic_rates (List[float]): A list of harmonic rate values for each fundamental frequency value, which\n",
    "              can be interpreted as a confidence value.\n",
    "            * argmins (List[float]): A list of the minimums of the Cumulative Mean Normalized Difference Function for\n",
    "              each analysis window.\n",
    "            * times (List[float]): A list of the time of each estimation, in seconds.\n",
    "\n",
    "    References:\n",
    "        [1] A. K. Jain, Fundamentals of Digital Image Processing, Prentice Hall, 1989.\n",
    "        [2] A. de Cheveigné and H. Kawahara, \"YIN, a fundamental frequency estimator for speech and music,\" The Journal\n",
    "            of the Acoustical Society of America, vol. 111, no. 4, pp. 1917-1930, 2002.\n",
    "    \"\"\"\n",
    "    sig_torch = sig_torch.view(1, 1, -1)\n",
    "    sig_torch = F.pad(\n",
    "        sig_torch.unsqueeze(1),\n",
    "        (int((w_len - w_step) / 2), int((w_len - w_step) / 2), 0, 0),\n",
    "        mode=\"reflect\",\n",
    "    )\n",
    "\n",
    "    sig_torch_n: np.ndarray = np.array(sig_torch.view(-1).tolist())\n",
    "\n",
    "    # sig_torch_n: np.ndarray = sig_torch.view(-1).numpy()\n",
    "\n",
    "    tau_min = int(sr / f0_max)\n",
    "    tau_max = int(sr / f0_min)\n",
    "\n",
    "    timeScale = range(\n",
    "        0, len(sig_torch_n) - w_len, w_step,\n",
    "    )  # time values for each analysis window\n",
    "    times = [t / float(sr) for t in timeScale]\n",
    "    frames = [sig_torch_n[t : t + w_len] for t in timeScale]\n",
    "\n",
    "    pitches = [0.0] * len(timeScale)\n",
    "    harmonic_rates = [0.0] * len(timeScale)\n",
    "    argmins = [0.0] * len(timeScale)\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        # Compute YIN\n",
    "        df = differenceFunction(frame, w_len, tau_max)\n",
    "        cmdf = cumulativeMeanNormalizedDifferenceFunction(df, tau_max)\n",
    "        p = getPitch(cmdf, tau_min, tau_max, harmo_thresh)\n",
    "\n",
    "        # Get results\n",
    "        if np.argmin(cmdf) > tau_min:\n",
    "            argmins[i] = float(sr / np.argmin(cmdf))\n",
    "        if p != 0:  # A pitch was found\n",
    "            pitches[i] = float(sr / p)\n",
    "            harmonic_rates[i] = cmdf[p]\n",
    "        else:  # No pitch, but we compute a value of the harmonic rate\n",
    "            harmonic_rates[i] = min(cmdf)\n",
    "\n",
    "    return np.array(pitches), harmonic_rates, argmins, times\n",
    "\n",
    "\n",
    "def norm_interp_f0(f0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    r\"\"\"Normalize and interpolate the fundamental frequency (f0) values.\n",
    "\n",
    "    Args:\n",
    "        f0 (np.ndarray): The input f0 values.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: A tuple containing the normalized f0 values and a boolean array indicating which values were interpolated.\n",
    "\n",
    "    Examples:\n",
    "        >>> f0 = np.array([0, 100, 0, 200, 0])\n",
    "        >>> norm_interp_f0(f0)\n",
    "        (\n",
    "            np.array([100, 100, 150, 200, 200]),\n",
    "            np.array([True, False, True, False, True]),\n",
    "        )\n",
    "    \"\"\"\n",
    "    uv: np.ndarray = f0 == 0\n",
    "    if sum(uv) == len(f0):\n",
    "        f0[uv] = 0\n",
    "    elif sum(uv) > 0:\n",
    "        f0[uv] = np.interp(np.where(uv)[0], np.where(~uv)[0], f0[~uv])\n",
    "    return f0, uv\n",
    "\n",
    "\n",
    "def compute_pitch(\n",
    "    sig_torch: torch.Tensor,\n",
    "    sr: int,\n",
    "    w_len: int = 1024,\n",
    "    w_step: int = 256,\n",
    "    f0_min: int = 50,\n",
    "    f0_max: int = 1000,\n",
    "    harmo_thresh: float = 0.25,\n",
    "):\n",
    "    r\"\"\"Compute the pitch of an audio signal using the Yin algorithm.\n",
    "\n",
    "    The Yin algorithm is a widely used method for pitch detection in speech and music signals. This function uses the\n",
    "    Yin algorithm to compute the pitch of the input audio signal, and then normalizes and interpolates the pitch values.\n",
    "    Returns the normalized and interpolated pitch values.\n",
    "\n",
    "    Args:\n",
    "        sig_torch (torch.Tensor): The audio signal as a 1D numpy array of floats.\n",
    "        sr (int): The sampling rate of the signal.\n",
    "        w_len (int, optional): The size of the analysis window in samples.\n",
    "        w_step (int, optional): The size of the lag between two consecutive windows in samples.\n",
    "        f0_min (int, optional): The minimum fundamental frequency that can be detected in Hz.\n",
    "        f0_max (int, optional): The maximum fundamental frequency that can be detected in Hz.\n",
    "        harmo_thresh (float, optional): The threshold of detection. The algorithm returns the first minimum of the CMND function below this threshold.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized and interpolated pitch values of the audio signal.\n",
    "    \"\"\"\n",
    "    pitch, _, _, _ = compute_yin(\n",
    "        sig_torch,\n",
    "        sr=sr,\n",
    "        w_len=w_len,\n",
    "        w_step=w_step,\n",
    "        f0_min=f0_min,\n",
    "        f0_max=f0_max,\n",
    "        harmo_thresh=harmo_thresh,\n",
    "    )\n",
    "\n",
    "    pitch, _ = norm_interp_f0(pitch)\n",
    "\n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import random\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import betabinom\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from models.config import PreprocessingConfig, VocoderBasicConfig, get_lang_map\n",
    "\n",
    "# from .audio import normalize_loudness, preprocess_audio\n",
    "# from .audio_processor import AudioProcessor\n",
    "# from .compute_yin import compute_yin, norm_interp_f0\n",
    "# from .normalize_text import NormalizeText\n",
    "# from .tacotron_stft import TacotronSTFT\n",
    "# from .tokenizer_ipa_espeak import TokenizerIpaEspeak as TokenizerIPA\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessForAcousticResult:\n",
    "    wav: torch.Tensor\n",
    "    mel: torch.Tensor\n",
    "    pitch: torch.Tensor\n",
    "    phones_ipa: Union[str, List[str]]\n",
    "    phones: torch.Tensor\n",
    "    attn_prior: torch.Tensor\n",
    "    energy: torch.Tensor\n",
    "    raw_text: str\n",
    "    normalized_text: str\n",
    "    speaker_id: int\n",
    "    chapter_id: str | int\n",
    "    utterance_id: str\n",
    "    pitch_is_normalized: bool\n",
    "\n",
    "\n",
    "class PreprocessLibriTTS:\n",
    "    r\"\"\"Preprocessing PreprocessLibriTTS audio and text data for use with a TacotronSTFT model.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): The preprocessing configuration.\n",
    "        lang (str): The language of the input text.\n",
    "\n",
    "    Attributes:\n",
    "        min_seconds (float): The minimum duration of audio clips in seconds.\n",
    "        max_seconds (float): The maximum duration of audio clips in seconds.\n",
    "        hop_length (int): The hop length of the STFT.\n",
    "        sampling_rate (int): The sampling rate of the audio.\n",
    "        use_audio_normalization (bool): Whether to normalize the loudness of the audio.\n",
    "        tacotronSTFT (TacotronSTFT): The TacotronSTFT object used for computing mel spectrograms.\n",
    "        min_samples (int): The minimum number of audio samples in a clip.\n",
    "        max_samples (int): The maximum number of audio samples in a clip.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        lang: str = \"en\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        lang_map = get_lang_map(lang)\n",
    "\n",
    "        self.phonemizer_lang = lang_map.phonemizer\n",
    "        normilize_text_lang = lang_map.nemo\n",
    "\n",
    "        self.normilize_text = NormalizeText(normilize_text_lang)\n",
    "        self.tokenizer = TokenizerIpaEspeak(lang)\n",
    "        self.vocoder_train_config = VocoderBasicConfig()\n",
    "\n",
    "        self.preprocess_config = preprocess_config\n",
    "\n",
    "        self.sampling_rate = self.preprocess_config.sampling_rate\n",
    "        self.use_audio_normalization = self.preprocess_config.use_audio_normalization\n",
    "\n",
    "        self.hop_length = self.preprocess_config.stft.hop_length\n",
    "        self.filter_length = self.preprocess_config.stft.filter_length\n",
    "        self.mel_fmin = self.preprocess_config.stft.mel_fmin\n",
    "        self.win_length = self.preprocess_config.stft.win_length\n",
    "\n",
    "        self.tacotronSTFT = TacotronSTFT(\n",
    "            filter_length=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.preprocess_config.stft.win_length,\n",
    "            n_mel_channels=self.preprocess_config.stft.n_mel_channels,\n",
    "            sampling_rate=self.sampling_rate,\n",
    "            mel_fmin=self.mel_fmin,\n",
    "            mel_fmax=self.preprocess_config.stft.mel_fmax,\n",
    "            center=False,\n",
    "        )\n",
    "\n",
    "        min_seconds, max_seconds = (\n",
    "            self.preprocess_config.min_seconds,\n",
    "            self.preprocess_config.max_seconds,\n",
    "        )\n",
    "\n",
    "        self.min_samples = int(self.sampling_rate * min_seconds)\n",
    "        self.max_samples = int(self.sampling_rate * max_seconds)\n",
    "\n",
    "        self.audio_processor = AudioProcessor()\n",
    "\n",
    "    def beta_binomial_prior_distribution(\n",
    "        self,\n",
    "        phoneme_count: int,\n",
    "        mel_count: int,\n",
    "        scaling_factor: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Computes the beta-binomial prior distribution for the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            phoneme_count (int): Number of phonemes in the input text.\n",
    "            mel_count (int): Number of mel frames in the input mel-spectrogram.\n",
    "            scaling_factor (float, optional): Scaling factor for the beta distribution. Defaults to 1.0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A 2D tensor containing the prior distribution.\n",
    "        \"\"\"\n",
    "        P, M = phoneme_count, mel_count\n",
    "        x = np.arange(0, P)\n",
    "        mel_text_probs = []\n",
    "        for i in range(1, M + 1):\n",
    "            a, b = scaling_factor * i, scaling_factor * (M + 1 - i)\n",
    "            rv: Any = betabinom(P, a, b)\n",
    "            mel_i_prob = rv.pmf(x)\n",
    "            mel_text_probs.append(mel_i_prob)\n",
    "        return torch.tensor(np.array(mel_text_probs))\n",
    "\n",
    "    def acoustic(\n",
    "        self,\n",
    "        row: Tuple[torch.Tensor, int, str, str, int, str | int, str],\n",
    "    ) -> Union[None, PreprocessForAcousticResult]:\n",
    "        r\"\"\"Preprocesses audio and text data for use with a TacotronSTFT model.\n",
    "\n",
    "        Args:\n",
    "            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the preprocessed audio and text data.\n",
    "\n",
    "        Examples:\n",
    "            >>> preprocess_audio = PreprocessAudio(\"english_only\")\n",
    "            >>> audio = torch.randn(1, 44100)\n",
    "            >>> sr_actual = 44100\n",
    "            >>> raw_text = \"Hello, world!\"\n",
    "            >>> output = preprocess_audio(audio, sr_actual, raw_text)\n",
    "            >>> output.keys()\n",
    "            dict_keys(['wav', 'mel', 'pitch', 'phones', 'raw_text', 'normalized_text', 'speaker_id', 'chapter_id', 'utterance_id', 'pitch_is_normalized'])\n",
    "        \"\"\"\n",
    "        (\n",
    "            audio,\n",
    "            sr_actual,\n",
    "            raw_text,\n",
    "            normalized_text,\n",
    "            speaker_id,\n",
    "            chapter_id,\n",
    "            utterance_id,\n",
    "        ) = row\n",
    "\n",
    "        wav, sampling_rate = preprocess_audio(audio, sr_actual, self.sampling_rate)\n",
    "\n",
    "        # TODO: check this, maybe you need to move it to some other place\n",
    "        # TODO: maybe we can increate the max_samples ?\n",
    "        # if wav.shape[0] < self.min_samples or wav.shape[0] > self.max_samples:\n",
    "        #     return None\n",
    "\n",
    "        if self.use_audio_normalization:\n",
    "            wav = normalize_loudness(wav)\n",
    "\n",
    "        normalized_text = self.normilize_text(normalized_text)\n",
    "\n",
    "        # NOTE: fixed version of tokenizer with punctuation\n",
    "        phones_ipa, phones = self.tokenizer(normalized_text)\n",
    "\n",
    "        # Convert to tensor\n",
    "        phones = torch.Tensor(phones)\n",
    "\n",
    "        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n",
    "\n",
    "        # Skipping small sample due to the mel-spectrogram containing less than self.mel_fmin frames\n",
    "        # if mel_spectrogram.shape[1] < self.mel_fmin:\n",
    "        #     return None\n",
    "\n",
    "        # Text is longer than mel, will be skipped due to monotonic alignment search\n",
    "        if phones.shape[0] >= mel_spectrogram.shape[1]:\n",
    "            return None\n",
    "\n",
    "        pitch, _, _, _ = compute_yin(\n",
    "            wav,\n",
    "            sr=sampling_rate,\n",
    "            w_len=self.filter_length,\n",
    "            w_step=self.hop_length,\n",
    "            f0_min=50,\n",
    "            f0_max=1000,\n",
    "            harmo_thresh=0.25,\n",
    "        )\n",
    "\n",
    "        pitch, _ = norm_interp_f0(pitch)\n",
    "\n",
    "        if np.sum(pitch != 0) <= 1:\n",
    "            return None\n",
    "\n",
    "        pitch = torch.tensor(pitch)\n",
    "\n",
    "        # TODO this shouldnt be necessary, currently pitch sometimes has 1 less frame than spectrogram,\n",
    "        # We should find out why\n",
    "        mel_spectrogram = mel_spectrogram[:, : pitch.shape[0]]\n",
    "\n",
    "        attn_prior = self.beta_binomial_prior_distribution(\n",
    "            phones.shape[0],\n",
    "            mel_spectrogram.shape[1],\n",
    "        ).T\n",
    "\n",
    "        assert pitch.shape[0] == mel_spectrogram.shape[1], (\n",
    "            pitch.shape,\n",
    "            mel_spectrogram.shape[1],\n",
    "        )\n",
    "\n",
    "        energy = self.audio_processor.wav_to_energy(\n",
    "            wav.unsqueeze(0),\n",
    "            self.filter_length,\n",
    "            self.hop_length,\n",
    "            self.win_length,\n",
    "        )\n",
    "\n",
    "        return PreprocessForAcousticResult(\n",
    "            wav=wav,\n",
    "            mel=mel_spectrogram,\n",
    "            pitch=pitch,\n",
    "            attn_prior=attn_prior,\n",
    "            energy=energy,\n",
    "            phones_ipa=phones_ipa,\n",
    "            phones=phones,\n",
    "            raw_text=raw_text,\n",
    "            normalized_text=normalized_text,\n",
    "            speaker_id=speaker_id,\n",
    "            chapter_id=chapter_id,\n",
    "            utterance_id=utterance_id,\n",
    "            # TODO: check the pitch normalization process\n",
    "            pitch_is_normalized=False,\n",
    "        )\n",
    "\n",
    "    def univnet(self, row: Tuple[torch.Tensor, int, str, str, int, str | int, str]):\n",
    "        r\"\"\"Preprocesses audio data for use with a UnivNet model.\n",
    "\n",
    "        This method takes a row of data, extracts the audio and preprocesses it.\n",
    "        It then selects a random segment from the preprocessed audio and its corresponding mel spectrogram.\n",
    "\n",
    "        Args:\n",
    "            row (Tuple[torch.FloatTensor, int, str, str, int, str | int, str]): The input row. The row is a tuple containing the following elements: (audio, sr_actual, raw_text, normalized_text, speaker_id, chapter_id, utterance_id).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, int]: A tuple containing the selected segment of the mel spectrogram, the corresponding audio segment, and the speaker ID.\n",
    "\n",
    "        Examples:\n",
    "            >>> preprocess = PreprocessLibriTTS()\n",
    "            >>> audio = torch.randn(1, 44100)\n",
    "            >>> sr_actual = 44100\n",
    "            >>> speaker_id = 0\n",
    "            >>> mel, audio_segment, speaker_id = preprocess.preprocess_univnet((audio, sr_actual, \"\", \"\", speaker_id, 0, \"\"))\n",
    "        \"\"\"\n",
    "        (\n",
    "            audio,\n",
    "            sr_actual,\n",
    "            _,\n",
    "            _,\n",
    "            speaker_id,\n",
    "            _,\n",
    "            _,\n",
    "        ) = row\n",
    "\n",
    "        segment_size = self.vocoder_train_config.segment_size\n",
    "        frames_per_seg = math.ceil(segment_size / self.hop_length)\n",
    "\n",
    "        wav, _ = preprocess_audio(audio, sr_actual, self.sampling_rate)\n",
    "\n",
    "        if self.use_audio_normalization:\n",
    "            wav = normalize_loudness(wav)\n",
    "\n",
    "        mel_spectrogram = self.tacotronSTFT.get_mel_from_wav(wav)\n",
    "\n",
    "        if wav.shape[0] < segment_size:\n",
    "            wav = F.pad(\n",
    "                wav,\n",
    "                (0, segment_size - wav.shape[0]),\n",
    "                \"constant\",\n",
    "            )\n",
    "\n",
    "        if mel_spectrogram.shape[1] < frames_per_seg:\n",
    "            mel_spectrogram = F.pad(\n",
    "                mel_spectrogram,\n",
    "                (0, frames_per_seg - mel_spectrogram.shape[1]),\n",
    "                \"constant\",\n",
    "            )\n",
    "\n",
    "        from_frame = random.randint(0, mel_spectrogram.shape[1] - frames_per_seg)\n",
    "\n",
    "        # Skip last frame, otherwise errors are thrown, find out why\n",
    "        if from_frame > 0:\n",
    "            from_frame -= 1\n",
    "\n",
    "        till_frame = from_frame + frames_per_seg\n",
    "\n",
    "        mel_spectrogram = mel_spectrogram[:, from_frame:till_frame]\n",
    "        wav = wav[from_frame * self.hop_length : till_frame * self.hop_length]\n",
    "\n",
    "        return mel_spectrogram, wav, speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "def pad_1D(inputs: List[Tensor], pad_value: float = 0.0) -> Tensor:\n",
    "    r\"\"\"Pad a list of 1D tensor list to the same length.\n",
    "\n",
    "    Args:\n",
    "        inputs (List[torch.Tensor]): List of 1D numpy arrays to pad.\n",
    "        pad_value (float): Value to use for padding. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 2D numpy array of shape (len(inputs), max_len), where max_len is the length of the longest input array.\n",
    "    \"\"\"\n",
    "    max_len = max(x.size(0) for x in inputs)\n",
    "    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(0)), value=pad_value) for x in inputs]\n",
    "    return torch.stack(padded_inputs)\n",
    "\n",
    "\n",
    "def pad_2D(\n",
    "    inputs: List[Tensor], maxlen: Union[int, None] = None, pad_value: float = 0.0,\n",
    ") -> Tensor:\n",
    "    r\"\"\"Pad a list of 2D tensor arrays to the same length.\n",
    "\n",
    "    Args:\n",
    "        inputs (List[torch.Tensor]): List of 2D numpy arrays to pad.\n",
    "        maxlen (Union[int, None]): Maximum length to pad the arrays to. If None, pad to the length of the longest array. Default is None.\n",
    "        pad_value (float): Value to use for padding. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 3D numpy array of shape (len(inputs), max_len, input_dim), where max_len is the maximum length of the input arrays, and input_dim is the dimension of the input arrays.\n",
    "    \"\"\"\n",
    "    max_len = max(x.size(1) for x in inputs) if maxlen is None else maxlen\n",
    "\n",
    "    padded_inputs = [nn.functional.pad(x, (0, max_len - x.size(1), 0, 0), value=pad_value) for x in inputs]\n",
    "    return torch.stack(padded_inputs)\n",
    "\n",
    "\n",
    "def pad_3D(inputs: Union[Tensor, List[Tensor]], B: int, T: int, L: int) -> Tensor:\n",
    "    r\"\"\"Pad a 3D torch tensor to a specified shape.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): 3D numpy array to pad.\n",
    "        B (int): Batch size to pad the array to.\n",
    "        T (int): Time steps to pad the array to.\n",
    "        L (int): Length to pad the array to.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Padded 3D numpy array of shape (B, T, L), where B is the batch size, T is the time steps, and L is the length.\n",
    "    \"\"\"\n",
    "    if isinstance(inputs, list):\n",
    "        inputs_padded = torch.zeros(B, T, L, dtype=inputs[0].dtype)\n",
    "        for i, input_ in enumerate(inputs):\n",
    "            inputs_padded[i, :input_.size(0), :input_.size(1)] = input_\n",
    "\n",
    "    elif isinstance(inputs, torch.Tensor):\n",
    "        inputs_padded = torch.zeros(B, T, L, dtype=inputs.dtype)\n",
    "        inputs_padded[:inputs.size(0), :inputs.size(1), :inputs.size(2)] = inputs\n",
    "\n",
    "    return inputs_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchaudio import datasets\n",
    "\n",
    "# from models.config import PreprocessingConfigUnivNet, get_lang_map\n",
    "\n",
    "# from training.preprocess import PreprocessLibriTTS\n",
    "# from training.tools import pad_1D, pad_2D\n",
    "\n",
    "\n",
    "class LibriTTSDatasetVocoder(Dataset):\n",
    "    r\"\"\"Loading preprocessed univnet model data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        batch_size: int,\n",
    "        download: bool = True,\n",
    "        lang: str = \"en\",\n",
    "    ):\n",
    "        r\"\"\"A PyTorch dataset for loading preprocessed univnet data.\n",
    "\n",
    "        Args:\n",
    "            root (str): Path to the directory where the dataset is found or downloaded.\n",
    "            batch_size (int): Batch size for the dataset.\n",
    "            download (bool, optional): Whether to download the dataset if it is not found. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.LIBRITTS(root=root, download=download)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        lang_map = get_lang_map(lang)\n",
    "        self.preprocess_libtts = PreprocessLibriTTS(\n",
    "            PreprocessingConfigUnivNet(lang_map.processing_lang_type),\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns\n",
    "            int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        r\"\"\"Returns a sample from the dataset at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to return.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing the sample data.\n",
    "        \"\"\"\n",
    "        # Retrive the dataset row\n",
    "        data = self.dataset[idx]\n",
    "\n",
    "        data = self.preprocess_libtts.univnet(data)\n",
    "\n",
    "        if data is None:\n",
    "            # print(\"Skipping due to preprocessing error\")\n",
    "            rand_idx = np.random.randint(0, self.__len__())\n",
    "            return self.__getitem__(rand_idx)\n",
    "\n",
    "        mel, audio, speaker_id = data\n",
    "\n",
    "        return {\n",
    "            \"mel\": mel,\n",
    "            \"audio\": audio,\n",
    "            \"speaker_id\": speaker_id,\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, data: List) -> List:\n",
    "        r\"\"\"Collates a batch of data samples.\n",
    "\n",
    "        Args:\n",
    "            data (List): A list of data samples.\n",
    "\n",
    "        Returns:\n",
    "            List: A list of reprocessed data batches.\n",
    "        \"\"\"\n",
    "        data_size = len(data)\n",
    "\n",
    "        idxs = list(range(data_size))\n",
    "\n",
    "        # Initialize empty lists to store extracted values\n",
    "        empty_lists: List[List] = [[] for _ in range(4)]\n",
    "        (\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            audios,\n",
    "            speaker_ids,\n",
    "        ) = empty_lists\n",
    "\n",
    "        # Extract fields from data dictionary and populate the lists\n",
    "        for idx in idxs:\n",
    "            data_entry = data[idx]\n",
    "\n",
    "            mels.append(data_entry[\"mel\"])\n",
    "            mel_lens.append(data_entry[\"mel\"].shape[1])\n",
    "            audios.append(data_entry[\"audio\"])\n",
    "            speaker_ids.append(data_entry[\"speaker_id\"])\n",
    "\n",
    "        mels = torch.tensor(pad_2D(mels), dtype=torch.float32)\n",
    "        mel_lens = torch.tensor(mel_lens, dtype=torch.int64)\n",
    "        audios = torch.tensor(pad_1D(audios), dtype=torch.float32)\n",
    "        speaker_ids = torch.tensor(speaker_ids, dtype=torch.int64)\n",
    "\n",
    "        return [\n",
    "            mels,\n",
    "            mel_lens,\n",
    "            audios,\n",
    "            speaker_ids,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 2\n",
    "# dataset = LibriTTSDatasetVocoder(\n",
    "#             root=\"datasets_cache/LIBRITTS\",\n",
    "#             batch_size=batch_size,\n",
    "#             download=False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(dataset) == 33236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset[0]\n",
    "\n",
    "# assert sample[\"mel\"].shape == torch.Size([100, 64])\n",
    "# assert sample[\"audio\"].shape == torch.Size([16384])\n",
    "# assert sample[\"speaker_id\"] == 1034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     dataset[0],\n",
    "#     dataset[2],\n",
    "# ]\n",
    "# result = dataset.collate_fn(data)\n",
    "# assert len(result) == 4\n",
    "# for batch in result:\n",
    "#     assert len(batch) == batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=dataset.collate_fn,\n",
    "# )\n",
    "\n",
    "# dataloader_iter = iter(dataloader)\n",
    "\n",
    "# for _, items in enumerate([next(dataloader_iter), next(dataloader_iter)]):\n",
    "#     assert len(items) == 4\n",
    "#     for it in items:\n",
    "#         assert len(it) == batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConformerConfig:\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_hidden: int\n",
    "    p_dropout: float\n",
    "    kernel_size_conv_mod: int\n",
    "    kernel_size_depthwise: int\n",
    "    with_ff: bool\n",
    "    \n",
    "@dataclass\n",
    "class ReferenceEncoderConfig:\n",
    "    bottleneck_size_p: int\n",
    "    bottleneck_size_u: int\n",
    "    ref_enc_filters: List[int]\n",
    "    ref_enc_size: int\n",
    "    ref_enc_strides: List[int]\n",
    "    ref_enc_pad: List[int]\n",
    "    ref_enc_gru_size: int\n",
    "    ref_attention_dropout: float\n",
    "    token_num: int\n",
    "    predictor_kernel_size: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VarianceAdaptorConfig:\n",
    "    n_hidden: int\n",
    "    kernel_size: int\n",
    "    emb_kernel_size: int\n",
    "    p_dropout: float\n",
    "    n_bins: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AcousticLossConfig:\n",
    "    ssim_loss_alpha: float\n",
    "    mel_loss_alpha: float\n",
    "    aligner_loss_alpha: float\n",
    "    pitch_loss_alpha: float\n",
    "    energy_loss_alpha: float\n",
    "    u_prosody_loss_alpha: float\n",
    "    p_prosody_loss_alpha: float\n",
    "    dur_loss_alpha: float\n",
    "    binary_align_loss_alpha: float\n",
    "    binary_loss_warmup_epochs: int\n",
    "\n",
    "@dataclass\n",
    "class AcousticENModelConfig:\n",
    "    speaker_embed_dim: int = 1024\n",
    "    lang_embed_dim: int = 1\n",
    "    encoder: ConformerConfig = field(\n",
    "        default_factory=lambda: ConformerConfig(\n",
    "            n_layers=6,\n",
    "            n_heads=8,\n",
    "            n_hidden=512,\n",
    "            p_dropout=0.1,\n",
    "            kernel_size_conv_mod=7,\n",
    "            kernel_size_depthwise=7,\n",
    "            with_ff=True,\n",
    "        ),\n",
    "    )\n",
    "    decoder: ConformerConfig = field(\n",
    "        default_factory=lambda: ConformerConfig(\n",
    "            n_layers=6,\n",
    "            n_heads=8,\n",
    "            n_hidden=512,\n",
    "            p_dropout=0.1,\n",
    "            kernel_size_conv_mod=11,\n",
    "            kernel_size_depthwise=11,\n",
    "            with_ff=True,\n",
    "        ),\n",
    "    )\n",
    "    reference_encoder: ReferenceEncoderConfig = field(\n",
    "        default_factory=lambda: ReferenceEncoderConfig(\n",
    "            bottleneck_size_p=4,\n",
    "            bottleneck_size_u=256,\n",
    "            ref_enc_filters=[32, 32, 64, 64, 128, 128],\n",
    "            ref_enc_size=3,\n",
    "            ref_enc_strides=[1, 2, 1, 2, 1],\n",
    "            ref_enc_pad=[1, 1],\n",
    "            ref_enc_gru_size=32,\n",
    "            ref_attention_dropout=0.2,\n",
    "            token_num=32,\n",
    "            predictor_kernel_size=5,\n",
    "        ),\n",
    "    )\n",
    "    variance_adaptor: VarianceAdaptorConfig = field(\n",
    "        default_factory=lambda: VarianceAdaptorConfig(\n",
    "            n_hidden=512,\n",
    "            kernel_size=5,\n",
    "            emb_kernel_size=3,\n",
    "            p_dropout=0.5,\n",
    "            n_bins=256,\n",
    "        ),\n",
    "    )\n",
    "    loss: AcousticLossConfig = field(\n",
    "        default_factory=lambda: AcousticLossConfig(\n",
    "            ssim_loss_alpha=1.0,\n",
    "            mel_loss_alpha=1.0,\n",
    "            aligner_loss_alpha=1.0,\n",
    "            pitch_loss_alpha=1.0,\n",
    "            energy_loss_alpha=1.0,\n",
    "            u_prosody_loss_alpha=0.25,\n",
    "            p_prosody_loss_alpha=0.25,\n",
    "            dur_loss_alpha=1.0,\n",
    "            binary_align_loss_alpha=0.1,\n",
    "            binary_loss_warmup_epochs=10,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "AcousticModelConfigType = Union[AcousticENModelConfig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPPORTED_LANGUAGES = [\n",
    "    \"en\",\n",
    "    \"uk\",\n",
    "]\n",
    "\n",
    "lang2id = {s: i for i, s in enumerate(SUPPORTED_LANGUAGES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AcousticTrainingOptimizerConfig:\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    lr_decay: float\n",
    "    betas: Tuple[float, float] = (0.9, 0.98)\n",
    "    eps: float = 0.000000001\n",
    "    grad_clip_thresh: float = 1.0\n",
    "    warm_up_step: float = 4000\n",
    "    anneal_steps: List[int] = field(default_factory=list)\n",
    "    anneal_rate: float = 0.3\n",
    "\n",
    "@dataclass\n",
    "class AcousticPretrainingConfig:\n",
    "    batch_size = 5\n",
    "    grad_acc_step = 5\n",
    "    train_steps = 500000\n",
    "    log_step = 20\n",
    "    synth_step = 250\n",
    "    val_step = 4000\n",
    "    save_step = 1000\n",
    "    freeze_bert_until = 4000\n",
    "    mcd_gen_max_samples = 400\n",
    "    only_train_speaker_until = 0\n",
    "    optimizer_config: AcousticTrainingOptimizerConfig = field(\n",
    "        default_factory=lambda: AcousticTrainingOptimizerConfig(\n",
    "            learning_rate=0.0002,\n",
    "            weight_decay=0.01,\n",
    "            lr_decay=1.0,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseConv1d(Module):\n",
    "    r\"\"\"Implements Depthwise 1D convolution. This module will apply a spatial convolution over inputs\n",
    "    independently over each input channel in the style of depthwise convolutions.\n",
    "\n",
    "    In a depthwise convolution, each input channel is convolved with its own set of filters, as opposed\n",
    "    to standard convolutions where each input channel is convolved with all filters.\n",
    "    At `groups=in_channels`, each input channel is convolved with its own set of filters.\n",
    "    Filters in the\n",
    "    DepthwiseConv1d are not shared among channels. This method can drastically reduce the number of\n",
    "    parameters/learnable weights in the model, as each input channel gets its own filter.\n",
    "\n",
    "    This technique is best suited to scenarios where the correlation between different channels is\n",
    "    believed to be low. It is commonly employed in MobileNet models due to the reduced number of\n",
    "    parameters, which is critical in mobile devices where computational resources are limited.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int): Size of the convolving kernel\n",
    "        padding (int): Zero-padding added to both sides of the input\n",
    "\n",
    "    Shape:\n",
    "        - Input: (N, C_in, L_in)\n",
    "        - Output: (N, C_out, L_out), where\n",
    "\n",
    "          `L_out = [L_in + 2*padding - (dilation*(kernel_size-1) + 1)]/stride + 1`\n",
    "\n",
    "    Attributes:\n",
    "        weight (Tensor): the learnable weights of shape (`out_channels`, `in_channels`/`group`, `kernel_size`)\n",
    "        bias (Tensor, optional): the learnable bias of the module of shape (`out_channels`)\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    m = DepthWiseConv1d(16, 33, 3, padding=1)\n",
    "    input = torch.randn(20, 16, 50)\n",
    "    output = m(input)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=padding,\n",
    "            groups=in_channels,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: input tensor of shape (batch_size, in_channels, signal_length)\n",
    "\n",
    "        Returns:\n",
    "            output tensor of shape (batch_size, out_channels, signal_length)\n",
    "        \"\"\"\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class PointwiseConv1d(Module):\n",
    "    r\"\"\"Applies a 1D pointwise (aka 1x1) convolution over an input signal composed of several input\n",
    "    planes, officially known as channels in this context.\n",
    "\n",
    "    The operation implemented is also known as a \"channel mixing\" operation, as each output channel can be\n",
    "    seen as a linear combination of input channels.\n",
    "\n",
    "    In the simplest case, the output value of the layer with input size\n",
    "    (N, C_in, L) and output (N, C_out, L_out) can be\n",
    "    precisely described as:\n",
    "\n",
    "    $$out(N_i, C_{out_j}) = bias(C_{out_j}) +\n",
    "        weight(C_{out_j}, k) * input(N_i, k)$$\n",
    "\n",
    "    where 'N' is a batch size, 'C' denotes a number of channels,\n",
    "    'L' is a length of signal sequence.\n",
    "    The symbol '*' in the above indicates a 1D cross-correlation operation.\n",
    "\n",
    "    The 1D cross correlation operation \"*\": [Wikipedia Cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation)\n",
    "\n",
    "    This module supports `TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        stride (int): Stride of the convolution. Default: 1\n",
    "        padding (int): Zero-padding added to both sides of the input. Default: 0\n",
    "        bias (bool): If set to False, the layer will not learn an additive bias. Default: True\n",
    "        kernel_size (int): Size of the convolving kernel. Default: 1\n",
    "\n",
    "    Shape:\n",
    "        - Input: (N, C_in, L_in)\n",
    "        - Output: (N, C_out, L_out), where\n",
    "\n",
    "          L_out = [L_in + 2*padding - (dilation*(kernel_size-1) + 1)]/stride + 1\n",
    "\n",
    "    Attributes:\n",
    "        weight (Tensor): the learnable weights of shape (out_channels, in_channels, kernel_size)\n",
    "        bias (Tensor, optional): the learnable bias of the module of shape (out_channels)\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    m = PointwiseConv1d(16, 33, 1, padding=0, bias=True)\n",
    "    input = torch.randn(20, 16, 50)\n",
    "    output = m(input)\n",
    "    ```\n",
    "\n",
    "\n",
    "    Description of parameters:\n",
    "        stride (default 1): Controls the stride for the operation, which is the number of steps the convolutional\n",
    "        kernel moves for each operation. A stride of 1 means that the kernel moves one step at a time and a stride\n",
    "        of 2 means skipping every other step. Higher stride values can down sample the output and lead to smaller\n",
    "        output shapes.\n",
    "\n",
    "        padding (default 0): Controls the amount of padding applied to the input. By adding padding, the spatial\n",
    "        size of the output can be controlled. If it is set to 0, no padding is applied. If it is set to 1, zero\n",
    "        padding of one pixel width is added to the input data.\n",
    "\n",
    "        bias (default True): Controls whether the layer uses a bias vector. By default, it is True, meaning that\n",
    "        the layer has a learnable bias parameter.\n",
    "\n",
    "        kernel_size (default 1): The size of the convolving kernel. In the case of 1D convolution, kernel_size is\n",
    "        a single integer that specifies the number of elements the filter that convolves the input should have.\n",
    "        In your PointwiseConv1d case, the default kernel size is 1, indicating a 1x1 convolution is applied\n",
    "        which is commonly known as a pointwise convolution.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        bias: bool = True,\n",
    "        kernel_size: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor of shape (batch_size, in_channels, signal_length)\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): tensor of shape (batch_size, out_channels, signal_length)\n",
    "        \"\"\"\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSConv1d(Module):\n",
    "    r\"\"\"`BSConv1d` implements the `BSConv` concept which is based on the paper [BSConv:\n",
    "    Binarized Separated Convolutional Neural Networks](https://arxiv.org/pdf/2003.13549.pdf).\n",
    "\n",
    "    `BSConv` is an amalgamation of depthwise separable convolution and pointwise convolution.\n",
    "    Depthwise separable convolution utilizes far fewer parameters by separating the spatial\n",
    "    (depthwise) and channel-wise (pointwise) operations. Meanwhile, pointwise convolution\n",
    "    helps in transforming the channel characteristics without considering the channel's context.\n",
    "\n",
    "    Args:\n",
    "        channels_in (int): Number of input channels\n",
    "        channels_out (int): Number of output channels produced by the convolution\n",
    "        kernel_size (int): Size of the kernel used in depthwise convolution\n",
    "        padding (int): Zeropadding added around the input tensor along the height and width directions\n",
    "\n",
    "    Attributes:\n",
    "        pointwise (PointwiseConv1d): Pointwise convolution module\n",
    "        depthwise (DepthWiseConv1d): Depthwise separable convolution module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in: int,\n",
    "        channels_out: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Instantiate Pointwise Convolution Module:\n",
    "        # First operation in BSConv: the number of input channels is transformed to the number\n",
    "        # of output channels without taking into account the channel context.\n",
    "        self.pointwise = PointwiseConv1d(channels_in, channels_out)\n",
    "\n",
    "        # Instantiate Depthwise Convolution Module:\n",
    "        # Second operation in BSConv: A spatial convolution is performed independently over each output\n",
    "        # channel from the pointwise convolution.\n",
    "        self.depthwise = DepthWiseConv1d(\n",
    "            channels_out,\n",
    "            channels_out,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Propagate input tensor through pointwise convolution.\n",
    "        x1 = self.pointwise(x)\n",
    "\n",
    "        # Propagate the result of the previous pointwise convolution through the depthwise convolution.\n",
    "        # Return final output of the sequence of pointwise and depthwise convolutions\n",
    "        return self.depthwise(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dGLU(Module):\n",
    "    r\"\"\"`Conv1dGLU` implements a variant of Convolutional Layer with a Gated Linear Unit (GLU).\n",
    "    It's based on the Deep Voice 3 project.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): model dimension parameter.\n",
    "        kernel_size (int): kernel size for the convolution layer.\n",
    "        padding (int): padding size for the convolution layer.\n",
    "        embedding_dim (int): dimension of the embedding.\n",
    "\n",
    "    Attributes:\n",
    "         bsconv1d (BSConv1d) : an instance of the Binarized Separated Convolution (1d)\n",
    "         embedding_proj (torch.nn.Modules.Linear): linear transformation for embeddings.\n",
    "         sqrt (torch.Tensor): buffer that stores the square root of 0.5\n",
    "         softsign (torch.nn.SoftSign): SoftSign Activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        embedding_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bsconv1d = BSConv1d(\n",
    "            d_model,\n",
    "            2 * d_model,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "        self.embedding_proj = nn.Linear(\n",
    "            embedding_dim,\n",
    "            d_model,\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"sqrt\", torch.sqrt(torch.tensor([0.5])).squeeze(0))\n",
    "\n",
    "        self.softsign = torch.nn.Softsign()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation method for the Conv1dGLU layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor\n",
    "            embeddings (torch.Tensor): input embeddings\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): output tensor after application of Conv1dGLU\n",
    "        \"\"\"\n",
    "        x = x.permute((0, 2, 1))\n",
    "        residual = x\n",
    "        x = self.bsconv1d(x)\n",
    "        splitdim = 1\n",
    "        a, b = x.split(x.size(splitdim) // 2, dim=splitdim)\n",
    "        embeddings = self.embedding_proj(embeddings)\n",
    "        softsign = self.softsign(embeddings)\n",
    "        a = a + softsign.permute((0, 2, 1))\n",
    "        x = a * torch.sigmoid(b)\n",
    "        x = x + residual\n",
    "        x = x * self.sqrt\n",
    "        return x.permute((0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAKY_RELU_SLOPE = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(Module):\n",
    "    r\"\"\"Creates a feed-forward neural network.\n",
    "    The network includes a layer normalization, an activation function (LeakyReLU), and dropout layers.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The number of expected features in the input.\n",
    "        kernel_size (int): The size of the convolving kernel for the first conv1d layer.\n",
    "        dropout (float): The dropout probability.\n",
    "        expansion_factor (int, optional): The expansion factor for the hidden layer size in the feed-forward network, default is 4.\n",
    "        leaky_relu_slope (float, optional): Controls the angle of the negative slope of LeakyReLU activation, default is `LEAKY_RELU_SLOPE`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        kernel_size: int,\n",
    "        dropout: float,\n",
    "        expansion_factor: int = 4,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            d_model,\n",
    "            d_model * expansion_factor,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(leaky_relu_slope)\n",
    "        self.conv_2 = nn.Conv1d(d_model * expansion_factor, d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the feed-forward neural network.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, num_features).\n",
    "        \"\"\"\n",
    "        # Apply layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Forward pass through the first convolution layer, activation layer and dropout layer\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.conv_1(x)\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Forward pass through the second convolution layer and dropout layer\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.conv_2(x)\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Scale the output by 0.5 (this helps with training stability)\n",
    "        return 0.5 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUActivation(Module):\n",
    "    r\"\"\"Implements the Gated Linear Unit (GLU) activation function.\n",
    "\n",
    "    The GLU activation splits the input in half across the channel dimension.\n",
    "    One half is passed through a nonlinear activation function (like sigmoid or leaky ReLU),\n",
    "    and the output from this activation function is used as a gate to control the\n",
    "    amplitude of the other half of the input. An element-wise multiplication is then performed\n",
    "    between the gating signal and the other half of the input.\n",
    "\n",
    "    The GLU activation allows the model to dynamically choose which inputs to pass through and\n",
    "    what information to suppress, which can help improving the model performance on certain tasks.\n",
    "\n",
    "    Args:\n",
    "        slope: Controls the slope for the leaky ReLU activation function. Default: 0.3 or see the const `LEAKY_RELU_SLOPE`\n",
    "\n",
    "    Shape:\n",
    "        - Input: (N, 2*C, L) where C is the number of input channels.\n",
    "        - Output: (N, C, L)\n",
    "\n",
    "    Examples:\n",
    "    ```python\n",
    "    m = GLUActivation(0.3)\n",
    "    input = torch.randn(16, 2*20, 44)\n",
    "    output = m(input)\n",
    "    ```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, slope: float = LEAKY_RELU_SLOPE):\n",
    "        super().__init__()\n",
    "        self.lrelu = nn.LeakyReLU(slope)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, 2*channels, signal_length)\n",
    "\n",
    "        Returns:\n",
    "            x: The output tensor of shape (batch_size, channels, signal_length)\n",
    "        \"\"\"\n",
    "        # Split the input into two equal parts (chunks) along dimension 1\n",
    "        out, gate = x.chunk(2, dim=1)\n",
    "\n",
    "        # Perform element-wise multiplication of the first half (out)\n",
    "        # with the result of applying LeakyReLU on the second half (gate)\n",
    "        return out * self.lrelu(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_same_padding(kernel_size: int) -> Tuple[int, int]:\n",
    "    r\"\"\"Calculates the necessary padding for 'same' padding in convolutional operations.\n",
    "\n",
    "    For 'same' padding, the output size is the same as the input size for `stride=1`. This function returns\n",
    "    two integers, representing the padding to be added on either side of the input to achieve 'same' padding.\n",
    "\n",
    "    Args:\n",
    "        kernel_size (int): Size of the convolving kernel.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int]: A tuple of two integers representing the number of padding elements to be applied on\n",
    "        left and right (or top and bottom for 2D) of the input tensor respectively.\n",
    "    \"\"\"\n",
    "    # Check if kernel_size is an integer greater than zero\n",
    "    if not isinstance(kernel_size, int) or kernel_size <= 0:\n",
    "        raise ValueError(\"kernel_size must be an integer greater than zero\")\n",
    "\n",
    "    # Determine base padding amount (equal to half the kernel size, truncated down)\n",
    "    pad = kernel_size // 2\n",
    "\n",
    "    # Return padding for each side of the kernel. If kernel size is odd, padding is (pad, pad).\n",
    "    # If kernel size is even, padding is (pad, pad - 1) because we can't pad equally on both sides.\n",
    "    return (pad, pad - (kernel_size + 1) % 2)\n",
    "\n",
    "class ConformerConvModule(Module):\n",
    "    r\"\"\"Conformer Convolution Module class represents a module in the Conformer model architecture.\n",
    "    The module includes a layer normalization, pointwise and depthwise convolutional layers,\n",
    "    Gated Linear Units (GLU) activation, and dropout layer.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The number of expected features in the input.\n",
    "        expansion_factor (int): The expansion factor for the hidden layer size in the feed-forward network, default is 2.\n",
    "        kernel_size (int): The size of the convolving kernel, default is 7.\n",
    "        dropout (float): The dropout probability, default is 0.1.\n",
    "        leaky_relu_slope (float): Controls the angle of the negative slope of the LeakyReLU activation, default is `LEAKY_RELU_SLOPE`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        expansion_factor: int = 2,\n",
    "        kernel_size: int = 7,\n",
    "        dropout: float = 0.1,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = d_model * expansion_factor\n",
    "        self.ln_1 = nn.LayerNorm(d_model)\n",
    "        self.conv_1 = PointwiseConv1d(\n",
    "            d_model,\n",
    "            inner_dim * 2,\n",
    "        )\n",
    "        self.conv_act = GLUActivation()\n",
    "        self.depthwise = DepthWiseConv1d(\n",
    "            inner_dim,\n",
    "            inner_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=calc_same_padding(kernel_size)[0],\n",
    "        )\n",
    "        self.ln_2 = nn.GroupNorm(\n",
    "            1,\n",
    "            inner_dim,\n",
    "        )\n",
    "        self.activation = nn.LeakyReLU(leaky_relu_slope)\n",
    "        self.conv_2 = PointwiseConv1d(\n",
    "            inner_dim,\n",
    "            d_model,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the Conformer conv module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n",
    "        \"\"\"\n",
    "        x = self.ln_1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_act(x)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.ln_2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeMultiHeadAttention(Module):\n",
    "    r\"\"\"Multi-head attention with relative positional encoding.\n",
    "    This concept was proposed in the\n",
    "    [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of model\n",
    "        num_heads (int): The number of attention heads.\n",
    "\n",
    "    Inputs: query, key, value, pos_embedding, mask\n",
    "        - **query** (batch, time, dim): Tensor containing query vector\n",
    "        - **key** (batch, time, dim): Tensor containing key vector\n",
    "        - **value** (batch, time, dim): Tensor containing value vector\n",
    "        - **pos_embedding** (batch, time, dim): Positional embedding tensor\n",
    "        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n",
    "    Returns:\n",
    "        - **outputs**: Tensor produces by relative multi head attention module.\n",
    "\n",
    "    Note: `d_model` should be divisible by `num_heads` in other words `d_model % num_heads` should be zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model % num_heads should be zero.\"\n",
    "        self.d_model = d_model\n",
    "        self.d_head = int(d_model / num_heads)\n",
    "        self.num_heads = num_heads\n",
    "        self.sqrt_dim = math.sqrt(d_model)\n",
    "\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.pos_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.u_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n",
    "        self.v_bias = nn.Parameter(torch.Tensor(self.num_heads, self.d_head))\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.u_bias)\n",
    "        torch.nn.init.xavier_uniform_(self.v_bias)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        pos_embedding: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Function applies multi-head attention along with relative positional encoding to the inputs. It restructures the input queries, keys, and values according to individual attention heads, applies biases, calculates content and position scores, and combines these to get the final score. A softmax activation is applied over the final score, followed by the calculation of context (contextual representation of input).\n",
    "\n",
    "        Performs the forward pass on the queries, keys, values, and positional embeddings with a mask.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The input tensor containing query vectors.\n",
    "            key (torch.Tensor): The input tensor containing key vectors.\n",
    "            value (torch.Tensor): The input tensor containing value vectors.\n",
    "            pos_embedding (torch.Tensor): The positional embedding tensor.\n",
    "            mask (torch.Tensor): The mask tensor containing indices to be masked.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The context and attention tensors.\n",
    "            Tensor produces by relative multi head attention module.\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_head)\n",
    "        key = (\n",
    "            self.key_proj(key)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        value = (\n",
    "            self.value_proj(value)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .permute(0, 2, 1, 3)\n",
    "        )\n",
    "        pos_embedding = self.pos_proj(pos_embedding).view(\n",
    "            batch_size, -1, self.num_heads, self.d_head,\n",
    "        )\n",
    "        u_bias = self.u_bias.expand_as(query)\n",
    "        v_bias = self.v_bias.expand_as(query)\n",
    "        a = (query + u_bias).transpose(1, 2)\n",
    "        content_score = a @ key.transpose(2, 3)\n",
    "        b = (query + v_bias).transpose(1, 2)\n",
    "        pos_score = b @ pos_embedding.permute(0, 2, 3, 1)\n",
    "        pos_score = self._relative_shift(pos_score)\n",
    "\n",
    "        score = content_score + pos_score\n",
    "        score = score * (1.0 / self.sqrt_dim)\n",
    "\n",
    "        score.masked_fill_(mask, -1e9)\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "\n",
    "        context = (attn @ value).transpose(1, 2)\n",
    "        context = context.contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        return self.out_proj(context), attn\n",
    "\n",
    "    def _relative_shift(self, pos_score: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"The main idea of relative positional encoding is that the attention score doesn't only depend on the query and the key, but also on the relative position of the key with respect to the query. This becomes particularly useful when working with sequences of tokens, like in NLP tasks, as it helps the model to be aware of the position of the words (or tokens) in the sentence.\n",
    "\n",
    "        Performs the relative shift operation on the positional scores.\n",
    "\n",
    "        Args:\n",
    "            pos_score (torch.Tensor): The positional scores tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The shifted positional scores tensor.\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_length1, seq_length2 = pos_score.size()\n",
    "        zeros = torch.zeros(\n",
    "            (batch_size, num_heads, seq_length1, 1), device=pos_score.device,\n",
    "        )\n",
    "        padded_pos_score = torch.cat([zeros, pos_score], dim=-1)\n",
    "        padded_pos_score = padded_pos_score.view(\n",
    "            batch_size, num_heads, seq_length2 + 1, seq_length1,\n",
    "        )\n",
    "        return padded_pos_score[:, :, 1:].view_as(pos_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerMultiHeadedSelfAttention(Module):\n",
    "    \"\"\"Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL,\n",
    "    the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention\n",
    "    module to generalize better on different input length and the resulting encoder is more robust to the variance of\n",
    "    the utterance length. Conformer use `prenorm` residual units with dropout which helps training\n",
    "    and regularizing deeper models.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of model\n",
    "        num_heads (int): The number of attention heads.\n",
    "        dropout_p (float): probability of dropout\n",
    "\n",
    "    Inputs: inputs, mask\n",
    "        - **inputs** (batch, time, dim): Tensor containing input vector\n",
    "        - **mask** (batch, 1, time2) or (batch, time1, time2): Tensor containing indices to be masked\n",
    "\n",
    "    Returns:\n",
    "        (batch, time, dim): Tensor produces by relative multi headed self attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        dropout_p: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the RelativeMultiHeadAttention module passing the model dimension and number of attention heads\n",
    "        self.attention = RelativeMultiHeadAttention(\n",
    "            d_model=d_model, num_heads=num_heads,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        encoding: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size, _, _ = key.size()\n",
    "\n",
    "        # Trim or extend the \"encoding\" to match the size of key, and repeat this for each input in the batch\n",
    "        encoding = encoding[:, : key.shape[1]]\n",
    "        encoding = encoding.repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Pass inputs through the RelativeMultiHeadAttention layer, dropout the resulting outputs\n",
    "        outputs, attn = self.attention(\n",
    "            query, key, value, pos_embedding=encoding, mask=mask,\n",
    "        )\n",
    "\n",
    "        # Apply dropout to the attention outputs\n",
    "        outputs = self.dropout(outputs)\n",
    "        return outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerBlock(Module):\n",
    "    r\"\"\"ConformerBlock class represents a block in the Conformer model architecture.\n",
    "    The block includes a pointwise convolution followed by Gated Linear Units (`GLU`) activation layer (`Conv1dGLU`),\n",
    "    a Conformer self attention layer (`ConformerMultiHeadedSelfAttention`), and optional feed-forward layer (`FeedForward`).\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The number of expected features in the input.\n",
    "        n_head (int): The number of heads for the multiheaded attention mechanism.\n",
    "        kernel_size_conv_mod (int): The size of the convolving kernel for the convolution module.\n",
    "        embedding_dim (int): The dimension of the embeddings.\n",
    "        dropout (float): The dropout probability.\n",
    "        with_ff (bool): If True, uses FeedForward layer inside ConformerBlock.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_head: int,\n",
    "        kernel_size_conv_mod: int,\n",
    "        embedding_dim: int,\n",
    "        dropout: float,\n",
    "        with_ff: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.with_ff = with_ff\n",
    "        self.conditioning = Conv1dGLU(\n",
    "            d_model=d_model,\n",
    "            kernel_size=kernel_size_conv_mod,\n",
    "            padding=kernel_size_conv_mod // 2,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        if self.with_ff:\n",
    "            self.ff = FeedForward(\n",
    "                d_model=d_model,\n",
    "                dropout=dropout,\n",
    "                kernel_size=3,\n",
    "            )\n",
    "        self.conformer_conv_1 = ConformerConvModule(\n",
    "            d_model,\n",
    "            kernel_size=kernel_size_conv_mod,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(\n",
    "            d_model,\n",
    "        )\n",
    "        self.slf_attn = ConformerMultiHeadedSelfAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=n_head,\n",
    "            dropout_p=dropout,\n",
    "        )\n",
    "        self.conformer_conv_2 = ConformerConvModule(\n",
    "            d_model,\n",
    "            kernel_size=kernel_size_conv_mod,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        embeddings: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        slf_attn_mask: torch.Tensor,\n",
    "        encoding: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the Conformer block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n",
    "            embeddings (Tensor): Embeddings tensor.\n",
    "            mask (Tensor): The mask tensor.\n",
    "            slf_attn_mask (Tensor): The mask for self-attention layer.\n",
    "            encoding (Tensor): The positional encoding tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n",
    "        \"\"\"\n",
    "        x = self.conditioning.forward(x, embeddings=embeddings)\n",
    "        if self.with_ff:\n",
    "            x = self.ff(x) + x\n",
    "        x = self.conformer_conv_1(x) + x\n",
    "        res = x\n",
    "        x = self.ln(x)\n",
    "        x, _ = self.slf_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            mask=slf_attn_mask,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "        x = x + res\n",
    "        x = x.masked_fill(mask.unsqueeze(-1), 0)\n",
    "        return self.conformer_conv_2(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conformer(Module):\n",
    "    r\"\"\"`Conformer` class represents the `Conformer` model which is a sequence-to-sequence model\n",
    "    used in some modern automated speech recognition systems. It is composed of several `ConformerBlocks`.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The number of expected features in the input.\n",
    "        n_layers (int): The number of `ConformerBlocks` in the Conformer model.\n",
    "        n_heads (int): The number of heads in the multiheaded self-attention mechanism in each `ConformerBlock`.\n",
    "        embedding_dim (int): The dimension of the embeddings.\n",
    "        p_dropout (float): The dropout probability to be used in each `ConformerBlock`.\n",
    "        kernel_size_conv_mod (int): The size of the convolving kernel in the convolution module of each `ConformerBlock`.\n",
    "        with_ff (bool): If True, each `ConformerBlock` uses FeedForward layer inside it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        embedding_dim: int,\n",
    "        p_dropout: float,\n",
    "        kernel_size_conv_mod: int,\n",
    "        with_ff: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.ModuleList(\n",
    "            [\n",
    "                ConformerBlock(\n",
    "                    dim,\n",
    "                    n_heads,\n",
    "                    kernel_size_conv_mod=kernel_size_conv_mod,\n",
    "                    dropout=p_dropout,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    with_ff=with_ff,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        embeddings: torch.Tensor,\n",
    "        encoding: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Forward Pass of the Conformer block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, num_features).\n",
    "            mask (Tensor): The mask tensor.\n",
    "            embeddings (Tensor): Embeddings tensor.\n",
    "            encoding (Tensor): The positional encoding tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor of shape (batch_size, seq_len, num_features).\n",
    "        \"\"\"\n",
    "        attn_mask = mask.view((mask.shape[0], 1, 1, mask.shape[1]))\n",
    "        attn_mask.to(x.device)\n",
    "        for enc_layer in self.layer_stack:\n",
    "            x = enc_layer(\n",
    "                x,\n",
    "                mask=mask,\n",
    "                slf_attn_mask=attn_mask,\n",
    "                embeddings=embeddings,\n",
    "                encoding=encoding,\n",
    "            )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransposed(Module):\n",
    "    r\"\"\"`ConvTransposed` applies a 1D convolution operation, with the main difference that it transposes the\n",
    "    last two dimensions of the input tensor before and after applying the `BSConv1d` convolution operation.\n",
    "    This can be useful in certain architectures where the tensor dimensions are processed in a different order.\n",
    "\n",
    "    The `ConvTransposed` class performs a `BSConv` operation after transposing the input tensor dimensions. Specifically, it swaps the channels and width dimensions of a tensor, applies the convolution, and then swaps the dimensions back to their original order. The intuition behind swapping dimensions can depend on the specific use case in the larger architecture; typically, it's used when the operation or sequence of operations expected a different arrangement of dimensions.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int): Size of the kernel used in convolution\n",
    "        padding (int): Zero-padding added around the input tensor along the width direction\n",
    "\n",
    "    Attributes:\n",
    "        conv (BSConv1d): `BSConv1d` module to apply convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 1,\n",
    "        padding: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define BSConv1d convolutional layer\n",
    "        self.conv = BSConv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation method for the ConvTransposed layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): output tensor after application of ConvTransposed\n",
    "        \"\"\"\n",
    "        # Transpose the last two dimensions (dimension 1 and 2 here). Now the tensor has shape (N, W, C)\n",
    "        x = x.contiguous().transpose(1, 2)\n",
    "\n",
    "        # Apply BSConv1d convolution.\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Transpose the last two dimensions back to their original order. Now the tensor has shape (N, C, W)\n",
    "        # Return final output tensor\n",
    "        return x.contiguous().transpose(1, 2)\n",
    "\n",
    "class VariancePredictor(Module):\n",
    "    r\"\"\"Duration and Pitch predictor neural network module in PyTorch.\n",
    "\n",
    "    It consists of multiple layers, including `ConvTransposed` layers (custom convolution transpose layers from\n",
    "    the `model.conv_blocks` module), LeakyReLU activation functions, Layer Normalization and Dropout layers.\n",
    "\n",
    "    Constructor for `VariancePredictor` class.\n",
    "\n",
    "    Args:\n",
    "        channels_in (int): Number of input channels.\n",
    "        channels (int): Number of output channels for ConvTransposed layers and input channels for linear layer.\n",
    "        channels_out (int): Number of output channels for linear layer.\n",
    "        kernel_size (int): Size of the kernel for ConvTransposed layers.\n",
    "        p_dropout (float): Probability of dropout.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in: int,\n",
    "        channels: int,\n",
    "        channels_out: int,\n",
    "        kernel_size: int,\n",
    "        p_dropout: float,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                # Convolution transpose layer followed by LeakyReLU, LayerNorm and Dropout\n",
    "                ConvTransposed(\n",
    "                    channels_in,\n",
    "                    channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                ),\n",
    "                nn.LeakyReLU(leaky_relu_slope),\n",
    "                nn.LayerNorm(\n",
    "                    channels,\n",
    "                ),\n",
    "                nn.Dropout(p_dropout),\n",
    "                # Another \"block\" of ConvTransposed, LeakyReLU, LayerNorm, and Dropout\n",
    "                ConvTransposed(\n",
    "                    channels,\n",
    "                    channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                ),\n",
    "                nn.LeakyReLU(leaky_relu_slope),\n",
    "                nn.LayerNorm(\n",
    "                    channels,\n",
    "                ),\n",
    "                nn.Dropout(p_dropout),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Output linear layer\n",
    "        self.linear_layer = nn.Linear(\n",
    "            channels,\n",
    "            channels_out,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass for `VariancePredictor`.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            mask (torch.Tensor): Mask tensor, has the same size as x.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        # Sequentially pass the input through all defined layers\n",
    "        # (ConvTransposed -> LeakyReLU -> LayerNorm -> Dropout -> ConvTransposed -> LeakyReLU -> LayerNorm -> Dropout)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.linear_layer(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x.masked_fill(mask, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_over_durations(values: torch.Tensor, durs: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Function calculates the average of values over specified durations.\n",
    "\n",
    "    Args:\n",
    "    values (torch.Tensor): A 3D tensor of shape [B, 1, T_de] where B is the batch size,\n",
    "                           T_de is the duration of each element in the batch. The values\n",
    "                           represent some quantity that needs to be averaged over durations.\n",
    "    durs (torch.Tensor): A 2D tensor of shape [B, T_en] where B is the batch size,\n",
    "                         T_en is the number of elements in each batch. The values represent\n",
    "                         the durations over which the averaging needs to be done.\n",
    "\n",
    "    Returns:\n",
    "    avg (torch.Tensor): A 3D tensor of shape [B, 1, T_en] where B is the batch size,\n",
    "                        T_en is the number of elements in each batch. The values represent\n",
    "                        the average of the input values over the specified durations.\n",
    "\n",
    "    Note:\n",
    "    The function uses PyTorch operations for efficient computation on GPU.\n",
    "\n",
    "    Shapes:\n",
    "        - values: :math:`[B, 1, T_de]`\n",
    "        - durs: :math:`[B, T_en]`\n",
    "        - avg: :math:`[B, 1, T_en]`\n",
    "    \"\"\"\n",
    "    durs_cums_ends = torch.cumsum(durs, dim=1).long()\n",
    "    durs_cums_starts = torch.nn.functional.pad(durs_cums_ends[:, :-1], (1, 0))\n",
    "    values_nonzero_cums = torch.nn.functional.pad(torch.cumsum(values != 0.0, dim=2), (1, 0))\n",
    "    values_cums = torch.nn.functional.pad(torch.cumsum(values, dim=2), (1, 0))\n",
    "\n",
    "    bs, l = durs_cums_ends.size()\n",
    "    n_formants = values.size(1)\n",
    "    dcs = durs_cums_starts[:, None, :].expand(bs, n_formants, l)\n",
    "    dce = durs_cums_ends[:, None, :].expand(bs, n_formants, l)\n",
    "\n",
    "    values_sums = (torch.gather(values_cums, 2, dce) - torch.gather(values_cums, 2, dcs)).float()\n",
    "    values_nelems = (torch.gather(values_nonzero_cums, 2, dce) - torch.gather(values_nonzero_cums, 2, dcs)).float()\n",
    "\n",
    "    avg = torch.where(values_nelems == 0.0, values_nelems, values_sums / values_nelems)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchAdaptorConv(nn.Module):\n",
    "    \"\"\"The PitchAdaptorConv class is a pitch adaptor network in the model.\n",
    "    Updated version of the PitchAdaptorConv uses the conv embeddings for the pitch.\n",
    "\n",
    "    Args:\n",
    "        channels_in (int): Number of in channels for conv layers.\n",
    "        channels_out (int): Number of out channels.\n",
    "        kernel_size (int): Size the kernel for the conv layers.\n",
    "        dropout (float): Probability of dropout.\n",
    "        leaky_relu_slope (float): Slope for the leaky relu.\n",
    "        emb_kernel_size (int): Size the kernel for the pitch embedding.\n",
    "\n",
    "    Inputs: inputs, mask\n",
    "        - **inputs** (batch, time1, dim): Tensor containing input vector\n",
    "        - **target** (batch, 1, time2): Tensor containing the pitch target\n",
    "        - **dr** (batch, time1): Tensor containing aligner durations vector\n",
    "        - **mask** (batch, time1): Tensor containing indices to be masked\n",
    "    Returns:\n",
    "        - **pitch prediction** (batch, 1, time1): Tensor produced by pitch predictor\n",
    "        - **pitch embedding** (batch, channels, time1): Tensor produced pitch adaptor\n",
    "        - **average pitch target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in: int,\n",
    "        channels_hidden: int,\n",
    "        channels_out: int,\n",
    "        kernel_size: int,\n",
    "        dropout: float,\n",
    "        leaky_relu_slope: float,\n",
    "        emb_kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pitch_predictor = VariancePredictor(\n",
    "            channels_in=channels_in,\n",
    "            channels=channels_hidden,\n",
    "            channels_out=channels_out,\n",
    "            kernel_size=kernel_size,\n",
    "            p_dropout=dropout,\n",
    "            leaky_relu_slope=leaky_relu_slope,\n",
    "        )\n",
    "        self.pitch_emb = nn.Conv1d(\n",
    "            1,\n",
    "            channels_hidden,\n",
    "            kernel_size=emb_kernel_size,\n",
    "            padding=int((emb_kernel_size - 1) / 2),\n",
    "        )\n",
    "\n",
    "    def get_pitch_embedding_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        dr: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Function is used during training to get the pitch prediction, average pitch target,\n",
    "        and pitch embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n",
    "                            T_src is the source sequence length, and C is the number of channels.\n",
    "            target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n",
    "                                T_max2 is the maximum target sequence length.\n",
    "            dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the durations.\n",
    "            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the mask.\n",
    "\n",
    "        Returns:\n",
    "            pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                        T_src is the source sequence length. The values represent the pitch prediction.\n",
    "            avg_pitch_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                            T_src is the source sequence length. The values represent the average pitch target.\n",
    "            pitch_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n",
    "                                    C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n",
    "        Shapes:\n",
    "            x: :math: `[B, T_src, C]`\n",
    "            target: :math: `[B, 1, T_max2]`\n",
    "            dr: :math: `[B, T_src]`\n",
    "            mask: :math: `[B, T_src]`\n",
    "        \"\"\"\n",
    "        pitch_pred = self.pitch_predictor.forward(x, mask)\n",
    "        pitch_pred = pitch_pred.unsqueeze(1)\n",
    "\n",
    "        avg_pitch_target = average_over_durations(target, dr)\n",
    "        pitch_emb = self.pitch_emb(avg_pitch_target)\n",
    "\n",
    "        return pitch_pred, avg_pitch_target, pitch_emb\n",
    "\n",
    "    def add_pitch_embedding_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        dr: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Add pitch embedding during training.\n",
    "\n",
    "        This method calculates the pitch embedding and adds it to the input tensor 'x'.\n",
    "        It also returns the predicted pitch and the average target pitch.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n",
    "            target (torch.Tensor): The target tensor used in the pitch embedding calculation.\n",
    "            dr (torch.Tensor): The duration tensor used in the pitch embedding calculation.\n",
    "            mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): The input tensor with added pitch embedding.\n",
    "            pitch_pred (torch.Tensor): The predicted pitch tensor.\n",
    "            avg_pitch_target (torch.Tensor): The average target pitch tensor.\n",
    "        \"\"\"\n",
    "        pitch_pred, avg_pitch_target, pitch_emb = self.get_pitch_embedding_train(\n",
    "            x=x,\n",
    "            target=target.unsqueeze(1),\n",
    "            dr=dr,\n",
    "            mask=mask,\n",
    "        )\n",
    "        x_pitch = x + pitch_emb.transpose(1, 2)\n",
    "        return x_pitch, pitch_pred, avg_pitch_target\n",
    "\n",
    "    def get_pitch_embedding(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Function is used during inference to get the pitch embedding and pitch prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n",
    "                            T_src is the source sequence length, and C is the number of channels.\n",
    "            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the mask.\n",
    "\n",
    "        Returns:\n",
    "            pitch_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n",
    "                                            C is the number of channels, T_src is the source sequence length. The values represent the pitch embedding.\n",
    "            pitch_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                        T_src is the source sequence length. The values represent the pitch prediction.\n",
    "        \"\"\"\n",
    "        pitch_pred = self.pitch_predictor.forward(x, mask)\n",
    "        pitch_pred = pitch_pred.unsqueeze(1)\n",
    "\n",
    "        pitch_emb_pred = self.pitch_emb(pitch_pred)\n",
    "        return pitch_emb_pred, pitch_pred\n",
    "\n",
    "    def add_pitch_embedding(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Add pitch embedding during inference.\n",
    "\n",
    "        This method calculates the pitch embedding and adds it to the input tensor 'x'.\n",
    "        It also returns the predicted pitch.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to which the pitch embedding will be added.\n",
    "            mask (torch.Tensor): The mask tensor used in the pitch embedding calculation.\n",
    "            pitch_transform (Callable): A function to transform the pitch prediction.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): The input tensor with added pitch embedding.\n",
    "            pitch_pred (torch.Tensor): The predicted pitch tensor.\n",
    "        \"\"\"\n",
    "        pitch_emb_pred, pitch_pred = self.get_pitch_embedding(x, mask)\n",
    "        x_pitch = x + pitch_emb_pred.transpose(1, 2)\n",
    "        return x_pitch, pitch_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnergyAdaptor(nn.Module):\n",
    "    \"\"\"Variance Adaptor with an added 1D conv layer. Used to\n",
    "    get energy embeddings.\n",
    "\n",
    "    Args:\n",
    "        channels_in (int): Number of in channels for conv layers.\n",
    "        channels_out (int): Number of out channels.\n",
    "        kernel_size (int): Size the kernel for the conv layers.\n",
    "        dropout (float): Probability of dropout.\n",
    "        leaky_relu_slope (float): Slope for the leaky relu.\n",
    "        emb_kernel_size (int): Size the kernel for the pitch embedding.\n",
    "\n",
    "    Inputs: inputs, mask\n",
    "        - **inputs** (batch, time1, dim): Tensor containing input vector\n",
    "        - **target** (batch, 1, time2): Tensor containing the energy target\n",
    "        - **dr** (batch, time1): Tensor containing aligner durations vector\n",
    "        - **mask** (batch, time1): Tensor containing indices to be masked\n",
    "    Returns:\n",
    "        - **energy prediction** (batch, 1, time1): Tensor produced by energy predictor\n",
    "        - **energy embedding** (batch, channels, time1): Tensor produced energy adaptor\n",
    "        - **average energy target(train only)** (batch, 1, time1): Tensor produced after averaging over durations\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels_in: int,\n",
    "        channels_hidden: int,\n",
    "        channels_out: int,\n",
    "        kernel_size: int,\n",
    "        dropout: float,\n",
    "        leaky_relu_slope: float,\n",
    "        emb_kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.energy_predictor = VariancePredictor(\n",
    "            channels_in=channels_in,\n",
    "            channels=channels_hidden,\n",
    "            channels_out=channels_out,\n",
    "            kernel_size=kernel_size,\n",
    "            p_dropout=dropout,\n",
    "            leaky_relu_slope=leaky_relu_slope,\n",
    "        )\n",
    "        self.energy_emb = nn.Conv1d(\n",
    "            1,\n",
    "            channels_hidden,\n",
    "            kernel_size=emb_kernel_size,\n",
    "            padding=int((emb_kernel_size - 1) / 2),\n",
    "        )\n",
    "\n",
    "    def get_energy_embedding_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        dr: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Function is used during training to get the energy prediction, average energy target, and energy embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n",
    "                            T_src is the source sequence length, and C is the number of channels.\n",
    "            target (torch.Tensor): A 3D tensor of shape [B, 1, T_max2] where B is the batch size,\n",
    "                                T_max2 is the maximum target sequence length.\n",
    "            dr (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the durations.\n",
    "            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the mask.\n",
    "\n",
    "        Returns:\n",
    "            energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                        T_src is the source sequence length. The values represent the energy prediction.\n",
    "            avg_energy_target (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                            T_src is the source sequence length. The values represent the average energy target.\n",
    "            energy_emb (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n",
    "                                    C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n",
    "        Shapes:\n",
    "            x: :math: `[B, T_src, C]`\n",
    "            target: :math: `[B, 1, T_max2]`\n",
    "            dr: :math: `[B, T_src]`\n",
    "            mask: :math: `[B, T_src]`\n",
    "        \"\"\"\n",
    "        energy_pred = self.energy_predictor.forward(x, mask)\n",
    "        energy_pred = energy_pred.unsqueeze(1)\n",
    "\n",
    "        avg_energy_target = average_over_durations(target, dr)\n",
    "        energy_emb = self.energy_emb(avg_energy_target)\n",
    "\n",
    "        return energy_pred, avg_energy_target, energy_emb\n",
    "\n",
    "    def add_energy_embedding_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        dr: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Add energy embedding during training.\n",
    "\n",
    "        This method calculates the energy embedding and adds it to the input tensor 'x'.\n",
    "        It also returns the predicted energy and the average target energy.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to which the energy embedding will be added.\n",
    "            target (torch.Tensor): The target tensor used in the energy embedding calculation.\n",
    "            dr (torch.Tensor): The duration tensor used in the energy embedding calculation.\n",
    "            mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): The input tensor with added energy embedding.\n",
    "            energy_pred (torch.Tensor): The predicted energy tensor.\n",
    "            avg_energy_target (torch.Tensor): The average target energy tensor.\n",
    "        \"\"\"\n",
    "        energy_pred, avg_energy_target, energy_emb = self.get_energy_embedding_train(\n",
    "            x=x,\n",
    "            target=target,\n",
    "            dr=dr,\n",
    "            mask=mask,\n",
    "        )\n",
    "        x_energy = x + energy_emb.transpose(1, 2)\n",
    "        return x_energy, energy_pred, avg_energy_target\n",
    "\n",
    "    def get_energy_embedding(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Function is used during inference to get the energy embedding and energy prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3D tensor of shape [B, T_src, C] where B is the batch size,\n",
    "                            T_src is the source sequence length, and C is the number of channels.\n",
    "            mask (torch.Tensor): A 2D tensor of shape [B, T_src] where B is the batch size,\n",
    "                                T_src is the source sequence length. The values represent the mask.\n",
    "\n",
    "        Returns:\n",
    "            energy_emb_pred (torch.Tensor): A 3D tensor of shape [B, C, T_src] where B is the batch size,\n",
    "                                            C is the number of channels, T_src is the source sequence length. The values represent the energy embedding.\n",
    "            energy_pred (torch.Tensor): A 3D tensor of shape [B, 1, T_src] where B is the batch size,\n",
    "                                        T_src is the source sequence length. The values represent the energy prediction.\n",
    "        \"\"\"\n",
    "        energy_pred = self.energy_predictor.forward(x, mask)\n",
    "        energy_pred = energy_pred.unsqueeze(1)\n",
    "\n",
    "        energy_emb_pred = self.energy_emb(energy_pred)\n",
    "        return energy_emb_pred, energy_pred\n",
    "\n",
    "    def add_energy_embedding(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Add energy embedding during inference.\n",
    "\n",
    "        This method calculates the energy embedding and adds it to the input tensor 'x'.\n",
    "        It also returns the predicted energy.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor to which the energy embedding will be added.\n",
    "            mask (torch.Tensor): The mask tensor used in the energy embedding calculation.\n",
    "            energy_transform (Callable): A function to transform the energy prediction.\n",
    "\n",
    "        Returns:\n",
    "            x (torch.Tensor): The input tensor with added energy embedding.\n",
    "            energy_pred (torch.Tensor): The predicted energy tensor.\n",
    "        \"\"\"\n",
    "        energy_emb_pred, energy_pred = self.get_energy_embedding(x, mask)\n",
    "        x_energy = x + energy_emb_pred.transpose(1, 2)\n",
    "        return x_energy, energy_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(input_ele: List[torch.Tensor], max_len: int) -> torch.Tensor:\n",
    "    r\"\"\"Takes a list of 1D or 2D tensors and pads them to match the maximum length.\n",
    "\n",
    "    Args:\n",
    "        input_ele (List[torch.Tensor]): The list of tensors to be padded.\n",
    "        max_len (int): The length to which the tensors should be padded.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing all the padded input tensors.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the padded tensors\n",
    "    out_list = torch.jit.annotate(List[torch.Tensor], [])\n",
    "    for batch in input_ele:\n",
    "        if len(batch.shape) == 1:\n",
    "            # Perform padding for 1D tensor\n",
    "            one_batch_padded = F.pad(\n",
    "                batch, (0, max_len - batch.size(0)), \"constant\", 0.0,\n",
    "            )\n",
    "        else:\n",
    "            # Perform padding for 2D tensor\n",
    "            one_batch_padded = F.pad(\n",
    "                batch, (0, 0, 0, max_len - batch.size(0)), \"constant\", 0.0,\n",
    "            )\n",
    "        # Append the padded tensor to the list\n",
    "        out_list.append(one_batch_padded)\n",
    "\n",
    "    # Stack all the tensors in the list into a single tensor\n",
    "    return torch.stack(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthAdaptor(Module):\n",
    "    r\"\"\"DEPRECATED: The LengthAdaptor module is used to adjust the duration of phonemes.\n",
    "    It contains a dedicated duration predictor and methods to upsample the input features to match predicted durations.\n",
    "\n",
    "    Args:\n",
    "        model_config (AcousticModelConfigType): The model configuration object containing model parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: AcousticModelConfigType,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Initialize the duration predictor\n",
    "        self.duration_predictor = VariancePredictor(\n",
    "            channels_in=model_config.encoder.n_hidden,\n",
    "            channels=model_config.variance_adaptor.n_hidden,\n",
    "            channels_out=1,\n",
    "            kernel_size=model_config.variance_adaptor.kernel_size,\n",
    "            p_dropout=model_config.variance_adaptor.p_dropout,\n",
    "        )\n",
    "\n",
    "    def length_regulate(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        duration: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Regulates the length of the input tensor using the duration tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            duration (torch.Tensor): The tensor containing duration for each time step in x.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The regulated output tensor and the tensor containing the length of each sequence in the batch.\n",
    "        \"\"\"\n",
    "        output = torch.jit.annotate(List[torch.Tensor], [])\n",
    "        mel_len = torch.jit.annotate(List[int], [])\n",
    "        max_len = 0\n",
    "        for batch, expand_target in zip(x, duration):\n",
    "            expanded = self.expand(batch, expand_target)\n",
    "            if expanded.shape[0] > max_len:\n",
    "                max_len = expanded.shape[0]\n",
    "            output.append(expanded)\n",
    "            mel_len.append(expanded.shape[0])\n",
    "        output = pad(output, max_len)\n",
    "        return output, torch.tensor(mel_len, dtype=torch.int64)\n",
    "\n",
    "    def expand(self, batch: torch.Tensor, predicted: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Expands the input tensor based on the predicted values.\n",
    "\n",
    "        Args:\n",
    "            batch (torch.Tensor): The input tensor.\n",
    "            predicted (torch.Tensor): The tensor containing predicted expansion factors.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The expanded tensor.\n",
    "        \"\"\"\n",
    "        out = torch.jit.annotate(List[torch.Tensor], [])\n",
    "        for i, vec in enumerate(batch):\n",
    "            expand_size = predicted[i].item()\n",
    "            out.append(vec.expand(max(int(expand_size), 0), -1))\n",
    "        return torch.cat(out, 0)\n",
    "\n",
    "    def upsample_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_res: torch.Tensor,\n",
    "        duration_target: torch.Tensor,\n",
    "        embeddings: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Upsamples the input tensor during training using ground truth durations.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            x_res (torch.Tensor): Another input tensor for duration prediction.\n",
    "            duration_target (torch.Tensor): The ground truth durations tensor.\n",
    "            embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n",
    "            src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, log duration prediction, and upsampled embeddings.\n",
    "        \"\"\"\n",
    "        x_res = x_res.detach()\n",
    "        log_duration_prediction = self.duration_predictor(\n",
    "            x_res,\n",
    "            src_mask,\n",
    "        )  # type: torch.Tensor\n",
    "        x, _ = self.length_regulate(x, duration_target)\n",
    "        embeddings, _ = self.length_regulate(embeddings, duration_target)\n",
    "        return x, log_duration_prediction, embeddings\n",
    "\n",
    "    def upsample(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        x_res: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        embeddings: torch.Tensor,\n",
    "        control: float,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Upsamples the input tensor during inference.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            x_res (torch.Tensor): Another input tensor for duration prediction.\n",
    "            src_mask (torch.Tensor): The mask tensor indicating valid entries in x and x_res.\n",
    "            embeddings (torch.Tensor): The tensor containing phoneme embeddings.\n",
    "            control (float): A control parameter for pitch regulation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The upsampled x, approximated duration, and upsampled embeddings.\n",
    "        \"\"\"\n",
    "        log_duration_prediction = self.duration_predictor(\n",
    "            x_res,\n",
    "            src_mask,\n",
    "        )\n",
    "        duration_rounded = torch.clamp(\n",
    "            (torch.round(torch.exp(log_duration_prediction) - 1) * control),\n",
    "            min=0,\n",
    "        )\n",
    "        x, _ = self.length_regulate(x, duration_rounded)\n",
    "        embeddings, _ = self.length_regulate(embeddings, duration_rounded)\n",
    "        return x, duration_rounded, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCoords(Module):\n",
    "    r\"\"\"AddCoords is a PyTorch module that adds additional channels to the input tensor containing the relative\n",
    "    (normalized to `[-1, 1]`) coordinates of each input element along the specified number of dimensions (`rank`).\n",
    "    Essentially, it adds spatial context information to the tensor.\n",
    "\n",
    "    Typically, these inputs are feature maps coming from some CNN, where the spatial organization of the input\n",
    "    matters (such as an image or speech signal).\n",
    "\n",
    "    This additional spatial context allows subsequent layers (such as convolutions) to learn position-dependent\n",
    "    features. For example, in tasks where the absolute position of features matters (such as denoising and\n",
    "    segmentation tasks), it helps the model to know where (in terms of relative position) the features are.\n",
    "\n",
    "    Args:\n",
    "        rank (int): The dimensionality of the input tensor. That is to say, this tells us how many dimensions the\n",
    "                    input tensor's spatial context has. It's assumed to be 1, 2, or 3 corresponding to some 1D, 2D,\n",
    "                    or 3D data (like an image).\n",
    "\n",
    "        with_r (bool): Boolean indicating whether to add an extra radial distance channel or not. If True, an extra\n",
    "                       channel is appended, which measures the Euclidean (L2) distance from the center of the image.\n",
    "                       This might be useful when the proximity to the center of the image is important to the task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rank: int, with_r: bool = False):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.with_r = with_r\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the AddCoords module. Depending on the rank of the tensor, it adds one or more new channels\n",
    "        with relative coordinate values. If `with_r` is True, an extra radial channel is included.\n",
    "\n",
    "        For example, for an image (`rank=2`), two channels would be added which contain the normalized x and y\n",
    "        coordinates respectively of each pixel.\n",
    "\n",
    "        Calling the forward method updates the original tensor `x` with the added channels.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): The input tensor with added coordinate and possibly radial channels.\n",
    "        \"\"\"\n",
    "        if self.rank == 1:\n",
    "            batch_size_shape, _, dim_x = x.shape\n",
    "            xx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n",
    "            xx_channel = xx_range[None, None, :]\n",
    "\n",
    "            xx_channel = xx_channel.float() / (dim_x - 1)\n",
    "            xx_channel = xx_channel * 2 - 1\n",
    "            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1)\n",
    "\n",
    "            out = torch.cat([x, xx_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(torch.pow(xx_channel - 0.5, 2))\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "\n",
    "        elif self.rank == 2:\n",
    "            batch_size_shape, _, dim_y, dim_x = x.shape\n",
    "            xx_ones = torch.ones([1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n",
    "            yy_ones = torch.ones([1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n",
    "\n",
    "            xx_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n",
    "            yy_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n",
    "            xx_range = xx_range[None, None, :, None]\n",
    "            yy_range = yy_range[None, None, :, None]\n",
    "\n",
    "            xx_channel = torch.matmul(xx_range, xx_ones)\n",
    "            yy_channel = torch.matmul(yy_range, yy_ones)\n",
    "\n",
    "            # transpose y\n",
    "            yy_channel = yy_channel.permute(0, 1, 3, 2)\n",
    "\n",
    "            xx_channel = xx_channel.float() / (dim_y - 1)\n",
    "            yy_channel = yy_channel.float() / (dim_x - 1)\n",
    "\n",
    "            xx_channel = xx_channel * 2 - 1\n",
    "            yy_channel = yy_channel * 2 - 1\n",
    "\n",
    "            xx_channel = xx_channel.repeat(batch_size_shape, 1, 1, 1)\n",
    "            yy_channel = yy_channel.repeat(batch_size_shape, 1, 1, 1)\n",
    "\n",
    "            out = torch.cat([x, xx_channel, yy_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(\n",
    "                    torch.pow(xx_channel - 0.5, 2) + torch.pow(yy_channel - 0.5, 2),\n",
    "                )\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "\n",
    "        elif self.rank == 3:\n",
    "            batch_size_shape, _, dim_z, dim_y, dim_x = x.shape\n",
    "            xx_ones = torch.ones([1, 1, 1, 1, dim_x], dtype=torch.int32, device=x.device)\n",
    "            yy_ones = torch.ones([1, 1, 1, 1, dim_y], dtype=torch.int32, device=x.device)\n",
    "            zz_ones = torch.ones([1, 1, 1, 1, dim_z], dtype=torch.int32, device=x.device)\n",
    "\n",
    "            xy_range = torch.arange(dim_y, dtype=torch.int32, device=x.device)\n",
    "            xy_range = xy_range[None, None, None, :, None]\n",
    "\n",
    "            yz_range = torch.arange(dim_z, dtype=torch.int32, device=x.device)\n",
    "            yz_range = yz_range[None, None, None, :, None]\n",
    "\n",
    "            zx_range = torch.arange(dim_x, dtype=torch.int32, device=x.device)\n",
    "            zx_range = zx_range[None, None, None, :, None]\n",
    "\n",
    "            xy_channel = torch.matmul(xy_range, xx_ones)\n",
    "            xx_channel = torch.cat([xy_channel + i for i in range(dim_z)], dim=2)\n",
    "\n",
    "            yz_channel = torch.matmul(yz_range, yy_ones)\n",
    "            yz_channel = yz_channel.permute(0, 1, 3, 4, 2)\n",
    "            yy_channel = torch.cat([yz_channel + i for i in range(dim_x)], dim=4)\n",
    "\n",
    "            zx_channel = torch.matmul(zx_range, zz_ones)\n",
    "            zx_channel = zx_channel.permute(0, 1, 4, 2, 3)\n",
    "            zz_channel = torch.cat([zx_channel + i for i in range(dim_y)], dim=3)\n",
    "\n",
    "            out = torch.cat([x, xx_channel, yy_channel, zz_channel], dim=1)\n",
    "\n",
    "            if self.with_r:\n",
    "                rr = torch.sqrt(\n",
    "                    torch.pow(xx_channel - 0.5, 2)\n",
    "                    + torch.pow(yy_channel - 0.5, 2)\n",
    "                    + torch.pow(zz_channel - 0.5, 2),\n",
    "                )\n",
    "                out = torch.cat([out, rr], dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules import conv\n",
    "\n",
    "class CoordConv1d(conv.Conv1d, Module):\n",
    "    r\"\"\"`CoordConv1d` is an extension of the standard 1D convolution layer (`conv.Conv1d`), with the addition of extra coordinate\n",
    "    channels. These extra channels encode positional coordinates, and optionally, the radial distance from the origin.\n",
    "    This is inspired by the paper:\n",
    "    [An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution](https://arxiv.org/abs/1807.03247)\n",
    "    and is designed to help Convolution layers to pay attention to the absolute position of features in the input space.\n",
    "\n",
    "    The responsibility of this class is to intercept the input tensor and append extra channels to it. These extra channels\n",
    "    encode the positional coordinates (and optionally, the radial distance from the center). The enhanced tensor is then\n",
    "    immediately passed through a standard Conv1D layer.\n",
    "\n",
    "    In concrete terms, this means Convolution layer does not just process the color in an image-based task, but also 'knows'\n",
    "    where in the overall image this color is located.\n",
    "\n",
    "    In a typical Text-To-Speech (TTS) system like DelightfulTTS, the utterance is processed in a sequential manner.\n",
    "    The importance of sequential data in such a use-case can benefit from `CoordConv` layer as it offers a way to draw\n",
    "    more attention to the positioning of data. `CoordConv` is a drop-in replacement for standard convolution layers,\n",
    "    enriches spatial representation in Convolutional Neural Networks (CNN) with additional positional information.\n",
    "\n",
    "    Hence, the resultant Convolution does not only process the characteristics of the sound in the input speech signal,\n",
    "    but also 'knows' where in the overall signal this particular sound is located, providing it with the spatial context.\n",
    "    This can be particularly useful in TTS systems where the sequence of phonemes and their timing can be critical.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input.\n",
    "        out_channels (int): Number of channels produced by the convolution.\n",
    "        kernel_size (int): Size of the convolving kernel.\n",
    "        stride (int): Stride of the convolution. Default: 1.\n",
    "        padding (int): Zero-padding added to both sides of the input . Default: 0.\n",
    "        dilation (int): Spacing between kernel elements. Default: 1.\n",
    "        groups (int): Number of blocked connections from input channels to output channels. Default: 1.\n",
    "        bias (bool): If True, adds a learnable bias to the output. Default: True.\n",
    "        with_r (bool): If True, adds a radial coordinate channel. Default: False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        with_r: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "        )\n",
    "\n",
    "        self.rank = 1\n",
    "        self.addcoords = AddCoords(self.rank, with_r)\n",
    "\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels + self.rank + int(with_r),\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            dilation,\n",
    "            groups,\n",
    "            bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"The forward pass of the `CoordConv1d` module. It adds the coordinate channels to the input tensor with the `AddCoords`\n",
    "        module, and then immediately passes the result through a 1D convolution.\n",
    "\n",
    "        As a result, the subsequent Conv layers don't merely process sound characteristics of the speech signal, but are\n",
    "        also aware of their relative positioning, offering a notable improvement over traditional methods, particularly for\n",
    "        challenging TTS tasks where the sequence is critical.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, out_channels, length).\n",
    "        \"\"\"\n",
    "        # Apply AddCoords layer to add coordinate channels to the input tensor\n",
    "        x = self.addcoords(x)\n",
    "\n",
    "        # Apply convolution\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Generate a mask tensor from a tensor of sequence lengths.\n",
    "\n",
    "    Args:\n",
    "        lengths (torch.Tensor): A tensor of sequence lengths of shape: (batch_size, )\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A mask tensor of shape: (batch_size, max_len) where max_len is the\n",
    "            maximum sequence length in the provided tensor. The mask tensor has a value of\n",
    "            True at each position that is more than the length of the sequence (padding positions).\n",
    "\n",
    "    Example:\n",
    "      lengths: `torch.tensor([2, 3, 1, 4])`\n",
    "      Mask tensor will be: `torch.tensor([\n",
    "            [False, False, True, True],\n",
    "            [False, False, False, True],\n",
    "            [False, True, True, True],\n",
    "            [False, False, False, False]\n",
    "        ])`\n",
    "    \"\"\"\n",
    "    # Get batch size\n",
    "    batch_size = lengths.shape[0]\n",
    "\n",
    "    # Get maximum sequence length in the batch\n",
    "    max_len = int(torch.max(lengths).item())\n",
    "\n",
    "    # Generate a tensor of shape (batch_size, max_len)\n",
    "    # where each row contains values from 0 to max_len\n",
    "    ids = (\n",
    "        torch.arange(0, max_len, device=lengths.device)\n",
    "        .unsqueeze(0)\n",
    "        .expand(batch_size, -1)\n",
    "    )\n",
    "    # Compare each value in the ids tensor with\n",
    "    # corresponding sequence length to generate a mask.\n",
    "    # The mask will have True at positions where id >= sequence length,\n",
    "    # indicating padding positions in the original sequences\n",
    "    return ids >= lengths.unsqueeze(1).type(torch.int64).expand(-1, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_lens_downsampling(lens: torch.Tensor, stride: int = 2) -> torch.Tensor:\n",
    "    r\"\"\"Function computes the lengths of 1D tensor when applying a stride for downsampling.\n",
    "\n",
    "    Args:\n",
    "        lens (torch.Tensor): Tensor containing the lengths to be downsampled.\n",
    "        stride (int, optional): The stride to be used for downsampling. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of the same shape as the input containing the downsampled lengths.\n",
    "    \"\"\"\n",
    "    # The torch.ceil function is used to handle cases where the length is not evenly divisible\n",
    "    # by the stride. The torch.ceil function rounds up to the nearest integer, ensuring that\n",
    "    # each item is present at least once in the downsampled lengths.\n",
    "    # Finally, the .int() is used to convert the resulting float32 tensor to an integer tensor.\n",
    "    return torch.ceil(lens / stride).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReferenceEncoder(Module):\n",
    "    r\"\"\"A class to define the reference encoder.\n",
    "    Similar to Tacotron model, the reference encoder is used to extract the high-level features from the reference\n",
    "\n",
    "    It consists of a number of convolutional blocks (`CoordConv1d` for the first one and `nn.Conv1d` for the rest),\n",
    "    then followed by instance normalization and GRU layers.\n",
    "    The `CoordConv1d` at the first layer to better preserve positional information, paper:\n",
    "    [Robust and fine-grained prosody control of end-to-end speech synthesis](https://arxiv.org/pdf/1811.02122.pdf)\n",
    "\n",
    "    Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): Configuration object with preprocessing parameters.\n",
    "        model_config (AcousticModelConfigType): Configuration object with acoustic model parameters.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. _First_: The sequence tensor\n",
    "            produced by the last GRU layer after padding has been removed. _Second_: The GRU's final hidden state tensor.\n",
    "            _Third_: The mask tensor, which has the same shape as x, and contains `True` at positions where the input x\n",
    "            has been masked.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        model_config: AcousticModelConfigType,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        n_mel_channels = preprocess_config.stft.n_mel_channels\n",
    "        ref_enc_filters = model_config.reference_encoder.ref_enc_filters\n",
    "        ref_enc_size = model_config.reference_encoder.ref_enc_size\n",
    "        ref_enc_strides = model_config.reference_encoder.ref_enc_strides\n",
    "        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n",
    "\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        K = len(ref_enc_filters)\n",
    "        filters = [self.n_mel_channels, *ref_enc_filters]\n",
    "        strides = [1, *ref_enc_strides]\n",
    "\n",
    "        # Use CoordConv1d at the first layer to better preserve positional information: https://arxiv.org/pdf/1811.02122.pdf\n",
    "        convs = [\n",
    "            CoordConv1d(\n",
    "                in_channels=filters[0],\n",
    "                out_channels=filters[0 + 1],\n",
    "                kernel_size=ref_enc_size,\n",
    "                stride=strides[0],\n",
    "                padding=ref_enc_size // 2,\n",
    "                with_r=True,\n",
    "            ),\n",
    "            *[\n",
    "                nn.Conv1d(\n",
    "                    in_channels=filters[i],\n",
    "                    out_channels=filters[i + 1],\n",
    "                    kernel_size=ref_enc_size,\n",
    "                    stride=strides[i],\n",
    "                    padding=ref_enc_size // 2,\n",
    "                )\n",
    "                for i in range(1, K)\n",
    "            ],\n",
    "        ]\n",
    "        # Define convolution layers (ModuleList)\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "        self.norms = nn.ModuleList(\n",
    "            [\n",
    "                nn.InstanceNorm1d(num_features=ref_enc_filters[i], affine=True)\n",
    "                for i in range(K)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Define GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=ref_enc_filters[-1],\n",
    "            hidden_size=ref_enc_gru_size,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mel_lens: torch.Tensor,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Forward pass of the ReferenceEncoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3-dimensional tensor containing the input sequences, its size is [N, n_mels, timesteps].\n",
    "            mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in x. Its length is N.\n",
    "            leaky_relu_slope (float): The slope of the leaky relu function.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing three tensors. _First_: The sequence tensor\n",
    "                produced by the last GRU layer after padding has been removed. _Second_: The GRU's final hidden state tensor.\n",
    "                _Third_: The mask tensor, which has the same shape as x, and contains `True` at positions where the input x\n",
    "                has been masked.\n",
    "        \"\"\"\n",
    "        mel_masks = get_mask_from_lengths(mel_lens).unsqueeze(1)\n",
    "        mel_masks = mel_masks.to(x.device)\n",
    "\n",
    "        x = x.masked_fill(mel_masks, 0)\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = x.float()\n",
    "            x = conv(x)\n",
    "            x = F.leaky_relu(x, leaky_relu_slope)  # [N, 128, Ty//2^K, n_mels//2^K]\n",
    "            x = norm(x)\n",
    "\n",
    "        for _ in range(2):\n",
    "            mel_lens = stride_lens_downsampling(mel_lens)\n",
    "\n",
    "        mel_masks = get_mask_from_lengths(mel_lens)\n",
    "\n",
    "        x = x.masked_fill(mel_masks.unsqueeze(1), 0)\n",
    "        x = x.permute((0, 2, 1))\n",
    "\n",
    "        packed_sequence = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            x,\n",
    "            lengths=mel_lens.cpu().int(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        self.gru.flatten_parameters()\n",
    "        # memory --- [N, Ty, E//2], out --- [1, N, E//2]\n",
    "        out, memory = self.gru(packed_sequence)\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "\n",
    "        return out, memory, mel_masks\n",
    "\n",
    "    def calculate_channels(\n",
    "        self,\n",
    "        L: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        pad: int,\n",
    "        n_convs: int,\n",
    "    ) -> int:\n",
    "        r\"\"\"Calculate the number of channels after applying convolutions.\n",
    "\n",
    "        Args:\n",
    "            L (int): The original size.\n",
    "            kernel_size (int): The kernel size used in the convolutions.\n",
    "            stride (int): The stride used in the convolutions.\n",
    "            pad (int): The padding used in the convolutions.\n",
    "            n_convs (int): The number of convolutions.\n",
    "\n",
    "        Returns:\n",
    "            int: The size after the convolutions.\n",
    "        \"\"\"\n",
    "        # Loop through each convolution\n",
    "        for _ in range(n_convs):\n",
    "            # Calculate the size after each convolution\n",
    "            L = (L - kernel_size + 2 * pad) // stride + 1\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEmbedAttention(Module):\n",
    "    r\"\"\"Mechanism is being used to extract style features from audio data in the form of spectrograms.\n",
    "\n",
    "    Each style token (parameterized by an embedding vector) represents a unique style feature. The model applies the `StyleEmbedAttention` mechanism to combine these style tokens (style features) in a weighted manner. The output of the attention module is a sum of style tokens, with each token weighted by its relevance to the input.\n",
    "\n",
    "    This technique is often used in text-to-speech synthesis (TTS) such as Tacotron-2, where the goal is to modulate the prosody, stress, and intonation of the synthesized speech based on the reference audio or some control parameters. The concept of \"global style tokens\" (GST) was introduced in\n",
    "    [Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis](https://arxiv.org/abs/1803.09017) by Yuxuan Wang et al.\n",
    "\n",
    "    The `StyleEmbedAttention` class is a PyTorch module implementing the attention mechanism.\n",
    "    This class is specifically designed for handling multiple attention heads.\n",
    "    Attention here operates on a query and a set of key-value pairs to produce an output.\n",
    "\n",
    "    Builds the `StyleEmbedAttention` network.\n",
    "\n",
    "    Args:\n",
    "        query_dim (int): Dimensionality of the query vectors.\n",
    "        key_dim (int): Dimensionality of the key vectors.\n",
    "        num_units (int): Total dimensionality of the query, key, and value vectors.\n",
    "        num_heads (int): Number of parallel attention layers (heads).\n",
    "\n",
    "    Note: `num_units` should be divisible by `num_heads`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_dim: int,\n",
    "        key_dim: int,\n",
    "        num_units: int,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "\n",
    "        self.W_query = nn.Linear(\n",
    "            in_features=query_dim,\n",
    "            out_features=num_units,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.W_key = nn.Linear(in_features=key_dim, out_features=num_units, bias=False)\n",
    "        self.W_value = nn.Linear(\n",
    "            in_features=key_dim, out_features=num_units, bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key_soft: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the StyleEmbedAttention module calculates the attention scores.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): The input tensor for queries of shape `[N, T_q, query_dim]`\n",
    "            key_soft (torch.Tensor): The input tensor for keys of shape `[N, T_k, key_dim]`\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor of shape `[N, T_q, num_units]`\n",
    "        \"\"\"\n",
    "        values = self.W_value(key_soft)\n",
    "        split_size = self.num_units // self.num_heads\n",
    "        values = torch.stack(torch.split(values, split_size, dim=2), dim=0)\n",
    "\n",
    "        # out_soft = scores_soft = None\n",
    "        queries = self.W_query(query)  # [N, T_q, num_units]\n",
    "        keys = self.W_key(key_soft)  # [N, T_k, num_units]\n",
    "\n",
    "        # [h, N, T_q, num_units/h]\n",
    "        queries = torch.stack(torch.split(queries, split_size, dim=2), dim=0)\n",
    "        # [h, N, T_k, num_units/h]\n",
    "        keys = torch.stack(torch.split(keys, split_size, dim=2), dim=0)\n",
    "        # [h, N, T_k, num_units/h]\n",
    "\n",
    "        # score = softmax(QK^T / (d_k ** 0.5))\n",
    "        scores_soft = torch.matmul(queries, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n",
    "        scores_soft = scores_soft / (self.key_dim**0.5)\n",
    "        scores_soft = F.softmax(scores_soft, dim=3)\n",
    "\n",
    "        # out = score * V\n",
    "        # [h, N, T_q, num_units/h]\n",
    "        out_soft = torch.matmul(scores_soft, values)\n",
    "        return torch.cat(torch.split(out_soft, 1, dim=0), dim=3).squeeze(\n",
    "            0,\n",
    "        )  # [N, T_q, num_units] scores_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STL(Module):\n",
    "    r\"\"\"Style Token Layer (STL).\n",
    "    This layer helps to encapsulate different speaking styles in token embeddings.\n",
    "\n",
    "    Args:\n",
    "        model_config (AcousticModelConfigType): An object containing the model's configuration parameters.\n",
    "\n",
    "    Attributes:\n",
    "        embed (nn.Parameter): The style token embedding tensor.\n",
    "        attention (StyleEmbedAttention): The attention module used to compute a weighted sum of embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: AcousticModelConfigType,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of attention heads\n",
    "        num_heads = 1\n",
    "        # Dimension of encoder hidden states\n",
    "        n_hidden = model_config.encoder.n_hidden\n",
    "        # Number of style tokens\n",
    "        self.token_num = model_config.reference_encoder.token_num\n",
    "\n",
    "        # Define a learnable tensor for style tokens embedding\n",
    "        self.embed = nn.Parameter(\n",
    "            torch.FloatTensor(self.token_num, n_hidden // num_heads),\n",
    "        )\n",
    "\n",
    "        # Dimension of query in attention\n",
    "        d_q = n_hidden // 2\n",
    "        # Dimension of keys in attention\n",
    "        d_k = n_hidden // num_heads\n",
    "\n",
    "        # Style Embedding Attention module\n",
    "        self.attention = StyleEmbedAttention(\n",
    "            query_dim=d_q,\n",
    "            key_dim=d_k,\n",
    "            num_units=n_hidden,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        # Initialize the embedding with normal distribution\n",
    "        torch.nn.init.normal_(self.embed, mean=0, std=0.5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the Style Token Layer\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns\n",
    "            torch.Tensor: The emotion embedded tensor after applying attention mechanism.\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "\n",
    "        # Reshape input tensor to [N, 1, n_hidden // 2]\n",
    "        query = x.unsqueeze(1)\n",
    "\n",
    "        keys_soft = (\n",
    "            torch.tanh(self.embed).unsqueeze(0).expand(N, -1, -1)\n",
    "        )  # [N, token_num, n_hidden // num_heads]\n",
    "\n",
    "        # Apply attention mechanism to get weighted sum of style token embeddings\n",
    "        return self.attention(query, keys_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteranceLevelProsodyEncoder(Module):\n",
    "    r\"\"\"A class to define the utterance level prosody encoder.\n",
    "\n",
    "    The encoder uses a Reference encoder class to convert input sequences into high-level features,\n",
    "    followed by prosody embedding, self attention on the embeddings, and a feedforward transformation to generate the final output.Initializes the encoder with given specifications and creates necessary layers.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): Configuration object with preprocessing parameters.\n",
    "        model_config (AcousticModelConfigType): Configuration object with acoustic model parameters.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A 3-dimensional tensor sized `[N, seq_len, E]`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        model_config: AcousticModelConfigType,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.E = model_config.encoder.n_hidden\n",
    "        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n",
    "        ref_attention_dropout = model_config.reference_encoder.ref_attention_dropout\n",
    "        bottleneck_size = model_config.reference_encoder.bottleneck_size_u\n",
    "\n",
    "        # Define important layers/modules for the encoder\n",
    "        self.encoder = ReferenceEncoder(preprocess_config, model_config)\n",
    "        self.encoder_prj = nn.Linear(ref_enc_gru_size, self.E // 2)\n",
    "        self.stl = STL(model_config)\n",
    "        self.encoder_bottleneck = nn.Linear(self.E, bottleneck_size)\n",
    "        self.dropout = nn.Dropout(ref_attention_dropout)\n",
    "\n",
    "    def forward(self, mels: torch.Tensor, mel_lens: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Defines the forward pass of the utterance level prosody encoder.\n",
    "\n",
    "        Args:\n",
    "            mels (torch.Tensor): A 3-dimensional tensor containing input sequences. Size is `[N, Ty/r, n_mels*r]`.\n",
    "            mel_lens (torch.Tensor): A 1-dimensional tensor containing the lengths of each sequence in mels. Length is N.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A 3-dimensional tensor sized `[N, seq_len, E]`.\n",
    "        \"\"\"\n",
    "        # Use the reference encoder to get prosody embeddings\n",
    "        _, embedded_prosody, _ = self.encoder(mels, mel_lens)\n",
    "\n",
    "        # Bottleneck\n",
    "        # Use the linear projection layer on the prosody embeddings\n",
    "        embedded_prosody = self.encoder_prj(embedded_prosody)\n",
    "\n",
    "        # Apply the style token layer followed by the bottleneck layer\n",
    "        out = self.encoder_bottleneck(self.stl(embedded_prosody))\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Reshape the output tensor before returning\n",
    "        return out.view((-1, 1, out.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeProsodyPredictor(Module):\n",
    "    r\"\"\"A class to define the Phoneme Prosody Predictor.\n",
    "\n",
    "    In linguistics, prosody (/ˈprɒsədi, ˈprɒzədi/) is the study of elements of speech that are not individual phonetic segments (vowels and consonants) but which are properties of syllables and larger units of speech, including linguistic functions such as intonation, stress, and rhythm. Such elements are known as suprasegmentals.\n",
    "\n",
    "    [Wikipedia Prosody (linguistics)](https://en.wikipedia.org/wiki/Prosody_(linguistics))\n",
    "\n",
    "    This prosody predictor is non-parallel and is inspired by the **work of Du et al., 2021 ?**. It consists of\n",
    "    multiple convolution transpose, Leaky ReLU activation, LayerNorm, and dropout layers, followed by a\n",
    "    linear transformation to generate the final output.\n",
    "\n",
    "    Args:\n",
    "        model_config (AcousticModelConfigType): Configuration object with model parameters.\n",
    "        phoneme_level (bool): A flag to decide whether to use phoneme level bottleneck size.\n",
    "        leaky_relu_slope (float): The negative slope of LeakyReLU activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_config: AcousticModelConfigType,\n",
    "        phoneme_level: bool,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Get the configuration\n",
    "        self.d_model = model_config.encoder.n_hidden\n",
    "        kernel_size = model_config.reference_encoder.predictor_kernel_size\n",
    "        dropout = model_config.encoder.p_dropout\n",
    "\n",
    "        # Decide on the bottleneck size based on phoneme level flag\n",
    "        bottleneck_size = (\n",
    "            model_config.reference_encoder.bottleneck_size_p\n",
    "            if phoneme_level\n",
    "            else model_config.reference_encoder.bottleneck_size_u\n",
    "        )\n",
    "\n",
    "        # Define the layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                ConvTransposed(\n",
    "                    self.d_model,\n",
    "                    self.d_model,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                ),\n",
    "                nn.LeakyReLU(leaky_relu_slope),\n",
    "                nn.LayerNorm(\n",
    "                    self.d_model,\n",
    "                ),\n",
    "                nn.Dropout(dropout),\n",
    "                ConvTransposed(\n",
    "                    self.d_model,\n",
    "                    self.d_model,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                ),\n",
    "                nn.LeakyReLU(leaky_relu_slope),\n",
    "                nn.LayerNorm(\n",
    "                    self.d_model,\n",
    "                ),\n",
    "                nn.Dropout(dropout),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Output bottleneck layer\n",
    "        self.predictor_bottleneck = nn.Linear(\n",
    "            self.d_model,\n",
    "            bottleneck_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass of the prosody predictor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): A 3-dimensional tensor `[B, src_len, d_model]`.\n",
    "            mask (torch.Tensor): A 2-dimensional tensor `[B, src_len]`.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A 3-dimensional tensor `[B, src_len, 2 * d_model]`.\n",
    "        \"\"\"\n",
    "        # Expand the mask tensor's dimensions from [B, src_len] to [B, src_len, 1]\n",
    "        mask = mask.unsqueeze(2)\n",
    "\n",
    "        # Pass the input through the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Apply mask\n",
    "        x = x.masked_fill(mask, 0.0)\n",
    "\n",
    "        # Final linear transformation\n",
    "        return self.predictor_bottleneck(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeLevelProsodyEncoder(Module):\n",
    "    r\"\"\"Phoneme Level Prosody Encoder Module\n",
    "\n",
    "    This Class is used to encode the phoneme level prosody in the speech synthesis pipeline.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): Configuration for preprocessing.\n",
    "        model_config (AcousticModelConfigType): Acoustic model configuration.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The encoded tensor after applying masked fill.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        model_config: AcousticModelConfigType,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Obtain the bottleneck size and reference encoder gru size from the model config.\n",
    "        bottleneck_size = model_config.reference_encoder.bottleneck_size_p\n",
    "        ref_enc_gru_size = model_config.reference_encoder.ref_enc_gru_size\n",
    "\n",
    "        # Initialize ReferenceEncoder, Linear layer and ConformerMultiHeadedSelfAttention for attention mechanism.\n",
    "        self.encoder = ReferenceEncoder(preprocess_config, model_config)\n",
    "        self.encoder_prj = nn.Linear(ref_enc_gru_size, model_config.encoder.n_hidden)\n",
    "        self.attention = ConformerMultiHeadedSelfAttention(\n",
    "            d_model=model_config.encoder.n_hidden,\n",
    "            num_heads=model_config.encoder.n_heads,\n",
    "            dropout_p=model_config.encoder.p_dropout,\n",
    "        )\n",
    "\n",
    "        # Bottleneck layer to transform the output of the attention mechanism.\n",
    "        self.encoder_bottleneck = nn.Linear(\n",
    "            model_config.encoder.n_hidden, bottleneck_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        mels: torch.Tensor,\n",
    "        mel_lens: torch.Tensor,\n",
    "        encoding: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"The forward pass of the PhonemeLevelProsodyEncoder. Input tensors are passed through the reference encoder,\n",
    "        attention mechanism, and a bottleneck.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [N, seq_len, encoder_embedding_dim].\n",
    "            src_mask (torch.Tensor): The mask tensor which contains `True` at positions where the input x has been masked.\n",
    "            mels (torch.Tensor): The mel-spectrogram with shape [N, Ty/r, n_mels*r], where r=1.\n",
    "            mel_lens (torch.Tensor): The lengths of each sequence in mels.\n",
    "            encoding (torch.Tensor): The relative positional encoding tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [N, seq_len, bottleneck_size].\n",
    "        \"\"\"\n",
    "        # Use the reference encoder to embed prosody representation\n",
    "        embedded_prosody, _, mel_masks = self.encoder(mels, mel_lens)\n",
    "\n",
    "        # Pass the prosody representation through a bottleneck (dimension reduction)\n",
    "        embedded_prosody = self.encoder_prj(embedded_prosody)\n",
    "\n",
    "        # Flatten and apply attention mask\n",
    "        attn_mask = mel_masks.view((mel_masks.shape[0], 1, 1, -1))\n",
    "        x, _ = self.attention(\n",
    "            query=x,\n",
    "            key=embedded_prosody,\n",
    "            value=embedded_prosody,\n",
    "            mask=attn_mask,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "\n",
    "        # Apply the bottleneck to the output and mask out irrelevant parts\n",
    "        x = self.encoder_bottleneck(x)\n",
    "        return x.masked_fill(src_mask.unsqueeze(-1), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "@njit(fastmath=True)\n",
    "def mas_width1(attn_map: np.ndarray) -> np.ndarray:\n",
    "    r\"\"\"Applies a Monotonic Alignments Shrink (MAS) operation with a hard-coded width of 1 to an attention map.\n",
    "    Mas with hardcoded width=1\n",
    "    Essentially, it produces optimal alignments based on previous attention distribution.\n",
    "\n",
    "    Args:\n",
    "        attn_map (np.ndarray): The original attention map, a 2D numpy array where rows correspond to mel bins and columns to text bins.\n",
    "\n",
    "    Returns:\n",
    "        opt (np.ndarray): Returns the optimal attention map after applying the MAS operation.\n",
    "    \"\"\"\n",
    "    # assumes mel x text\n",
    "    # Create a placeholder for the output\n",
    "    opt = np.zeros_like(attn_map)\n",
    "\n",
    "    # Convert the attention map to log scale for stability\n",
    "    attn_map = np.log(attn_map)\n",
    "\n",
    "    # Initialize the first row of attention map appropriately\n",
    "    attn_map[0, 1:] = -np.inf\n",
    "\n",
    "    # Initialize log_p with the first row of attention map\n",
    "    log_p = np.zeros_like(attn_map)\n",
    "    log_p[0, :] = attn_map[0, :]\n",
    "\n",
    "    # Placeholder to remember the previous indices for backtracking later\n",
    "    prev_ind = np.zeros_like(attn_map, dtype=np.int64)\n",
    "\n",
    "    # Compute the log probabilities based on previous attention distribution\n",
    "    for i in range(1, attn_map.shape[0]):\n",
    "        for j in range(attn_map.shape[1]):  # for each text dim\n",
    "            prev_log = log_p[i - 1, j]\n",
    "            prev_j = j\n",
    "\n",
    "            # Compare with left (j-1) pixel and update if the left pixel has larger log probability\n",
    "            if j - 1 >= 0 and log_p[i - 1, j - 1] >= log_p[i - 1, j]:\n",
    "                prev_log = log_p[i - 1, j - 1]\n",
    "                prev_j = j - 1\n",
    "\n",
    "            log_p[i, j] = attn_map[i, j] + prev_log\n",
    "\n",
    "            # Store the position of maximum cumulative log probability\n",
    "            prev_ind[i, j] = prev_j\n",
    "\n",
    "    # Backtrack to retrieve the path of attention with maximum cumulative log probability\n",
    "    curr_text_idx = attn_map.shape[1] - 1\n",
    "    for i in range(attn_map.shape[0] - 1, -1, -1):\n",
    "        opt[i, curr_text_idx] = 1\n",
    "        curr_text_idx = prev_ind[i, curr_text_idx]\n",
    "\n",
    "    # Mark the first position of the optimal path\n",
    "    opt[0, curr_text_idx] = 1\n",
    "    return opt\n",
    "\n",
    "\n",
    "# @njit(parallel=True)\n",
    "def b_mas(\n",
    "    b_attn_map: np.ndarray,\n",
    "    in_lens: np.ndarray,\n",
    "    out_lens: np.ndarray,\n",
    "    width: int=1) -> np.ndarray:\n",
    "    r\"\"\"Applies Monotonic Alignments Shrink (MAS) operation in parallel to the batches of an attention map.\n",
    "    It uses the `mas_width1` function internally to perform MAS operation.\n",
    "\n",
    "    Args:\n",
    "        b_attn_map (np.ndarray): The batched attention map; a 3D array where the first dimension is the batch size, second dimension corresponds to source length, and third dimension corresponds to target length.\n",
    "        in_lens (np.ndarray): Lengths of sequences in the input batch.\n",
    "        out_lens (np.ndarray): Lengths of sequences in the output batch.\n",
    "        width (int, optional): The width for the MAS operation. Defaults to 1.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If width is not equal to 1. This function currently supports only width of 1.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The batched attention map after applying the MAS operation. It has the same dimensions as `b_attn_map`.\n",
    "    \"\"\"\n",
    "    # Assert that the width is 1. This function currently supports only width of 1\n",
    "    assert width == 1\n",
    "    attn_out = np.zeros_like(b_attn_map)\n",
    "\n",
    "    # Loop over each attention map in the batch in parallel\n",
    "    for b in prange(b_attn_map.shape[0]):\n",
    "        # Apply Monotonic Alignments Shrink operation to the b-th attention map in the batch\n",
    "        out = mas_width1(b_attn_map[b, 0, : out_lens[b], : in_lens[b]])\n",
    "\n",
    "        # Update the b-th attention map in the output with the result of MAS operation\n",
    "        attn_out[b, 0, : out_lens[b], : in_lens[b]] = out\n",
    "\n",
    "    # Return the batched attention map after applying the MAS operation\n",
    "    return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner(Module):\n",
    "    r\"\"\"DEPRECATED: Aligner class represents a PyTorch module responsible for alignment tasks\n",
    "    in a sequence-to-sequence model. It uses convolutional layers combined with\n",
    "    LeakyReLU activation functions to project inputs to a hidden representation.\n",
    "    The class utilizes both softmax and log-softmax to calculate softmax\n",
    "    along dimension 3.\n",
    "\n",
    "    Args:\n",
    "        d_enc_in (int): Number of channels in the input for the encoder.\n",
    "        d_dec_in (int): Number of channels in the input for the decoder.\n",
    "        d_hidden (int): Number of channels in the output (hidden layers).\n",
    "        kernel_size_enc (int, optional): Size of the convolving kernel for encoder, default is 3.\n",
    "        kernel_size_dec (int, optional): Size of the convolving kernel for decoder, default is 7.\n",
    "        temperature (float, optional): The temperature value applied in Gaussian isotropic\n",
    "            attention mechanism, default is 0.0005.\n",
    "        leaky_relu_slope (float, optional): Controls the angle of the negative slope of\n",
    "            LeakyReLU activation, default is LEAKY_RELU_SLOPE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_enc_in: int,\n",
    "        d_dec_in: int,\n",
    "        d_hidden: int,\n",
    "        kernel_size_enc: int = 3,\n",
    "        kernel_size_dec: int = 7,\n",
    "        temperature: float = 0.0005,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim=3)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=3)\n",
    "\n",
    "        self.key_proj = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                d_enc_in,\n",
    "                d_hidden,\n",
    "                kernel_size=kernel_size_enc,\n",
    "                padding=kernel_size_enc // 2,\n",
    "            ),\n",
    "            nn.LeakyReLU(leaky_relu_slope),\n",
    "            nn.Conv1d(\n",
    "                d_hidden,\n",
    "                d_hidden,\n",
    "                kernel_size=kernel_size_enc,\n",
    "                padding=kernel_size_enc // 2,\n",
    "            ),\n",
    "            nn.LeakyReLU(leaky_relu_slope),\n",
    "        )\n",
    "\n",
    "        self.query_proj = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                d_dec_in,\n",
    "                d_hidden,\n",
    "                kernel_size=kernel_size_dec,\n",
    "                padding=kernel_size_dec // 2,\n",
    "            ),\n",
    "            nn.LeakyReLU(leaky_relu_slope),\n",
    "            nn.Conv1d(\n",
    "                d_hidden,\n",
    "                d_hidden,\n",
    "                kernel_size=kernel_size_dec,\n",
    "                padding=kernel_size_dec // 2,\n",
    "            ),\n",
    "            nn.LeakyReLU(leaky_relu_slope),\n",
    "            nn.Conv1d(\n",
    "                d_hidden,\n",
    "                d_hidden,\n",
    "                kernel_size=kernel_size_dec,\n",
    "                padding=kernel_size_dec // 2,\n",
    "            ),\n",
    "            nn.LeakyReLU(leaky_relu_slope),\n",
    "        )\n",
    "\n",
    "    def binarize_attention_parallel(\n",
    "        self,\n",
    "        attn: torch.Tensor,\n",
    "        in_lens: torch.Tensor,\n",
    "        out_lens: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"For training purposes only! Binarizes attention with MAS.\n",
    "        Binarizes the attention tensor using Maximum Attention Strategy (MAS).\n",
    "\n",
    "        This process is applied for training purposes only and the resulting\n",
    "        binarized attention tensor will no longer receive a gradient in the\n",
    "        backpropagation process.\n",
    "\n",
    "        Args:\n",
    "            attn (Tensor): The attention tensor. Must be of shape (B, 1, max_mel_len, max_text_len),\n",
    "                where B represents the batch size, max_mel_len represents the maximum length\n",
    "                of the mel spectrogram, and max_text_len represents the maximum length of the text.\n",
    "            in_lens (Tensor): A 1D tensor of shape (B,) that contains the input sequence lengths,\n",
    "                which likely corresponds to text sequence lengths.\n",
    "            out_lens (Tensor): A 1D tensor of shape (B,) that contains the output sequence lengths,\n",
    "                which likely corresponds to mel spectrogram lengths.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The binarized attention tensor. The output tensor has the same shape as the input `attn` tensor.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            attn_cpu = np.array(attn.data.cpu().tolist())\n",
    "            # .numpy()\n",
    "            attn_out = b_mas(\n",
    "                attn_cpu,\n",
    "                np.array(in_lens.cpu().tolist()),\n",
    "                # .numpy(),\n",
    "                np.array(out_lens.cpu().tolist()),\n",
    "                # .numpy(),\n",
    "                width=1,\n",
    "            )\n",
    "        return torch.tensor(attn_out)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        enc_in: torch.Tensor,\n",
    "        dec_in: torch.Tensor,\n",
    "        enc_len: torch.Tensor,\n",
    "        dec_len: torch.Tensor,\n",
    "        enc_mask: torch.Tensor,\n",
    "        attn_prior: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Performs the forward pass through the Aligner module.\n",
    "\n",
    "        Args:\n",
    "            enc_in (Tensor): The text encoder outputs.\n",
    "                Must be of shape (B, C_1, T_1), where B is the batch size, C_1 the number of\n",
    "                channels in encoder inputs,\n",
    "                and T_1 the sequence length of encoder inputs.\n",
    "            dec_in (Tensor): The data to align with encoder outputs.\n",
    "                Must be of shape (B, C_2, T_2), where C_2 is the number of channels in decoder inputs,\n",
    "                and T_2 the sequence length of decoder inputs.\n",
    "            enc_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `enc_in`.\n",
    "            dec_len (Tensor): 1D tensor representing the lengths of each sequence in the batch in `dec_in`.\n",
    "            enc_mask (Tensor): Binary mask tensor used to avoid attention to certain timesteps.\n",
    "            attn_prior (Tensor): Previous attention values for attention calculation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor, Tensor]: Returns a tuple of Tensors representing the log-probability, soft attention, hard attention, and hard attention duration.\n",
    "        \"\"\"\n",
    "        queries = dec_in.float()\n",
    "        keys = enc_in.float()\n",
    "        keys_enc = self.key_proj(keys)  # B x n_attn_dims x T2\n",
    "        queries_enc = self.query_proj(queries)\n",
    "\n",
    "        # Simplistic Gaussian Isotopic Attention\n",
    "        attn = (\n",
    "            queries_enc[:, :, :, None] - keys_enc[:, :, None]\n",
    "        ) ** 2  # B x n_attn_dims x T1 x T2\n",
    "        attn = -self.temperature * attn.sum(1, keepdim=True)\n",
    "\n",
    "        if attn_prior is not None:\n",
    "            # print(f\"AlignmentEncoder \\t| mel: {queries.shape} phone: {keys.shape}\n",
    "            # mask: {mask.shape} attn: {attn.shape} attn_prior: {attn_prior.shape}\")\n",
    "            attn = self.log_softmax(attn) + torch.log(\n",
    "                attn_prior.permute((0, 2, 1))[:, None] + 1e-8,\n",
    "            )\n",
    "            # print(f\"AlignmentEncoder \\t| After prior sum attn: {attn.shape}\")\"\"\"\n",
    "\n",
    "        attn_logprob = attn.clone()\n",
    "\n",
    "        if enc_mask is not None:\n",
    "            attn.masked_fill(enc_mask.unsqueeze(1).unsqueeze(1), -float(\"inf\"))\n",
    "\n",
    "        attn_soft = self.softmax(attn)  # softmax along T2\n",
    "        attn_hard = self.binarize_attention_parallel(attn_soft, enc_len, dec_len)\n",
    "        attn_hard_dur = attn_hard.sum(2)[:, 0, :]\n",
    "        return attn_logprob, attn_soft, attn_hard, attn_hard_dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embeddings(shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "    r\"\"\"Initialize embeddings using Kaiming initialization (He initialization).\n",
    "\n",
    "    This method is specifically designed for 2D matrices and helps to avoid\n",
    "    the vanishing/exploding gradient problem in deep neural networks.\n",
    "    This is achieved by keeping the variance of the outputs of a layer to be\n",
    "    the same as the variance of its inputs.\n",
    "\n",
    "    Args:\n",
    "        shape (Tuple[int, ...]): The shape of the embedding matrix to create, denoted as a tuple of integers.\n",
    "                                 The shape should comprise 2 dimensions, i.e., (embedding_dim, num_embeddings).\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: if the provided shape is not 2D.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: the created embedding matrix.\n",
    "    \"\"\"\n",
    "    # Check if the input shape is 2D\n",
    "    assert len(shape) == 2, \"Can only initialize 2-D embedding matrices ...\"\n",
    "\n",
    "    # Initialize the embedding matrix using Kaiming initialization\n",
    "    return torch.randn(shape) * np.sqrt(2 / shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    d_model: int, length: int,\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Function to calculate positional encoding for transformer model.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimension of the model (often corresponds to embedding size).\n",
    "        length (int): Length of sequences.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor having positional encodings.\n",
    "    \"\"\"\n",
    "    # Initialize placeholder for positional encoding\n",
    "    pe = torch.zeros(length, d_model)\n",
    "\n",
    "    # Generate position indices and reshape to have shape (length, 1)\n",
    "    position = torch.arange(0, length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    # Calculate term for division\n",
    "    div_term = torch.exp(\n",
    "        torch.arange(0, d_model, 2).float()\n",
    "        * -(math.log(10000.0) / d_model),\n",
    "    )\n",
    "\n",
    "    # Assign sin of position * div_term to even indices in the encoding matrix\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "    # Assign cos of position * div_term to odd indices in the encoding matrix\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    # Add an extra dimension to match expected output shape\n",
    "    return pe.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class AcousticModel(Module):\n",
    "    r\"\"\"The DelightfulTTS AcousticModel class represents a PyTorch module for an acoustic model in text-to-speech (TTS).\n",
    "    The acoustic model is responsible for predicting speech signals from phoneme sequences.\n",
    "\n",
    "    The model comprises multiple sub-modules including encoder, decoder and various prosody encoders and predictors.\n",
    "    Additionally, a pitch and length adaptor are instantiated.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config (PreprocessingConfig): Object containing the configuration used for preprocessing the data\n",
    "        model_config (AcousticModelConfigType): Configuration object containing various model parameters\n",
    "        n_speakers (int): Total number of speakers in the dataset\n",
    "        leaky_relu_slope (float, optional): Slope for the leaky relu. Defaults to LEAKY_RELU_SLOPE.\n",
    "\n",
    "    Note:\n",
    "        For more specific details on the implementation of sub-modules please refer to their individual respective modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        model_config: AcousticModelConfigType,\n",
    "        n_speakers: int,\n",
    "        leaky_relu_slope: float = LEAKY_RELU_SLOPE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb_dim = model_config.encoder.n_hidden\n",
    "\n",
    "        self.encoder = Conformer(\n",
    "            dim=model_config.encoder.n_hidden,\n",
    "            n_layers=model_config.encoder.n_layers,\n",
    "            n_heads=model_config.encoder.n_heads,\n",
    "            embedding_dim=model_config.speaker_embed_dim + model_config.lang_embed_dim,\n",
    "            p_dropout=model_config.encoder.p_dropout,\n",
    "            kernel_size_conv_mod=model_config.encoder.kernel_size_conv_mod,\n",
    "            with_ff=model_config.encoder.with_ff,\n",
    "        )\n",
    "\n",
    "        self.pitch_adaptor_conv = PitchAdaptorConv(\n",
    "            channels_in=model_config.encoder.n_hidden,\n",
    "            channels_hidden=model_config.variance_adaptor.n_hidden,\n",
    "            channels_out=1,\n",
    "            kernel_size=model_config.variance_adaptor.kernel_size,\n",
    "            emb_kernel_size=model_config.variance_adaptor.emb_kernel_size,\n",
    "            dropout=model_config.variance_adaptor.p_dropout,\n",
    "            leaky_relu_slope=leaky_relu_slope,\n",
    "        )\n",
    "\n",
    "        self.energy_adaptor = EnergyAdaptor(\n",
    "            channels_in=model_config.encoder.n_hidden,\n",
    "            channels_hidden=model_config.variance_adaptor.n_hidden,\n",
    "            channels_out=1,\n",
    "            kernel_size=model_config.variance_adaptor.kernel_size,\n",
    "            emb_kernel_size=model_config.variance_adaptor.emb_kernel_size,\n",
    "            dropout=model_config.variance_adaptor.p_dropout,\n",
    "            leaky_relu_slope=leaky_relu_slope,\n",
    "        )\n",
    "\n",
    "        self.length_regulator = LengthAdaptor(model_config)\n",
    "\n",
    "        self.utterance_prosody_encoder = UtteranceLevelProsodyEncoder(\n",
    "            preprocess_config,\n",
    "            model_config,\n",
    "        )\n",
    "\n",
    "        self.utterance_prosody_predictor = PhonemeProsodyPredictor(\n",
    "            model_config=model_config,\n",
    "            phoneme_level=False,\n",
    "        )\n",
    "\n",
    "        self.phoneme_prosody_encoder = PhonemeLevelProsodyEncoder(\n",
    "            preprocess_config,\n",
    "            model_config,\n",
    "        )\n",
    "\n",
    "        self.phoneme_prosody_predictor = PhonemeProsodyPredictor(\n",
    "            model_config=model_config,\n",
    "            phoneme_level=True,\n",
    "        )\n",
    "\n",
    "        self.u_bottle_out = nn.Linear(\n",
    "            model_config.reference_encoder.bottleneck_size_u,\n",
    "            model_config.encoder.n_hidden,\n",
    "        )\n",
    "\n",
    "        self.u_norm = nn.LayerNorm(\n",
    "            model_config.reference_encoder.bottleneck_size_u,\n",
    "            elementwise_affine=False,\n",
    "        )\n",
    "\n",
    "        self.p_bottle_out = nn.Linear(\n",
    "            model_config.reference_encoder.bottleneck_size_p,\n",
    "            model_config.encoder.n_hidden,\n",
    "        )\n",
    "\n",
    "        self.p_norm = nn.LayerNorm(\n",
    "            model_config.reference_encoder.bottleneck_size_p,\n",
    "            elementwise_affine=False,\n",
    "        )\n",
    "\n",
    "        self.aligner = Aligner(\n",
    "            d_enc_in=model_config.encoder.n_hidden,\n",
    "            d_dec_in=preprocess_config.stft.n_mel_channels,\n",
    "            d_hidden=model_config.encoder.n_hidden,\n",
    "        )\n",
    "\n",
    "        self.decoder = Conformer(\n",
    "            dim=model_config.decoder.n_hidden,\n",
    "            n_layers=model_config.decoder.n_layers,\n",
    "            n_heads=model_config.decoder.n_heads,\n",
    "            embedding_dim=model_config.speaker_embed_dim + model_config.lang_embed_dim,\n",
    "            p_dropout=model_config.decoder.p_dropout,\n",
    "            kernel_size_conv_mod=model_config.decoder.kernel_size_conv_mod,\n",
    "            with_ff=model_config.decoder.with_ff,\n",
    "        )\n",
    "\n",
    "        self.src_word_emb = Parameter(\n",
    "            initialize_embeddings(\n",
    "                (len(symbols), model_config.encoder.n_hidden),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.to_mel = nn.Linear(\n",
    "            model_config.decoder.n_hidden,\n",
    "            preprocess_config.stft.n_mel_channels,\n",
    "        )\n",
    "\n",
    "        # NOTE: here you can manage the speaker embeddings, can be used for the voice export ?\n",
    "        # NOTE: flexibility of the model binded by the n_speaker parameter, maybe I can find another way?\n",
    "        # NOTE: in LIBRITTS there are 2477 speakers, we can add more, just extend the speaker_embed matrix\n",
    "        # Need to think about it more\n",
    "        self.speaker_embed = Parameter(\n",
    "            initialize_embeddings(\n",
    "                (n_speakers, model_config.speaker_embed_dim),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.lang_embed = Parameter(\n",
    "            initialize_embeddings(\n",
    "                (len(SUPPORTED_LANGUAGES), model_config.lang_embed_dim),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_embeddings(\n",
    "        self,\n",
    "        token_idx: torch.Tensor,\n",
    "        speaker_idx: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        lang_idx: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Given the tokens, speakers, source mask, and language indices, compute\n",
    "        the embeddings for tokens, speakers and languages and return the\n",
    "        token_embeddings and combined speaker and language embeddings\n",
    "\n",
    "        Args:\n",
    "            token_idx (torch.Tensor): Tensor of token indices.\n",
    "            speaker_idx (torch.Tensor): Tensor of speaker identities.\n",
    "            src_mask (torch.Tensor): Mask tensor for source sequences.\n",
    "            lang_idx (torch.Tensor): Tensor of language indices.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Token embeddings tensor,\n",
    "            and combined speaker and language embeddings tensor.\n",
    "        \"\"\"\n",
    "        token_embeddings = F.embedding(token_idx, self.src_word_emb)\n",
    "        # NOTE: here you can manage the speaker embeddings, can be used for the voice export ?\n",
    "        speaker_embeds = F.embedding(speaker_idx, self.speaker_embed)\n",
    "        lang_embeds = F.embedding(lang_idx, self.lang_embed)\n",
    "\n",
    "        # Merge the speaker and language embeddings\n",
    "        embeddings = torch.cat([speaker_embeds, lang_embeds], dim=2)\n",
    "\n",
    "        # Apply the mask to the embeddings and token embeddings\n",
    "        embeddings = embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
    "        token_embeddings = token_embeddings.masked_fill(src_mask.unsqueeze(-1), 0.0)\n",
    "\n",
    "        return token_embeddings, embeddings\n",
    "\n",
    "    def prepare_for_export(self) -> None:\n",
    "        r\"\"\"Prepare the model for export.\n",
    "\n",
    "        This method is called when the model is about to be exported, such as for deployment\n",
    "        or serializing for later use. The method removes unnecessary components that are\n",
    "        not needed during inference. Specifically, it removes the phoneme and utterance\n",
    "        prosody encoders for this acoustic model. These components are typically used during\n",
    "        training and are not needed when the model is used for making predictions.\n",
    "\n",
    "        Returns\n",
    "            None\n",
    "        \"\"\"\n",
    "        del self.phoneme_prosody_encoder\n",
    "        del self.utterance_prosody_encoder\n",
    "\n",
    "    # NOTE: freeze/unfreeze params changed, because of the conflict with the lightning module\n",
    "    def freeze_params(self) -> None:\n",
    "        r\"\"\"Freeze the trainable parameters in the model.\n",
    "\n",
    "        By freezing, the parameters are no longer updated by gradient descent.\n",
    "        This is typically done when you want to keep parts of your model fixed while training other parts.\n",
    "        For this model, it freezes all parameters and then selectively unfreezes the\n",
    "        speaker embeddings and the pitch adaptor's pitch embeddings to allow these components to update during training.\n",
    "\n",
    "        Returns\n",
    "            None\n",
    "        \"\"\"\n",
    "        for par in self.parameters():\n",
    "            par.requires_grad = False\n",
    "        self.speaker_embed.requires_grad = True\n",
    "\n",
    "    # NOTE: freeze/unfreeze params changed, because of the conflict with the lightning module\n",
    "    def unfreeze_params(self, freeze_text_embed: bool, freeze_lang_embed: bool) -> None:\n",
    "        r\"\"\"Unfreeze the trainable parameters in the model, allowing them to be updated during training.\n",
    "\n",
    "        This method is typically used to 'unfreeze' previously 'frozen' parameters, making them trainable again.\n",
    "        For this model, it unfreezes all parameters and then selectively freezes the\n",
    "        text embeddings and language embeddings, if required.\n",
    "\n",
    "        Args:\n",
    "            freeze_text_embed (bool): Flag to indicate if text embeddings should remain frozen.\n",
    "            freeze_lang_embed (bool): Flag to indicate if language embeddings should remain frozen.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Iterate through all model parameters and make them trainable\n",
    "        for par in self.parameters():\n",
    "            par.requires_grad = True\n",
    "\n",
    "        # If freeze_text_embed flag is True, keep the source word embeddings frozen\n",
    "        if freeze_text_embed:\n",
    "            # @fixed self.src_word_emb.parameters has no parameters() method!\n",
    "            # for par in self.src_word_emb.parameters():\n",
    "            self.src_word_emb.requires_grad = False\n",
    "\n",
    "        # If freeze_lang_embed flag is True, keep the language embeddings frozen\n",
    "        if freeze_lang_embed:\n",
    "            self.lang_embed.requires_grad = False\n",
    "\n",
    "    def average_utterance_prosody(\n",
    "        self,\n",
    "        u_prosody_pred: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Compute the average utterance prosody over the length of non-masked elements.\n",
    "\n",
    "        This method averages the output of the utterance prosody predictor over\n",
    "        the sequence lengths (non-masked elements). This function will return\n",
    "        a tensor with the same first dimension but singleton trailing dimensions.\n",
    "\n",
    "        Args:\n",
    "            u_prosody_pred (torch.Tensor): Tensor containing the predicted utterance prosody of dimension (batch_size, T, n_features).\n",
    "            src_mask (torch.Tensor): Tensor of dimension (batch_size, T) acting as a mask where masked entries are set to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of dimension (batch_size, 1, n_features) containing average utterance prosody over non-masked sequence length.\n",
    "        \"\"\"\n",
    "        # Compute the real sequence lengths by negating the mask and summing along the sequence dimension\n",
    "        lengths = ((~src_mask) * 1.0).sum(1)\n",
    "\n",
    "        # Compute the sum of u_prosody_pred across the sequence length dimension,\n",
    "        #  then divide by the sequence lengths tensor to calculate the average.\n",
    "        #  This performs a broadcasting operation to account for the third dimension (n_features).\n",
    "        # Return the averaged prosody prediction\n",
    "        return u_prosody_pred.sum(1, keepdim=True) / lengths.view(-1, 1, 1)\n",
    "\n",
    "    def forward_train(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        speakers: torch.Tensor,\n",
    "        src_lens: torch.Tensor,\n",
    "        mels: torch.Tensor,\n",
    "        mel_lens: torch.Tensor,\n",
    "        pitches: torch.Tensor,\n",
    "        langs: torch.Tensor,\n",
    "        attn_priors: Union[torch.Tensor, None],\n",
    "        energies: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        r\"\"\"Forward pass during training phase.\n",
    "\n",
    "        For a given phoneme sequence, speaker identities, sequence lengths, mels,\n",
    "        mel lengths, pitches, language, and attention priors, the forward pass\n",
    "        processes these inputs through the defined architecture.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of phoneme sequence.\n",
    "            speakers (torch.Tensor): Tensor of speaker identities.\n",
    "            src_lens (torch.Tensor): Long tensor representing the lengths of source sequences.\n",
    "            mels (torch.Tensor): Tensor of mel spectrograms.\n",
    "            mel_lens (torch.Tensor): Long tensor representing the lengths of mel sequences.\n",
    "            pitches (torch.Tensor): Tensor of pitch values.\n",
    "            langs (torch.Tensor): Tensor of language identities.\n",
    "            attn_priors (torch.Tensor): Prior attention values.\n",
    "            energies (torch.Tensor): Tensor of energy values.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Returns the prediction outputs as a dictionary.\n",
    "        \"\"\"\n",
    "        # Generate masks for padding positions in the source sequences and mel sequences\n",
    "        src_mask = get_mask_from_lengths(src_lens)\n",
    "        mel_mask = get_mask_from_lengths(mel_lens)\n",
    "\n",
    "        x, embeddings = self.get_embeddings(\n",
    "            token_idx=x,\n",
    "            speaker_idx=speakers,\n",
    "            src_mask=src_mask,\n",
    "            lang_idx=langs,\n",
    "        )\n",
    "\n",
    "        encoding = positional_encoding(\n",
    "            self.emb_dim,\n",
    "            max(x.shape[1], int(mel_lens.max().item())),\n",
    "        )\n",
    "        x = x.to(src_mask.device)\n",
    "        encoding = encoding.to(src_mask.device)\n",
    "        embeddings = embeddings.to(src_mask.device)\n",
    "\n",
    "        x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n",
    "\n",
    "        u_prosody_ref = self.u_norm(\n",
    "            self.utterance_prosody_encoder(mels=mels, mel_lens=mel_lens),\n",
    "        )\n",
    "        u_prosody_pred = self.u_norm(\n",
    "            self.average_utterance_prosody(\n",
    "                u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n",
    "                src_mask=src_mask,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        p_prosody_ref = self.p_norm(\n",
    "            self.phoneme_prosody_encoder(\n",
    "                x=x,\n",
    "                src_mask=src_mask,\n",
    "                mels=mels,\n",
    "                mel_lens=mel_lens,\n",
    "                encoding=encoding,\n",
    "            ),\n",
    "        )\n",
    "        p_prosody_pred = self.p_norm(\n",
    "            self.phoneme_prosody_predictor(\n",
    "                x=x,\n",
    "                mask=src_mask,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        x = x + self.u_bottle_out(u_prosody_pred)\n",
    "        x = x + self.p_bottle_out(p_prosody_pred)\n",
    "\n",
    "        # Save the residual for later use\n",
    "        x_res = x\n",
    "\n",
    "        attn_logprob, attn_soft, attn_hard, attn_hard_dur = self.aligner(\n",
    "            enc_in=x_res.permute((0, 2, 1)),\n",
    "            dec_in=mels,\n",
    "            enc_len=src_lens,\n",
    "            dec_len=mel_lens,\n",
    "            enc_mask=src_mask,\n",
    "            attn_prior=attn_priors,\n",
    "        )\n",
    "\n",
    "        attn_hard_dur = attn_hard_dur.to(src_mask.device)\n",
    "\n",
    "        x, pitch_prediction, avg_pitch_target = (\n",
    "            self.pitch_adaptor_conv.add_pitch_embedding_train(\n",
    "                x=x,\n",
    "                target=pitches,\n",
    "                dr=attn_hard_dur,\n",
    "                mask=src_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        energies = energies.to(src_mask.device)\n",
    "\n",
    "        x, energy_pred, avg_energy_target = (\n",
    "            self.energy_adaptor.add_energy_embedding_train(\n",
    "                x=x,\n",
    "                target=energies,\n",
    "                dr=attn_hard_dur,\n",
    "                mask=src_mask,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        x, log_duration_prediction, embeddings = self.length_regulator.upsample_train(\n",
    "            x=x,\n",
    "            x_res=x_res,\n",
    "            duration_target=attn_hard_dur,\n",
    "            src_mask=src_mask,\n",
    "            embeddings=embeddings,\n",
    "        )\n",
    "\n",
    "        # Decode the encoder output to pred mel spectrogram\n",
    "        decoder_output = self.decoder(\n",
    "            x,\n",
    "            mel_mask,\n",
    "            embeddings=embeddings,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "\n",
    "        y_pred = self.to_mel(decoder_output)\n",
    "        y_pred = y_pred.permute((0, 2, 1))\n",
    "\n",
    "        return {\n",
    "            \"y_pred\": y_pred,\n",
    "            \"pitch_prediction\": pitch_prediction,\n",
    "            \"pitch_target\": avg_pitch_target,\n",
    "            \"energy_pred\": energy_pred,\n",
    "            \"energy_target\": avg_energy_target,\n",
    "            \"log_duration_prediction\": log_duration_prediction,\n",
    "            \"u_prosody_pred\": u_prosody_pred,\n",
    "            \"u_prosody_ref\": u_prosody_ref,\n",
    "            \"p_prosody_pred\": p_prosody_pred,\n",
    "            \"p_prosody_ref\": p_prosody_ref,\n",
    "            \"attn_logprob\": attn_logprob,\n",
    "            \"attn_soft\": attn_soft,\n",
    "            \"attn_hard\": attn_hard,\n",
    "            \"attn_hard_dur\": attn_hard_dur,\n",
    "        }\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        speakers: torch.Tensor,\n",
    "        langs: torch.Tensor,\n",
    "        d_control: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Forward pass during model inference.\n",
    "\n",
    "        The forward pass receives phoneme sequence, speaker identities, languages, pitch control and\n",
    "        duration control, conducts a series of operations on these inputs and returns the predicted mel\n",
    "        spectrogram.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of phoneme sequences.\n",
    "            speakers (torch.Tensor): Tensor of speaker identities.\n",
    "            langs (torch.Tensor): Tensor of language identities.\n",
    "            d_control (float): Duration control parameter. Defaults to 1.0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Predicted mel spectrogram.\n",
    "        \"\"\"\n",
    "        # Generate masks for padding positions in the source sequences\n",
    "        src_mask = get_mask_from_lengths(\n",
    "            torch.tensor([x.shape[1]], dtype=torch.int64),\n",
    "        ).to(x.device)\n",
    "\n",
    "        # Obtain the embeddings for the input\n",
    "        x, embeddings = self.get_embeddings(\n",
    "            token_idx=x,\n",
    "            speaker_idx=speakers,\n",
    "            src_mask=src_mask,\n",
    "            lang_idx=langs,\n",
    "        )\n",
    "\n",
    "        # Generate positional encodings\n",
    "        encoding = positional_encoding(\n",
    "            self.emb_dim,\n",
    "            x.shape[1],\n",
    "        ).to(x.device)\n",
    "\n",
    "        # Process the embeddings through the encoder\n",
    "        x = self.encoder(x, src_mask, embeddings=embeddings, encoding=encoding)\n",
    "\n",
    "        # Predict prosody at utterance level and phoneme level\n",
    "        u_prosody_pred = self.u_norm(\n",
    "            self.average_utterance_prosody(\n",
    "                u_prosody_pred=self.utterance_prosody_predictor(x=x, mask=src_mask),\n",
    "                src_mask=src_mask,\n",
    "            ),\n",
    "        )\n",
    "        p_prosody_pred = self.p_norm(\n",
    "            self.phoneme_prosody_predictor(\n",
    "                x=x,\n",
    "                mask=src_mask,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        x = x + self.u_bottle_out(u_prosody_pred)\n",
    "        x = x + self.p_bottle_out(p_prosody_pred)\n",
    "\n",
    "        x_res = x\n",
    "\n",
    "        x, _ = self.pitch_adaptor_conv.add_pitch_embedding(\n",
    "            x=x,\n",
    "            mask=src_mask,\n",
    "        )\n",
    "\n",
    "        x, _ = self.energy_adaptor.add_energy_embedding(\n",
    "            x=x,\n",
    "            mask=src_mask,\n",
    "        )\n",
    "\n",
    "        x, _, embeddings = self.length_regulator.upsample(\n",
    "            x=x,\n",
    "            x_res=x_res,\n",
    "            src_mask=src_mask,\n",
    "            control=d_control,\n",
    "            embeddings=embeddings,\n",
    "        )\n",
    "\n",
    "        mel_mask = get_mask_from_lengths(\n",
    "            torch.tensor([x.shape[1]], dtype=torch.int64),\n",
    "        ).to(x.device)\n",
    "\n",
    "        if x.shape[1] > encoding.shape[1]:\n",
    "            encoding = positional_encoding(self.emb_dim, x.shape[1]).to(x.device)\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            x,\n",
    "            mel_mask,\n",
    "            embeddings=embeddings,\n",
    "            encoding=encoding,\n",
    "        )\n",
    "\n",
    "        x = self.to_mel(decoder_output)\n",
    "        x = x.permute((0, 2, 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinLoss(Module):\n",
    "    r\"\"\"Binary cross-entropy loss for hard and soft attention.\n",
    "\n",
    "    Attributes\n",
    "        None\n",
    "\n",
    "    Methods\n",
    "        forward: Computes the binary cross-entropy loss for hard and soft attention.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, hard_attention: torch.Tensor, soft_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Computes the binary cross-entropy loss for hard and soft attention.\n",
    "\n",
    "        Args:\n",
    "            hard_attention (torch.Tensor): A binary tensor indicating the hard attention.\n",
    "            soft_attention (torch.Tensor): A tensor containing the soft attention probabilities.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The binary cross-entropy loss.\n",
    "\n",
    "        \"\"\"\n",
    "        log_sum = torch.log(\n",
    "            torch.clamp(soft_attention[hard_attention == 1], min=1e-12),\n",
    "        ).sum()\n",
    "        return -log_sum / hard_attention.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardSumLoss(Module):\n",
    "    r\"\"\"Computes the forward sum loss for sequence-to-sequence models with attention.\n",
    "\n",
    "    Args:\n",
    "        blank_logprob (float): The log probability of the blank symbol. Default: -1.\n",
    "\n",
    "    Attributes:\n",
    "        log_softmax (nn.LogSoftmax): The log softmax function.\n",
    "        ctc_loss (nn.CTCLoss): The CTC loss function.\n",
    "        blank_logprob (float): The log probability of the blank symbol.\n",
    "\n",
    "    Methods:\n",
    "        forward: Computes the forward sum loss for sequence-to-sequence models with attention.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, blank_logprob: float = -1):\n",
    "        super().__init__()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=3)\n",
    "        self.ctc_loss = nn.CTCLoss(zero_infinity=True)\n",
    "        self.blank_logprob = blank_logprob\n",
    "\n",
    "    def forward(\n",
    "        self, attn_logprob: torch.Tensor, in_lens: torch.Tensor, out_lens: torch.Tensor,\n",
    "    ) -> float:\n",
    "        r\"\"\"Computes the forward sum loss for sequence-to-sequence models with attention.\n",
    "\n",
    "        Args:\n",
    "            attn_logprob (torch.Tensor): The attention log probabilities of shape (batch_size, max_out_len, max_in_len).\n",
    "            in_lens (torch.Tensor): The input lengths of shape (batch_size,).\n",
    "            out_lens (torch.Tensor): The output lengths of shape (batch_size,).\n",
    "\n",
    "        Returns:\n",
    "            float: The forward sum loss.\n",
    "\n",
    "        \"\"\"\n",
    "        key_lens = in_lens\n",
    "        query_lens = out_lens\n",
    "        attn_logprob_padded = F.pad(\n",
    "            input=attn_logprob, pad=(1, 0), value=self.blank_logprob,\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for bid in range(attn_logprob.shape[0]):\n",
    "            target_seq = torch.arange(1, int(key_lens[bid]) + 1).unsqueeze(0)\n",
    "            curr_logprob = attn_logprob_padded[bid].permute(1, 0, 2)[\n",
    "                : int(query_lens[bid]), :, : int(key_lens[bid]) + 1,\n",
    "            ]\n",
    "\n",
    "            curr_logprob = self.log_softmax(curr_logprob[None])[0]\n",
    "            loss = self.ctc_loss(\n",
    "                curr_logprob,\n",
    "                target_seq,\n",
    "                input_lengths=query_lens[bid : bid + 1],\n",
    "                target_lengths=key_lens[bid : bid + 1],\n",
    "            )\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= attn_logprob.shape[0]\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_wise_min_max(x: Tensor) -> Tensor:\n",
    "    r\"\"\"Applies sample-wise min-max normalization to a tensor.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor of shape (batch_size, num_samples, num_features).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized tensor of the same shape as the input tensor.\n",
    "    \"\"\"\n",
    "    # Compute the maximum and minimum values of each sample in the batch\n",
    "    maximum = torch.amax(x, dim=(1, 2), keepdim=True)\n",
    "    minimum = torch.amin(x, dim=(1, 2), keepdim=True)\n",
    "\n",
    "    # Apply sample-wise min-max normalization to the input tensor\n",
    "    return (x - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/codec/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from piq import SSIMLoss\n",
    "\n",
    "class FastSpeech2LossGen(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bin_warmup: bool = True,\n",
    "        binarization_loss_enable_steps: int = 1260,\n",
    "        binarization_loss_warmup_steps: int = 700,\n",
    "    ):\n",
    "        r\"\"\"Initializes the FastSpeech2LossGen module.\n",
    "\n",
    "        Args:\n",
    "            bin_warmup (bool, optional): Whether to use binarization warmup. Defaults to True. NOTE: Switch this off if you preload the model with a checkpoint that has already passed the warmup phase.\n",
    "            binarization_loss_enable_steps (int, optional): Number of steps to enable the binarization loss. Defaults to 1260.\n",
    "            binarization_loss_warmup_steps (int, optional): Number of warmup steps for the binarization loss. Defaults to 700.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "        self.ssim_loss = SSIMLoss()\n",
    "        self.sum_loss = ForwardSumLoss()\n",
    "        self.bin_loss = BinLoss()\n",
    "\n",
    "        self.bin_warmup = bin_warmup\n",
    "        self.binarization_loss_enable_steps = binarization_loss_enable_steps\n",
    "        self.binarization_loss_warmup_steps = binarization_loss_warmup_steps\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src_masks: torch.Tensor,\n",
    "        mel_masks: torch.Tensor,\n",
    "        mel_targets: torch.Tensor,\n",
    "        mel_predictions: torch.Tensor,\n",
    "        log_duration_predictions: torch.Tensor,\n",
    "        u_prosody_ref: torch.Tensor,\n",
    "        u_prosody_pred: torch.Tensor,\n",
    "        p_prosody_ref: torch.Tensor,\n",
    "        p_prosody_pred: torch.Tensor,\n",
    "        durations: torch.Tensor,\n",
    "        pitch_predictions: torch.Tensor,\n",
    "        p_targets: torch.Tensor,\n",
    "        attn_logprob: torch.Tensor,\n",
    "        attn_soft: torch.Tensor,\n",
    "        attn_hard: torch.Tensor,\n",
    "        step: int,\n",
    "        src_lens: torch.Tensor,\n",
    "        mel_lens: torch.Tensor,\n",
    "        energy_pred: torch.Tensor,\n",
    "        energy_target: torch.Tensor,\n",
    "    ) -> Tuple[\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "    ]:\n",
    "        r\"\"\"Computes the loss for the FastSpeech2 model.\n",
    "\n",
    "        Args:\n",
    "            src_masks (torch.Tensor): Mask for the source sequence.\n",
    "            mel_masks (torch.Tensor): Mask for the mel-spectrogram.\n",
    "            mel_targets (torch.Tensor): Target mel-spectrogram.\n",
    "            mel_predictions (torch.Tensor): Predicted mel-spectrogram.\n",
    "            log_duration_predictions (torch.Tensor): Predicted log-duration.\n",
    "            u_prosody_ref (torch.Tensor): Reference unvoiced prosody.\n",
    "            u_prosody_pred (torch.Tensor): Predicted unvoiced prosody.\n",
    "            p_prosody_ref (torch.Tensor): Reference voiced prosody.\n",
    "            p_prosody_pred (torch.Tensor): Predicted voiced prosody.\n",
    "            durations (torch.Tensor): Ground-truth durations.\n",
    "            pitch_predictions (torch.Tensor): Predicted pitch.\n",
    "            p_targets (torch.Tensor): Ground-truth pitch.\n",
    "            attn_logprob (torch.Tensor): Log-probability of attention.\n",
    "            attn_soft (torch.Tensor): Soft attention.\n",
    "            attn_hard (torch.Tensor): Hard attention.\n",
    "            step (int): Current training step.\n",
    "            src_lens (torch.Tensor): Lengths of the source sequences.\n",
    "            mel_lens (torch.Tensor): Lengths of the mel-spectrograms.\n",
    "            energy_pred (torch.Tensor): Predicted energy.\n",
    "            energy_target (torch.Tensor): Ground-truth energy.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The total loss and its components.\n",
    "\n",
    "        Note:\n",
    "            Here is the description of the returned loss components:\n",
    "            `total_loss`: This is the total loss computed as the sum of all the other losses.\n",
    "            `mel_loss`: This is the mean absolute error (MAE) loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms.\n",
    "            `sc_mag_loss`: This is the spectral convergence loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n",
    "            `log_mag_loss`: This is the log STFT magnitude loss between the predicted and target mel-spectrograms. It measures how well the model predicts the mel-spectrograms in terms of their spectral structure.\n",
    "            `ssim_loss`: This is the Structural Similarity Index (SSIM) loss between the predicted and target mel-spectrograms. It measures the similarity between the two mel-spectrograms in terms of their structure, contrast, and luminance.\n",
    "            `duration_loss`: This is the mean squared error (MSE) loss between the predicted and target log-durations. It measures how well the model predicts the durations of the phonemes.\n",
    "            `u_prosody_loss`: This is the MAE loss between the predicted and reference unvoiced prosody. It measures how well the model predicts the prosody (rhythm, stress, and intonation) of the unvoiced parts of the speech.\n",
    "            `p_prosody_loss`: This is the MAE loss between the predicted and reference voiced prosody. It measures how well the model predicts the prosody of the voiced parts of the speech.\n",
    "            `pitch_loss`: This is the MSE loss between the predicted and target pitch. It measures how well the model predicts the pitch of the speech.\n",
    "            `ctc_loss`: This is the Connectionist Temporal Classification (CTC) loss computed from the log-probability of attention and the lengths of the source sequences and mel-spectrograms. It measures how well the model aligns the input and output sequences.\n",
    "            `bin_loss`: This is the binarization loss computed from the hard and soft attention. It measures how well the model learns to attend to the correct parts of the input sequence.\n",
    "            `energy_loss`: This is the MSE loss between the predicted and target energy. It measures how well the model predicts the energy of the speech.\n",
    "        \"\"\"\n",
    "        log_duration_targets = torch.log(durations.float() + 1).to(src_masks.device)\n",
    "\n",
    "        log_duration_targets.requires_grad = False\n",
    "        mel_targets.requires_grad = False\n",
    "        p_targets.requires_grad = False\n",
    "        energy_target.requires_grad = False\n",
    "\n",
    "        log_duration_predictions = log_duration_predictions.masked_select(~src_masks)\n",
    "        log_duration_targets = log_duration_targets.masked_select(~src_masks)\n",
    "\n",
    "        mel_masks_expanded = mel_masks.unsqueeze(1)\n",
    "\n",
    "        mel_predictions_normalized = (\n",
    "            sample_wise_min_max(mel_predictions).float().to(mel_predictions.device)\n",
    "        )\n",
    "        mel_targets_normalized = (\n",
    "            sample_wise_min_max(mel_targets).float().to(mel_predictions.device)\n",
    "        )\n",
    "\n",
    "        ssim_loss: torch.Tensor = self.ssim_loss(\n",
    "            mel_predictions_normalized.unsqueeze(1),\n",
    "            mel_targets_normalized.unsqueeze(1),\n",
    "        )\n",
    "\n",
    "        if ssim_loss.item() > 1.0 or ssim_loss.item() < 0.0:\n",
    "            ssim_loss = torch.tensor([1.0], device=mel_predictions.device)\n",
    "\n",
    "        masked_mel_predictions = mel_predictions.masked_select(~mel_masks_expanded)\n",
    "\n",
    "        masked_mel_targets = mel_targets.masked_select(~mel_masks_expanded)\n",
    "\n",
    "        mel_loss: torch.Tensor = self.mae_loss(\n",
    "            masked_mel_predictions,\n",
    "            masked_mel_targets,\n",
    "        )\n",
    "\n",
    "        p_prosody_ref = p_prosody_ref.permute((0, 2, 1))\n",
    "        p_prosody_pred = p_prosody_pred.permute((0, 2, 1))\n",
    "\n",
    "        p_prosody_ref = p_prosody_ref.masked_fill(src_masks.unsqueeze(1), 0.0)\n",
    "        p_prosody_pred = p_prosody_pred.masked_fill(src_masks.unsqueeze(1), 0.0)\n",
    "\n",
    "        p_prosody_ref = p_prosody_ref.detach()\n",
    "\n",
    "        p_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n",
    "            p_prosody_ref.masked_select(~src_masks.unsqueeze(1)),\n",
    "            p_prosody_pred.masked_select(~src_masks.unsqueeze(1)),\n",
    "        )\n",
    "\n",
    "        u_prosody_ref = u_prosody_ref.detach()\n",
    "        u_prosody_loss: torch.Tensor = 0.5 * self.mae_loss(\n",
    "            u_prosody_ref,\n",
    "            u_prosody_pred,\n",
    "        )\n",
    "\n",
    "        duration_loss: torch.Tensor = self.mse_loss(\n",
    "            log_duration_predictions,\n",
    "            log_duration_targets,\n",
    "        )\n",
    "\n",
    "        pitch_predictions = pitch_predictions.masked_select(~src_masks)\n",
    "        p_targets = p_targets.masked_select(~src_masks)\n",
    "\n",
    "        pitch_loss: torch.Tensor = self.mse_loss(pitch_predictions, p_targets)\n",
    "\n",
    "        ctc_loss: torch.Tensor = self.sum_loss(\n",
    "            attn_logprob=attn_logprob,\n",
    "            in_lens=src_lens,\n",
    "            out_lens=mel_lens,\n",
    "        )\n",
    "\n",
    "        if self.bin_warmup:\n",
    "            if step < self.binarization_loss_enable_steps:\n",
    "                bin_loss_weight = 0.0\n",
    "            else:\n",
    "                bin_loss_weight = (\n",
    "                    min(\n",
    "                        (step - self.binarization_loss_enable_steps)\n",
    "                        / self.binarization_loss_warmup_steps,\n",
    "                        1.0,\n",
    "                    )\n",
    "                    * 1.0\n",
    "                )\n",
    "\n",
    "            bin_loss: torch.Tensor = (\n",
    "                self.bin_loss(hard_attention=attn_hard, soft_attention=attn_soft)\n",
    "                * bin_loss_weight\n",
    "            )\n",
    "        else:\n",
    "            bin_loss: torch.Tensor = self.bin_loss(\n",
    "                hard_attention=attn_hard,\n",
    "                soft_attention=attn_soft,\n",
    "            )\n",
    "\n",
    "        energy_loss: torch.Tensor = self.mse_loss(energy_pred, energy_target)\n",
    "\n",
    "        total_loss = (\n",
    "            mel_loss\n",
    "            + duration_loss\n",
    "            + u_prosody_loss\n",
    "            + p_prosody_loss\n",
    "            + ssim_loss\n",
    "            + pitch_loss\n",
    "            + ctc_loss\n",
    "            + bin_loss\n",
    "            + energy_loss\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            total_loss,\n",
    "            mel_loss,\n",
    "            ssim_loss,\n",
    "            duration_loss,\n",
    "            u_prosody_loss,\n",
    "            p_prosody_loss,\n",
    "            pitch_loss,\n",
    "            ctc_loss,\n",
    "            bin_loss,\n",
    "            energy_loss,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selected speakers from the HiFiTTS dataset\n",
    "speakers_hifi_ids = [\n",
    "    # \"Cori Samuel\",  # 92,\n",
    "    # \"Tony Oliva\",  # 6671,\n",
    "    # \"John Van Stan\",  # 9017,\n",
    "    # \"Helen Taylor\",  # 9136,\n",
    "    # \"Phil Benson\",  # 6097,\n",
    "    # \"Mike Pelton\",  # 6670,\n",
    "    # \"Maria Kasper\",  # 8051,\n",
    "    # \"Sylviamb\",  # 11614,\n",
    "    # \"Celine Major\",  # 11697,\n",
    "    # \"LikeManyWaters\",  # 12787,\n",
    "]\n",
    "\n",
    "# The selected speakers from the LibriTTS dataset\n",
    "speakers_libri_ids = list(\n",
    "    map(\n",
    "        str,\n",
    "        [\n",
    "            84\n",
    "            # train-clean-100\n",
    "            # 40,\n",
    "            # 1088,\n",
    "            # train-clean-360\n",
    "            # 3307,\n",
    "            # 5935,\n",
    "            # train-other-500\n",
    "            # 215,\n",
    "            # 6594,\n",
    "            # 3867,\n",
    "            # 5733,\n",
    "            # 5181,\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "selected_speakers_ids = {\n",
    "    v: k\n",
    "    for k, v in enumerate(\n",
    "        speakers_hifi_ids + speakers_libri_ids,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreprocessingConfigHifiGAN(PreprocessingConfig):\n",
    "    stft: STFTConfig = field(\n",
    "        default_factory=lambda: STFTConfig(\n",
    "            filter_length=1024,\n",
    "            hop_length=256,\n",
    "            win_length=1024,\n",
    "            n_mel_channels=80,  # For univnet 100\n",
    "            mel_fmin=20,\n",
    "            mel_fmax=11025,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        r\"\"\"It modifies the 'stft' attribute based on the 'sampling_rate' attribute.\n",
    "        If 'sampling_rate' is 44100, 'stft' is set with specific values for this rate.\n",
    "        If 'sampling_rate' is not 22050 or 44100, a ValueError is raised.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If 'sampling_rate' is not 22050 or 44100.\n",
    "        \"\"\"\n",
    "        if self.sampling_rate == 44100:\n",
    "            self.stft = STFTConfig(\n",
    "                filter_length=2048,\n",
    "                hop_length=512,  # NOTE: 441 ?? https://github.com/jik876/hifi-gan/issues/116#issuecomment-1436999858\n",
    "                win_length=2048,\n",
    "                n_mel_channels=80,  # Based on https://github.com/jik876/hifi-gan/issues/116\n",
    "                mel_fmin=20,\n",
    "                mel_fmax=11025,\n",
    "            )\n",
    "        if self.sampling_rate not in [22050, 44100]:\n",
    "            raise ValueError(\"Sampling rate must be 22050 or 44100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lhotse import CutSet, RecordingSet, SupervisionSet\n",
    "\n",
    "def prep_2_cutset(prep: Dict[str, Dict[str, RecordingSet | SupervisionSet]]) -> CutSet:\n",
    "    r\"\"\"Prepare the dataset for the model. This function is used to convert the prepared dataset to a CutSet.\n",
    "\n",
    "    Args:\n",
    "        prep (Dict[str, Dict[str, RecordingSet | SupervisionSet]]): The prepared dataset.\n",
    "\n",
    "    Returns:\n",
    "        CutSet: The dataset prepared for the model.\n",
    "    \"\"\"\n",
    "    recordings_hifi = RecordingSet()\n",
    "    supervisions_hifi = SupervisionSet()\n",
    "\n",
    "    for hifi_row in prep.values():\n",
    "        record = hifi_row[\"recordings\"]\n",
    "        supervision = hifi_row[\"supervisions\"]\n",
    "\n",
    "        # Separate the recordings and supervisions\n",
    "        if isinstance(record, RecordingSet):\n",
    "            recordings_hifi += record\n",
    "\n",
    "        if isinstance(supervision, SupervisionSet):\n",
    "            supervisions_hifi += supervision\n",
    "\n",
    "    # Add the recordings and supervisions to the CutSet\n",
    "    return CutSet.from_manifests(\n",
    "        recordings=recordings_hifi,\n",
    "        supervisions=supervisions_hifi,\n",
    "    )\n",
    "\n",
    "\n",
    "DATASET_TYPES = Literal[\"hifitts\", \"libritts\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HifiLibriItem:\n",
    "    \"\"\"Dataset row for the HiFiTTS and LibriTTS datasets combined in this code.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the item.\n",
    "        wav (Tensor): The waveform of the audio.\n",
    "        mel (Tensor): The mel spectrogram.\n",
    "        pitch (Tensor): The pitch.\n",
    "        text (Tensor): The text.\n",
    "        attn_prior (Tensor): The attention prior.\n",
    "        energy (Tensor): The energy.\n",
    "        raw_text (str): The raw text.\n",
    "        normalized_text (str): The normalized text.\n",
    "        speaker (int): The speaker ID.\n",
    "        pitch_is_normalized (bool): Whether the pitch is normalized.\n",
    "        lang (int): The language ID.\n",
    "        dataset_type (DATASET_TYPES): The type of dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    id: str\n",
    "    wav: Tensor\n",
    "    mel: Tensor\n",
    "    pitch: Tensor\n",
    "    text: Tensor\n",
    "    attn_prior: Tensor\n",
    "    energy: Tensor\n",
    "    raw_text: str\n",
    "    normalized_text: str\n",
    "    speaker: int\n",
    "    pitch_is_normalized: bool\n",
    "    lang: int\n",
    "    dataset_type: DATASET_TYPES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from typing import Dict, List, Literal, Optional, Tuple\n",
    "from lhotse.cut import MonoCut\n",
    "from lhotse.recipes import hifitts, libritts\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from voicefixer import VoiceFixer\n",
    "\n",
    "# from models.config import PreprocessingConfigHifiGAN as PreprocessingConfig\n",
    "# from models.config import get_lang_map, lang2id\n",
    "# from training.preprocess import PreprocessLibriTTS\n",
    "# from training.tools import pad_1D, pad_2D, pad_3D\n",
    "\n",
    "NUM_JOBS = (os.cpu_count() or 2) - 1\n",
    "\n",
    "class HifiLibriDataset(Dataset):\n",
    "    r\"\"\"A PyTorch dataset for loading delightful TTS data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang: str = \"en\",\n",
    "        root: str = \"datasets_cache\",\n",
    "        sampling_rate: int = 44100,\n",
    "        hifitts_path: str = \"hifitts\",\n",
    "        hifi_cutset_file_name: str = \"hifi.json.gz\",\n",
    "        libritts_path: str = \"librittsr\",\n",
    "        libritts_cutset_file_name: str = \"libri.json.gz\",\n",
    "        libritts_subsets: List[str] | str = \"dev-clean\",\n",
    "        cache: bool = False,\n",
    "        cache_dir: str = \"/dev/shm\",\n",
    "        num_jobs: int = NUM_JOBS,\n",
    "        min_seconds: Optional[float] = None,\n",
    "        max_seconds: Optional[float] = None,\n",
    "        include_libri: bool = True,\n",
    "        libri_speakers: List[str] = speakers_libri_ids,\n",
    "        hifi_speakers: List[str] = speakers_hifi_ids,\n",
    "    ):\n",
    "        r\"\"\"Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            lang (str, optional): The language of the dataset. Defaults to \"en\".\n",
    "            root (str, optional): The root directory of the dataset. Defaults to \"datasets_cache\".\n",
    "            sampling_rate (int, optional): The sampling rate of the audio. Defaults to 44100.\n",
    "            hifitts_path (str, optional): The path to the HiFiTTS dataset. Defaults to \"hifitts\".\n",
    "            hifi_cutset_file_name (str, optional): The file name of the HiFiTTS cutset. Defaults to \"hifi.json.gz\".\n",
    "            libritts_path (str, optional): The path to the LibriTTS dataset. Defaults to \"librittsr\".\n",
    "            libritts_cutset_file_name (str, optional): The file name of the LibriTTS cutset. Defaults to \"libri.json.gz\".\n",
    "            libritts_subsets (Union[List[str], str], optional): The subsets of the LibriTTS dataset to use. Defaults to \"all\".\n",
    "            cache (bool, optional): Whether to cache the dataset. Defaults to False.\n",
    "            cache_dir (str, optional): The directory to cache the dataset in. Defaults to \"/dev/shm\".\n",
    "            num_jobs (int, optional): The number of jobs to use for preparing the dataset. Defaults to NUM_JOBS.\n",
    "            min_seconds (Optional[float], optional): The minimum duration of the audio. Defaults from the preprocess config.\n",
    "            max_seconds (Optional[float], optional): The maximum duration of the audio. Defaults from the preprocess config.\n",
    "            include_libri (bool, optional): Whether to include the LibriTTS dataset. Defaults to True.\n",
    "            libri_speakers (List[str], optional): The selected speakers from the LibriTTS dataset. Defaults to selected_speakers_libri_ids.\n",
    "            hifi_speakers (List[str], optional): The selected speakers from the HiFiTTS dataset. Defaults to selected_speakers_hi_fi_ids.\n",
    "        \"\"\"\n",
    "        lang_map = get_lang_map(lang)\n",
    "        processing_lang_type = lang_map.processing_lang_type\n",
    "        self.preprocess_config = PreprocessingConfigHifiGAN(\n",
    "            processing_lang_type,\n",
    "            sampling_rate=sampling_rate,\n",
    "        )\n",
    "\n",
    "        self.min_seconds = min_seconds or self.preprocess_config.min_seconds\n",
    "        self.max_seconds = max_seconds or self.preprocess_config.max_seconds\n",
    "\n",
    "        self.dur_filter = (\n",
    "            lambda duration: duration >= self.min_seconds\n",
    "            and duration <= self.max_seconds\n",
    "        )\n",
    "\n",
    "        self.preprocess_libtts = PreprocessLibriTTS(\n",
    "            self.preprocess_config,\n",
    "            lang,\n",
    "        )\n",
    "        self.root_dir = Path(root)\n",
    "        self.voicefixer = VoiceFixer()\n",
    "\n",
    "        # Map the speaker ids to string and list of selected speaker ids to set\n",
    "        self.selected_speakers_libri_ids_ = set(libri_speakers)\n",
    "        self.selected_speakers_hi_fi_ids_ = set(hifi_speakers)\n",
    "\n",
    "        self.cache = cache\n",
    "        self.cache_dir = Path(cache_dir) / f\"cache-{libritts_path}\"\n",
    "\n",
    "        # Prepare the HiFiTTS dataset\n",
    "        self.hifitts_path = self.root_dir / hifitts_path\n",
    "        hifi_cutset_file_path = self.root_dir / hifi_cutset_file_name\n",
    "\n",
    "        # Initialize the cutset\n",
    "        self.cutset = CutSet()\n",
    "\n",
    "        # Check if the HiFiTTS dataset has been prepared\n",
    "        # if hifi_cutset_file_path.exists():\n",
    "        #     self.cutset_hifi = CutSet.from_file(hifi_cutset_file_path)\n",
    "        # else:\n",
    "        #     hifitts_root = hifitts.download_hifitts(self.hifitts_path)\n",
    "        #     prepared_hifi = hifitts.prepare_hifitts(\n",
    "        #         hifitts_root,\n",
    "        #         num_jobs=num_jobs,\n",
    "        #     )\n",
    "\n",
    "        #     # Add the recordings and supervisions to the CutSet\n",
    "        #     self.cutset_hifi = prep_2_cutset(prepared_hifi)\n",
    "        #     # Save the prepared HiFiTTS dataset cutset\n",
    "        #     self.cutset_hifi.to_file(hifi_cutset_file_path)\n",
    "\n",
    "        # # Filter the HiFiTTS cutset to only include the selected speakers\n",
    "        # self.cutset_hifi = self.cutset_hifi.filter(\n",
    "        #     lambda cut: isinstance(cut, MonoCut)\n",
    "        #     and str(cut.supervisions[0].speaker) in self.selected_speakers_hi_fi_ids_\n",
    "        #     and self.dur_filter(cut.duration),\n",
    "        # ).to_eager()\n",
    "\n",
    "        # Add the HiFiTTS cutset to the final cutset\n",
    "        # self.cutset += self.cutset_hifi\n",
    "\n",
    "        if include_libri:\n",
    "            # Prepare the LibriTTS dataset\n",
    "            self.libritts_path = self.root_dir / libritts_path\n",
    "            libritts_cutset_file_path = self.root_dir / libritts_cutset_file_name\n",
    "\n",
    "            # Check if the LibriTTS dataset has been prepared\n",
    "            if libritts_cutset_file_path.exists():\n",
    "                self.cutset_libri = CutSet.from_file(libritts_cutset_file_path)\n",
    "            else:\n",
    "                libritts_root = libritts.download_librittsr(\n",
    "                    self.libritts_path,\n",
    "                    dataset_parts=libritts_subsets,\n",
    "                )\n",
    "                prepared_libri = libritts.prepare_librittsr(\n",
    "                    libritts_root / \"LibriTTS_R\",\n",
    "                    dataset_parts=libritts_subsets,\n",
    "                    num_jobs=num_jobs,\n",
    "                )\n",
    "\n",
    "                # Add the recordings and supervisions to the CutSet\n",
    "                self.cutset_libri = prep_2_cutset(prepared_libri)\n",
    "                # Save the prepared cutset for LibriTTS\n",
    "                self.cutset_libri.to_file(libritts_cutset_file_path)\n",
    "\n",
    "            # Filter the libri cutset to only include the selected speakers\n",
    "            self.cutset_libri = self.cutset_libri.filter(\n",
    "                lambda cut: isinstance(cut, MonoCut)\n",
    "                and str(cut.supervisions[0].speaker)\n",
    "                in self.selected_speakers_libri_ids_\n",
    "                and self.dur_filter(cut.duration),\n",
    "            ).to_eager()\n",
    "\n",
    "            # Add the LibriTTS cutset to the final cutset\n",
    "            self.cutset += self.cutset_libri\n",
    "\n",
    "        # to_eager() is used to evaluates all lazy operations on this manifest\n",
    "        self.cutset = self.cutset.to_eager()\n",
    "\n",
    "    def get_cache_subdir_path(self, idx: int) -> Path:\n",
    "        r\"\"\"Calculate the path to the cache subdirectory.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the cache subdirectory.\n",
    "\n",
    "        Returns:\n",
    "            Path: The path to the cache subdirectory.\n",
    "        \"\"\"\n",
    "        return self.cache_dir / str(((idx // 1000) + 1) * 1000)\n",
    "\n",
    "    def get_cache_file_path(self, idx: int) -> Path:\n",
    "        r\"\"\"Calculate the path to the cache file.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the cache file.\n",
    "\n",
    "        Returns:\n",
    "            Path: The path to the cache file.\n",
    "        \"\"\"\n",
    "        return self.get_cache_subdir_path(idx) / f\"{idx}.pt\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        r\"\"\"Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.cutset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> HifiLibriItem:\n",
    "        r\"\"\"Returns the item at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item.\n",
    "\n",
    "        Returns:\n",
    "            HifiLibriItem: The item at the specified index.\n",
    "        \"\"\"\n",
    "        cache_file = self.get_cache_file_path(idx)\n",
    "\n",
    "        if self.cache and cache_file.exists():\n",
    "            cached_data: Dict = torch.load(cache_file)\n",
    "            # Cast the cached data to the PreprocessForAcousticResult class\n",
    "            result = HifiLibriItem(**cached_data)\n",
    "            return result\n",
    "\n",
    "        print(self.cutset)\n",
    "        print(idx)\n",
    "        cutset = self.cutset[idx]\n",
    "\n",
    "        if isinstance(cutset, MonoCut) and cutset.recording is not None:\n",
    "            dataset_speaker_id = str(cutset.supervisions[0].speaker)\n",
    "\n",
    "            # Map the dataset speaker id to the speaker id in the model\n",
    "            speaker_id = selected_speakers_ids.get(\n",
    "                dataset_speaker_id,\n",
    "                len(selected_speakers_ids) + 1,\n",
    "            )\n",
    "\n",
    "            # Run voicefixer only for the libri speakers\n",
    "            # if str(dataset_speaker_id) in self.selected_speakers_libri_ids_:\n",
    "            #     audio_path = cutset.recording.sources[0].source\n",
    "            #     # Restore LibriTTS-R audio\n",
    "            #     with tempfile.NamedTemporaryFile(\n",
    "            #         suffix=\".wav\",\n",
    "            #         delete=True,\n",
    "            #     ) as out_file:\n",
    "            #         self.voicefixer.restore(\n",
    "            #             input=audio_path,  # low quality .wav/.flac file\n",
    "            #             output=out_file.name,  # save file path\n",
    "            #             cuda=False,  # GPU acceleration\n",
    "            #             mode=0,\n",
    "            #         )\n",
    "            #         audio, _ = sf.read(out_file.name)\n",
    "            #         # Convert the np audio to a tensor\n",
    "            #         audio = torch.from_numpy(audio).float().unsqueeze(0)\n",
    "            # else:\n",
    "            #     # Load the audio from the cutset\n",
    "            #     audio = torch.from_numpy(cutset.load_audio())\n",
    "\n",
    "\n",
    "            #     detach = audio.data.detach().tolist()       \n",
    "            #     audio = np.array(detach, dtype=float)\n",
    "            # # audio\n",
    "            # audio_np = resample(audio, orig_sr=sr_actual, target_sr=sr)\n",
    "            # # Convert back to torch tensor\n",
    "            # audio = torch.tensor(audio_np)\n",
    "\n",
    "            audio = torch.tensor(np.array(cutset.load_audio(), dtype=float))\n",
    "\n",
    "            # audio = torch.from_numpy(cutset.load_audio())\n",
    "\n",
    "            text: str = str(cutset.supervisions[0].text)\n",
    "\n",
    "            fileid = str(cutset.supervisions[0].recording_id)\n",
    "\n",
    "            split_fileid = fileid.split(\"_\")\n",
    "            chapter_id = split_fileid[1]\n",
    "            utterance_id = split_fileid[-1]\n",
    "\n",
    "            libri_row = (\n",
    "                audio,\n",
    "                cutset.sampling_rate,\n",
    "                text,\n",
    "                text,\n",
    "                speaker_id,\n",
    "                chapter_id,\n",
    "                utterance_id,\n",
    "            )\n",
    "            \n",
    "            data = self.preprocess_libtts.acoustic(libri_row)\n",
    "\n",
    "            if data is None:\n",
    "                rand_idx = int(\n",
    "                    torch.randint(\n",
    "                        0,\n",
    "                        self.__len__(),\n",
    "                        (1,),\n",
    "                    ).item(),\n",
    "                )\n",
    "                return self.__getitem__(rand_idx)\n",
    "\n",
    "            data.wav = data.wav.unsqueeze(0)\n",
    "\n",
    "            result = HifiLibriItem(\n",
    "                id=data.utterance_id,\n",
    "                wav=data.wav,\n",
    "                mel=data.mel,\n",
    "                pitch=data.pitch,\n",
    "                text=data.phones,\n",
    "                attn_prior=data.attn_prior,\n",
    "                energy=data.energy,\n",
    "                raw_text=data.raw_text,\n",
    "                normalized_text=data.normalized_text,\n",
    "                speaker=speaker_id,\n",
    "                pitch_is_normalized=data.pitch_is_normalized,\n",
    "                lang=lang2id[\"en\"],\n",
    "                dataset_type=\"libritts\",\n",
    "\n",
    "                # dataset_type=\"hifitts\" if idx < len(self.cutset_hifi) else \"libritts\",\n",
    "            )\n",
    "\n",
    "            if self.cache:\n",
    "                # Create the cache subdirectory if it doesn't exist\n",
    "                Path.mkdir(\n",
    "                    self.get_cache_subdir_path(idx),\n",
    "                    parents=True,\n",
    "                    exist_ok=True,\n",
    "                )\n",
    "                # Save the preprocessed data to the cache\n",
    "                torch.save(asdict(result), cache_file)\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Cut not found at index {idx}.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        r\"\"\"Method makes the class iterable. It iterates over the `_walker` attribute\n",
    "        and for each item, it gets the corresponding item from the dataset using the\n",
    "        `__getitem__` method.\n",
    "\n",
    "        Yields:\n",
    "        The item from the dataset corresponding to the current item in `_walker`.\n",
    "        \"\"\"\n",
    "        for item in range(self.__len__()):\n",
    "            yield self.__getitem__(item)\n",
    "\n",
    "    def collate_fn(self, data: List[HifiLibriItem]) -> List:\n",
    "        r\"\"\"Collates a batch of data samples.\n",
    "\n",
    "        Args:\n",
    "            data (List[HifiLibriItem]): A list of data samples.\n",
    "\n",
    "        Returns:\n",
    "            List: A list of reprocessed data batches.\n",
    "        \"\"\"\n",
    "        data_size = len(data)\n",
    "\n",
    "        idxs = list(range(data_size))\n",
    "\n",
    "        # Initialize empty lists to store extracted values\n",
    "        empty_lists: List[List] = [[] for _ in range(12)]\n",
    "        (\n",
    "            ids,\n",
    "            speakers,\n",
    "            texts,\n",
    "            raw_texts,\n",
    "            mels,\n",
    "            pitches,\n",
    "            attn_priors,\n",
    "            langs,\n",
    "            src_lens,\n",
    "            mel_lens,\n",
    "            wavs,\n",
    "            energy,\n",
    "        ) = empty_lists\n",
    "\n",
    "        # Extract fields from data dictionary and populate the lists\n",
    "        for idx in idxs:\n",
    "            data_entry = data[idx]\n",
    "            ids.append(data_entry.id)\n",
    "            speakers.append(data_entry.speaker)\n",
    "            texts.append(data_entry.text)\n",
    "            raw_texts.append(data_entry.raw_text)\n",
    "            mels.append(data_entry.mel)\n",
    "            pitches.append(data_entry.pitch)\n",
    "            attn_priors.append(data_entry.attn_prior)\n",
    "            langs.append(data_entry.lang)\n",
    "            src_lens.append(data_entry.text.shape[0])\n",
    "            mel_lens.append(data_entry.mel.shape[1])\n",
    "            wavs.append(data_entry.wav)\n",
    "            energy.append(data_entry.energy)\n",
    "\n",
    "        # Convert langs, src_lens, and mel_lens to numpy arrays\n",
    "        langs = np.array(langs)\n",
    "        src_lens = np.array(src_lens)\n",
    "        mel_lens = np.array(mel_lens)\n",
    "\n",
    "        # NOTE: Instead of the pitches for the whole dataset, used stat for the batch\n",
    "        # Take only min and max values for pitch\n",
    "        pitches_stat = list(self.normalize_pitch(pitches)[:2])\n",
    "\n",
    "        texts = pad_1D(texts)\n",
    "        mels = pad_2D(mels)\n",
    "        pitches = pad_1D(pitches)\n",
    "        attn_priors = pad_3D(attn_priors, len(idxs), max(src_lens), max(mel_lens))\n",
    "\n",
    "        speakers = np.repeat(\n",
    "            np.expand_dims(np.array(speakers), axis=1),\n",
    "            texts.shape[1],\n",
    "            axis=1,\n",
    "        )\n",
    "        langs = np.repeat(\n",
    "            np.expand_dims(np.array(langs), axis=1),\n",
    "            texts.shape[1],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        wavs = pad_2D(wavs)\n",
    "        energy = pad_2D(energy)\n",
    "\n",
    "        return [\n",
    "            ids,\n",
    "            raw_texts,\n",
    "            torch.tensor(speakers, dtype=int),\n",
    "            texts.int(),\n",
    "            torch.tensor(src_lens, dtype=int),\n",
    "            mels,\n",
    "            pitches,\n",
    "            pitches_stat,\n",
    "            torch.tensor(mel_lens, dtype=int),\n",
    "            torch.tensor(langs, dtype=int),\n",
    "            attn_priors,\n",
    "            wavs,\n",
    "            energy,\n",
    "        ]\n",
    "\n",
    "    def normalize_pitch(\n",
    "        self,\n",
    "        pitches: List[torch.Tensor],\n",
    "    ) -> Tuple[float, float, float, float]:\n",
    "        r\"\"\"Normalizes the pitch values.\n",
    "\n",
    "        Args:\n",
    "            pitches (List[torch.Tensor]): A list of pitch values.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the normalized pitch values.\n",
    "        \"\"\"\n",
    "        pitches_t = torch.concatenate(pitches)\n",
    "\n",
    "        min_value = torch.min(pitches_t).item()\n",
    "        max_value = torch.max(pitches_t).item()\n",
    "\n",
    "        mean = torch.mean(pitches_t).item()\n",
    "        std = torch.std(pitches_t).item()\n",
    "\n",
    "        return min_value, max_value, mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataloader(\n",
    "    batch_size: int = 6,\n",
    "    num_workers: int = 5,\n",
    "    sampling_rate: int = 22050,\n",
    "    shuffle: bool = False,\n",
    "    lang: str = \"en\",\n",
    "    root: str = \"datasets_cache\",\n",
    "    hifitts_path: str = \"hifitts\",\n",
    "    hifi_cutset_file_name: str = \"hifi.json.gz\",\n",
    "    libritts_path: str = \"librittsr\",\n",
    "    libritts_cutset_file_name: str = \"libri.json.gz\",\n",
    "    libritts_subsets: List[str] | str = \"all\",\n",
    "    cache: bool = False,\n",
    "    cache_dir: str = \"/dev/shm\",\n",
    "    include_libri: bool = True,\n",
    "    libri_speakers: List[str] = speakers_libri_ids,\n",
    "    hifi_speakers: List[str] = speakers_hifi_ids,\n",
    ") -> DataLoader:\n",
    "    r\"\"\"Returns the training dataloader, that is using the HifiLibriDataset dataset.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): The batch size.\n",
    "        num_workers (int): The number of workers.\n",
    "        sampling_rate (int): The sampling rate of the audio. Defaults to 22050.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "        lang (str): The language of the dataset.\n",
    "        root (str): The root directory of the dataset.\n",
    "        hifitts_path (str): The path to the HiFiTTS dataset.\n",
    "        hifi_cutset_file_name (str): The file name of the HiFiTTS cutset.\n",
    "        libritts_path (str): The path to the LibriTTS dataset.\n",
    "        libritts_cutset_file_name (str): The file name of the LibriTTS cutset.\n",
    "        libritts_subsets (List[str] | str): The subsets of the LibriTTS dataset to use.\n",
    "        cache (bool): Whether to cache the dataset.\n",
    "        cache_dir (str): The directory to cache the dataset in.\n",
    "        include_libri (bool): Whether to include the LibriTTS dataset.\n",
    "        libri_speakers (List[str]): The selected speakers from the LibriTTS dataset.\n",
    "        hifi_speakers (List[str]): The selected speakers from the HiFiTTS dataset.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: The training dataloader.\n",
    "    \"\"\"\n",
    "    dataset = HifiLibriDataset(\n",
    "        root=root,\n",
    "        hifitts_path=hifitts_path,\n",
    "        sampling_rate=sampling_rate,\n",
    "        hifi_cutset_file_name=hifi_cutset_file_name,\n",
    "        libritts_path=libritts_path,\n",
    "        libritts_cutset_file_name=libritts_cutset_file_name,\n",
    "        libritts_subsets=libritts_subsets,\n",
    "        cache=cache,\n",
    "        cache_dir=cache_dir,\n",
    "        lang=lang,\n",
    "        include_libri=include_libri,\n",
    "        libri_speakers=libri_speakers,\n",
    "        hifi_speakers=hifi_speakers,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        # 4x80Gb max 10 sec audio\n",
    "        # batch_size=20, # self.train_config.batch_size,\n",
    "        # 4*80Gb max ~20.4 sec audio\n",
    "        batch_size=batch_size,\n",
    "        # TODO: find the optimal num_workers\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=dataset.collate_fn,\n",
    "    )\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "from lightning.pytorch.core import LightningModule\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from models.config import (\n",
    "#     AcousticFinetuningConfig,\n",
    "#     AcousticModelConfigType,\n",
    "#     AcousticMultilingualModelConfig,\n",
    "#     AcousticPretrainingConfig,\n",
    "#     AcousticTrainingConfig,\n",
    "#     PreprocessingConfig,\n",
    "#     get_lang_map,\n",
    "#     lang2id,\n",
    "# )\n",
    "# from models.helpers.tools import get_mask_from_lengths\n",
    "# from training.datasets.hifi_libri_dataset import (\n",
    "#     speakers_hifi_ids,\n",
    "#     speakers_libri_ids,\n",
    "#     train_dataloader,\n",
    "# )\n",
    "# from training.loss import FastSpeech2LossGen\n",
    "# from training.preprocess.normalize_text import NormalizeText\n",
    "\n",
    "# # Updated version of the tokenizer\n",
    "# from training.preprocess.tokenizer_ipa_espeak import TokenizerIpaEspeak as TokenizerIPA\n",
    "\n",
    "# from .acoustic_model import AcousticModel\n",
    "\n",
    "MEL_SPEC_EVERY_N_STEPS = 1000\n",
    "AUDIO_EVERY_N_STEPS = 100\n",
    "\n",
    "\n",
    "class DelightfulTTS(LightningModule):\n",
    "    r\"\"\"Trainer for the acoustic model.\n",
    "\n",
    "    Args:\n",
    "        preprocess_config PreprocessingConfig: The preprocessing configuration.\n",
    "        model_config AcousticModelConfigType: The model configuration.\n",
    "        fine_tuning (bool, optional): Whether to use fine-tuning mode or not. Defaults to False.\n",
    "        bin_warmup (bool, optional): Whether to use binarization warmup for the loss or not. Defaults to True.\n",
    "        lang (str): Language of the dataset.\n",
    "        n_speakers (int): Number of speakers in the dataset.generation during training.\n",
    "        batch_size (int): The batch size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocess_config: PreprocessingConfig,\n",
    "        model_config: AcousticModelConfigType = AcousticENModelConfig(),\n",
    "        fine_tuning: bool = False,\n",
    "        bin_warmup: bool = True,\n",
    "        lang: str = \"en\",\n",
    "        n_speakers: int = 5392,\n",
    "        batch_size: int = 19,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lang = lang\n",
    "        self.lang_id = lang2id[self.lang]\n",
    "\n",
    "        self.fine_tuning = fine_tuning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        lang_map = get_lang_map(lang)\n",
    "        normilize_text_lang = lang_map.nemo\n",
    "\n",
    "        self.tokenizer = TokenizerIpaEspeak(lang)\n",
    "        self.normilize_text = NormalizeText(normilize_text_lang)\n",
    "\n",
    "        self.train_config_acoustic = AcousticPretrainingConfig()\n",
    "\n",
    "        self.preprocess_config = preprocess_config\n",
    "\n",
    "        # TODO: fix the arguments!\n",
    "        self.acoustic_model = AcousticModel(\n",
    "            preprocess_config=self.preprocess_config,\n",
    "            model_config=model_config,\n",
    "            # NOTE: this parameter may be hyperparameter that you can define based on the demands\n",
    "            n_speakers=n_speakers,\n",
    "        )\n",
    "\n",
    "        # NOTE: in case of training from 0 bin_warmup should be True!\n",
    "        self.loss_acoustic = FastSpeech2LossGen(\n",
    "            bin_warmup=bin_warmup,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text: str,\n",
    "        speaker_idx: Tensor,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Performs a forward pass through the AcousticModel.\n",
    "        This code must be run only with the loaded weights from the checkpoint!\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text.\n",
    "            speaker_idx (Tensor): The index of the speaker\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The generated waveform with hifi-gan.\n",
    "        \"\"\"\n",
    "        normalized_text = self.normilize_text(text)\n",
    "        _, phones = self.tokenizer(normalized_text)\n",
    "\n",
    "        # Convert to tensor\n",
    "        x = torch.tensor(\n",
    "            phones,\n",
    "            dtype=torch.int,\n",
    "            device=speaker_idx.device,\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        speakers = speaker_idx.repeat(x.shape[1]).unsqueeze(0)\n",
    "\n",
    "        langs = (\n",
    "            torch.tensor(\n",
    "                [self.lang_id],\n",
    "                dtype=torch.int,\n",
    "                device=speaker_idx.device,\n",
    "            )\n",
    "            .repeat(x.shape[1])\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        mel_pred = self.acoustic_model.forward(\n",
    "            x=x,\n",
    "            speakers=speakers,\n",
    "            langs=langs,\n",
    "        )\n",
    "\n",
    "        return mel_pred\n",
    "\n",
    "    def training_step(self, batch: List, _: int):\n",
    "        r\"\"\"Performs a training step for the model.\n",
    "\n",
    "        Args:\n",
    "        batch (List): The batch of data for training. The batch should contain:\n",
    "            - ids: List of indexes.\n",
    "            - raw_texts: Raw text inputs.\n",
    "            - speakers: Speaker identities.\n",
    "            - texts: Text inputs.\n",
    "            - src_lens: Lengths of the source sequences.\n",
    "            - mels: Mel spectrogram targets.\n",
    "            - pitches: Pitch targets.\n",
    "            - pitches_stat: Statistics of the pitches.\n",
    "            - mel_lens: Lengths of the mel spectrograms.\n",
    "            - langs: Language identities.\n",
    "            - attn_priors: Prior attention weights.\n",
    "            - wavs: Waveform targets.\n",
    "            - energies: Energy targets.\n",
    "        batch_idx (int): Index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            - 'loss': The total loss for the training step.\n",
    "        \"\"\"\n",
    "        (\n",
    "            _,\n",
    "            _,\n",
    "            speakers,\n",
    "            texts,\n",
    "            src_lens,\n",
    "            mels,\n",
    "            pitches,\n",
    "            _,\n",
    "            mel_lens,\n",
    "            langs,\n",
    "            attn_priors,\n",
    "            _,\n",
    "            energies,\n",
    "        ) = batch\n",
    "\n",
    "        outputs = self.acoustic_model.forward_train(\n",
    "            x=texts,\n",
    "            speakers=speakers,\n",
    "            src_lens=src_lens,\n",
    "            mels=mels,\n",
    "            mel_lens=mel_lens,\n",
    "            pitches=pitches,\n",
    "            langs=langs,\n",
    "            attn_priors=attn_priors,\n",
    "            energies=energies,\n",
    "        )\n",
    "\n",
    "        y_pred = outputs[\"y_pred\"]\n",
    "        log_duration_prediction = outputs[\"log_duration_prediction\"]\n",
    "        p_prosody_ref = outputs[\"p_prosody_ref\"]\n",
    "        p_prosody_pred = outputs[\"p_prosody_pred\"]\n",
    "        pitch_prediction = outputs[\"pitch_prediction\"]\n",
    "        energy_pred = outputs[\"energy_pred\"]\n",
    "        energy_target = outputs[\"energy_target\"]\n",
    "\n",
    "        src_mask = get_mask_from_lengths(src_lens)\n",
    "        mel_mask = get_mask_from_lengths(mel_lens)\n",
    "\n",
    "        (\n",
    "            total_loss,\n",
    "            mel_loss,\n",
    "            ssim_loss,\n",
    "            duration_loss,\n",
    "            u_prosody_loss,\n",
    "            p_prosody_loss,\n",
    "            pitch_loss,\n",
    "            ctc_loss,\n",
    "            bin_loss,\n",
    "            energy_loss,\n",
    "        ) = self.loss_acoustic.forward(\n",
    "            src_masks=src_mask,\n",
    "            mel_masks=mel_mask,\n",
    "            mel_targets=mels,\n",
    "            mel_predictions=y_pred,\n",
    "            log_duration_predictions=log_duration_prediction,\n",
    "            u_prosody_ref=outputs[\"u_prosody_ref\"],\n",
    "            u_prosody_pred=outputs[\"u_prosody_pred\"],\n",
    "            p_prosody_ref=p_prosody_ref,\n",
    "            p_prosody_pred=p_prosody_pred,\n",
    "            pitch_predictions=pitch_prediction,\n",
    "            p_targets=outputs[\"pitch_target\"],\n",
    "            durations=outputs[\"attn_hard_dur\"],\n",
    "            attn_logprob=outputs[\"attn_logprob\"],\n",
    "            attn_soft=outputs[\"attn_soft\"],\n",
    "            attn_hard=outputs[\"attn_hard\"],\n",
    "            src_lens=src_lens,\n",
    "            mel_lens=mel_lens,\n",
    "            energy_pred=energy_pred,\n",
    "            energy_target=energy_target,\n",
    "            step=self.trainer.global_step,\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"train_total_loss\",\n",
    "            total_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\"train_mel_loss\", mel_loss, sync_dist=True, batch_size=self.batch_size)\n",
    "        self.log(\n",
    "            \"train_ssim_loss\",\n",
    "            ssim_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_duration_loss\",\n",
    "            duration_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_u_prosody_loss\",\n",
    "            u_prosody_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_p_prosody_loss\",\n",
    "            p_prosody_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_pitch_loss\",\n",
    "            pitch_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        self.log(\"train_ctc_loss\", ctc_loss, sync_dist=True, batch_size=self.batch_size)\n",
    "        self.log(\"train_bin_loss\", bin_loss, sync_dist=True, batch_size=self.batch_size)\n",
    "        self.log(\n",
    "            \"train_energy_loss\",\n",
    "            energy_loss,\n",
    "            sync_dist=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        r\"\"\"Configures the optimizer used for training.\n",
    "\n",
    "        Returns\n",
    "            tuple: A tuple containing three dictionaries. Each dictionary contains the optimizer and learning rate scheduler for one of the models.\n",
    "        \"\"\"\n",
    "        lr_decay = self.train_config_acoustic.optimizer_config.lr_decay\n",
    "        default_lr = self.train_config_acoustic.optimizer_config.learning_rate\n",
    "\n",
    "        init_lr = (\n",
    "            default_lr\n",
    "            if self.trainer.global_step == 0\n",
    "            else default_lr * (lr_decay**self.trainer.global_step)\n",
    "        )\n",
    "\n",
    "        optimizer_acoustic = AdamW(\n",
    "            self.acoustic_model.parameters(),\n",
    "            lr=init_lr,\n",
    "            betas=self.train_config_acoustic.optimizer_config.betas,\n",
    "            eps=self.train_config_acoustic.optimizer_config.eps,\n",
    "            weight_decay=self.train_config_acoustic.optimizer_config.weight_decay,\n",
    "        )\n",
    "\n",
    "        scheduler_acoustic = ExponentialLR(optimizer_acoustic, gamma=lr_decay)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer_acoustic,\n",
    "            \"lr_scheduler\": scheduler_acoustic,\n",
    "        }\n",
    "\n",
    "    def train_dataloader(\n",
    "        self,\n",
    "        root: str = \"datasets_cache\",\n",
    "        cache: bool = True,\n",
    "        cache_dir: str = \"/dev/shm\",\n",
    "        include_libri: bool = False,\n",
    "        libri_speakers: List[str] = speakers_libri_ids,\n",
    "        hifi_speakers: List[str] = speakers_hifi_ids,\n",
    "    ) -> DataLoader:\n",
    "        r\"\"\"Returns the training dataloader, that is using the LibriTTS dataset.\n",
    "\n",
    "        Args:\n",
    "            root (str): The root directory of the dataset.\n",
    "            cache (bool): Whether to cache the preprocessed data.\n",
    "            cache_dir (str): The directory for the cache. Defaults to \"/dev/shm\".\n",
    "            include_libri (bool): Whether to include the LibriTTS dataset or not.\n",
    "            libri_speakers (List[str]): The list of LibriTTS speakers to include.\n",
    "            hifi_speakers (List[str]): The list of HiFi-GAN speakers to include.\n",
    "\n",
    "        Returns:\n",
    "            Tupple[DataLoader, DataLoader]: The training and validation dataloaders.\n",
    "        \"\"\"\n",
    "        return train_dataloader(\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.preprocess_config.workers,\n",
    "            sampling_rate=self.preprocess_config.sampling_rate,\n",
    "            root=root,\n",
    "            cache=cache,\n",
    "            cache_dir=cache_dir,\n",
    "            lang=self.lang,\n",
    "            include_libri=include_libri,\n",
    "            libri_speakers=libri_speakers,\n",
    "            hifi_speakers=hifi_speakers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from voicefixer import Vocoder\n",
    "\n",
    "\n",
    "cache_dir = \"datasets_cache\"\n",
    "dataset = HifiLibriDataset(cache_dir=cache_dir, cache=True)\n",
    "vocoder_vf = Vocoder(44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['000000'], ['\"It is a young girl.\"'], tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), tensor([[ 2, 63, 71, 50,  4, 42, 63, 71, 21, 50, 11, 52, 63, 71, 71, 63, 71, 50,\n",
      "          4, 42, 63, 71, 50, 38, 20, 63, 71, 71, 63, 71, 50,  7, 42, 63, 71, 71,\n",
      "         63, 71, 24, 50,  4, 42, 63, 71, 50, 17, 45, 63, 71, 12, 50, 22, 52, 63,\n",
      "         71, 50, 38, 16, 63, 71,  6, 48, 50, 11, 52, 63, 71, 71, 63, 71,  6, 48,\n",
      "         50, 11, 52, 63, 71, 50,  4, 42, 63, 71, 50, 35, 52, 40, 63, 71, 50, 38,\n",
      "         14, 63, 71, 63, 63, 71,  3]], dtype=torch.int32), tensor([97]), tensor([[[-2.1233, -1.5432, -1.8827,  ..., -3.0057, -3.1312, -2.7474],\n",
      "         [-3.0421, -3.1670, -3.9047,  ..., -3.7185, -3.7156, -3.9338],\n",
      "         [-3.5968, -3.7097, -4.5922,  ..., -4.7067, -4.4175, -4.1783],\n",
      "         ...,\n",
      "         [-7.4233, -7.5637, -8.3259,  ..., -8.2843, -8.3605, -8.1514],\n",
      "         [-7.5640, -7.6775, -8.4629,  ..., -8.4034, -8.4349, -8.0191],\n",
      "         [-7.6708, -7.8295, -8.8069,  ..., -8.5231, -8.3674, -8.1590]]]), tensor([[220.5000, 220.5000, 220.5000, 220.5000, 220.5000, 220.5000, 220.5000,\n",
      "         220.5000, 220.5000, 220.5000, 218.3168, 216.1765, 214.0777, 207.0423,\n",
      "         190.0862, 200.4545, 207.0423, 206.0748, 205.1163, 204.1667, 201.3699,\n",
      "         198.6487, 196.0000, 194.2731, 190.0862, 191.7987, 193.5112, 195.2237,\n",
      "         196.9362, 198.6487, 193.4211, 190.0862, 186.0759, 182.9875, 181.4815,\n",
      "         180.0000, 178.5425, 177.1084, 177.1084, 182.2314, 186.0759, 188.4615,\n",
      "         187.6596, 186.0759, 183.7500, 181.4815, 179.2683, 178.5425, 178.5425,\n",
      "         180.0000, 180.7377, 181.4815, 182.9875, 184.5188, 185.2941, 186.0759,\n",
      "         186.0759, 184.5188, 182.2314, 179.2683, 176.4000, 175.0000, 181.4815,\n",
      "         181.4815, 180.0000, 178.5425, 177.1084, 175.0000, 173.6220, 171.5953,\n",
      "         168.3206, 163.9405, 160.3636, 158.0645, 158.0645, 159.2058, 160.3636,\n",
      "         162.7306, 167.6806, 170.9302, 172.9412, 172.9412, 172.9412, 172.9412,\n",
      "         172.9412, 172.9412, 171.5953, 163.9405, 163.9405, 163.9405, 163.9405,\n",
      "         163.9405, 163.9405, 163.9405, 163.9405, 163.9405, 163.9405, 163.9405,\n",
      "         163.9405, 163.9405, 163.9405, 163.9405, 163.9405]]), [158.06451416015625, 220.5], tensor([103]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0]]), tensor([[[0.5150, 0.2640, 0.1347,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.2510, 0.2586, 0.1989,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.1217, 0.1891, 0.1948,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1582, 0.1222, 0.0587],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1948, 0.1891, 0.1217],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.1989, 0.2586, 0.2510]]]), tensor([[[ 5.7416e-10, -2.3966e-09, -1.0654e-09,  ..., -1.1736e-02,\n",
      "          -1.0726e-02, -5.6898e-03]]]), tensor([[[-1.4597, -1.3317, -1.3688, -1.4857, -1.4995, -1.5062, -1.5026,\n",
      "          -1.4434, -0.7073,  0.4967,  1.0257,  1.2229,  0.8492, -0.2568,\n",
      "          -0.7398, -0.2599,  0.4757,  1.1944,  1.6178,  1.6525,  1.3992,\n",
      "           0.9977,  0.4447, -0.1428, -0.5718, -0.7729, -0.6947, -0.7146,\n",
      "          -0.4628,  0.3601,  0.7372,  0.7525,  0.5343,  0.4040,  0.4719,\n",
      "           0.4452,  0.1590, -0.0030, -0.0263,  0.0432,  0.5187,  1.1273,\n",
      "           1.4782,  1.7023,  1.7763,  1.5350,  1.4026,  1.4066,  1.3493,\n",
      "           1.2757,  1.1850,  0.9682,  0.7763,  0.6008,  0.5036,  0.4422,\n",
      "           0.3888,  0.3141,  0.2377,  0.1101, -0.2453, -0.7095, -0.5057,\n",
      "           0.2232,  0.7802,  1.1203,  1.2946,  1.3681,  1.2436,  1.0435,\n",
      "           0.8343,  0.5941,  0.4264,  0.3348,  0.2336,  0.0413, -0.1006,\n",
      "          -0.2329, -0.2322, -0.1992, -0.2035, -0.1732, -0.1674, -0.2779,\n",
      "          -0.3399, -0.5916, -1.0042, -1.2468, -1.2715, -1.4084, -1.3350,\n",
      "          -1.2840, -1.3373, -1.4344, -1.4130, -1.3281, -1.3199, -1.4103,\n",
      "          -1.3919, -1.3928, -1.4672, -1.4696, -1.4770]]])]\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=dataset.collate_fn,\n",
    "    # persistent_workers=True,\n",
    "    # pin_memory=True,\n",
    "    # num_workers=2,\n",
    ")\n",
    "\n",
    "# preprocessing_config =  PreprocessingConfig(\n",
    "#     \"english_only\",\n",
    "#     stft=STFTConfig(\n",
    "#         filter_length=1024,\n",
    "#         hop_length=256,\n",
    "#         win_length=1024,\n",
    "#         n_mel_channels=100,\n",
    "#         mel_fmin=20,\n",
    "#         mel_fmax=11025,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "preprocessing_config =  PreprocessingConfigHifiGAN('multilingual')\n",
    "\n",
    "dataloader_iterator = iter(dataloader)\n",
    "\n",
    "# # Now you can fetch batches from it\n",
    "first_batch = next(dataloader_iterator)\n",
    "print(first_batch)  # This will print the first batch\n",
    "\n",
    "\n",
    "default_root_dir = \"checkpoints/vcoder\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    default_root_dir=default_root_dir,\n",
    "    fast_dev_run=1,\n",
    "    limit_train_batches=1,\n",
    "    max_epochs=1,\n",
    ")\n",
    "\n",
    "module = DelightfulTTS(preprocess_config=preprocessing_config)\n",
    "# module.train_dataloader()\n",
    "# result = trainer.fit(module, train_dataloaders=dataloader)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | acoustic_model | AcousticModel      | 133 M  | train\n",
      "1 | loss_acoustic  | FastSpeech2LossGen | 0      | train\n",
      "--------------------------------------------------------------\n",
      "133 M     Trainable params\n",
      "0         Non-trainable params\n",
      "133 M     Total params\n",
      "534.261   Total estimated model params size (MB)\n",
      "799       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/user/codec/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:08<00:00,  0.12it/s]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit(module, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
