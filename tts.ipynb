{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/cb/hhdf12sj7sn4s1r6my6y211w0000gn/T/ipykernel_69448/1976994746.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/user/codec/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# print(torch.__version__)\n",
    "# print(torchaudio.__version__)\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = \"_-!'(),.:;? abcdefghijklmnopqrstuvwxyz\"\n",
    "# look_up = {s: i for i, s in enumerate(symbols)}\n",
    "# symbols = set(symbols)\n",
    "\n",
    "\n",
    "# def text_to_sequence(text):\n",
    "#     text = text.lower()\n",
    "#     return [look_up[s] for s in text if s in symbols]\n",
    "\n",
    "\n",
    "# text = \"Hello world! Text to speech!\"\n",
    "# print(text_to_sequence(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         outputs, (hidden, cell) = self.rnn(x)\n",
    "#         return outputs, hidden, cell\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, hidden_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.attention = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "#     def forward(self, encoder_outputs, hidden):\n",
    "#         scores = torch.bmm(encoder_outputs, hidden.unsqueeze(2)).squeeze(2)\n",
    "#         attention_weights = torch.softmax(scores, dim=1)\n",
    "#         context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "#         return context\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, hidden_dim, output_dim):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "#     def forward(self, x, context):\n",
    "#         rnn_output, _ = self.rnn(x)\n",
    "#         output = self.fc(rnn_output + context.unsqueeze(1))\n",
    "#         return output\n",
    "\n",
    "# class TTSModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(TTSModel, self).__init__()\n",
    "#         self.encoder = Encoder(input_dim, hidden_dim)\n",
    "#         self.attention = Attention(hidden_dim)\n",
    "#         self.decoder = Decoder(hidden_dim, output_dim)\n",
    "    \n",
    "#     def forward(self, text_features, mel_spectrogram_target):\n",
    "#         encoder_outputs, hidden, cell = self.encoder(text_features)\n",
    "#         context = self.attention(encoder_outputs, hidden[-1])\n",
    "#         output = self.decoder(mel_spectrogram_target, context)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = torchaudio.datasets.LJSPEECH(root='dataset', download=True)\n",
    "# test_dataset = torchaudio.datasets.LJSPEECH(root='dataset', download=True)\n",
    "train_dataset = torchaudio.datasets.CMUARCTIC(root='dataset', download=True)\n",
    "test_dataset = torchaudio.datasets.CMUARCTIC(root='dataset', download=True)\n",
    "waveform, sample_rate, transcript, utterance_id = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_dataset[0]\n",
    "# waveform, sample_rate, transcript, utterance_id = train_dataset[0]\n",
    "# print(f'Waveform shape: {waveform.shape}')\n",
    "# print(f'Sample rate: {sample_rate}')\n",
    "# print(f'Utterance: {utterance_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, shuffle=True)\n",
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     waveform, sample_rate, transcript, utterance_id = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchaudio.transforms as T\n",
    "# # 1. Data Preprocessing\n",
    "\n",
    "\n",
    "# def preprocess_text(transcript):\n",
    "#     return torch.tensor(text_to_sequence(transcript), dtype=torch.long)\n",
    "\n",
    "# def preprocess_audio(waveform, sample_rate):\n",
    "#     mel_spectrogram_transform = T.MelSpectrogram(\n",
    "#         sample_rate=sample_rate,\n",
    "#         n_fft=1024,\n",
    "#         hop_length=256,\n",
    "#         n_mels=80\n",
    "#     )\n",
    "#     mel_spectrogram = mel_spectrogram_transform(waveform)\n",
    "#     return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/codec/.venv/lib/python3.12/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = torchaudio.datasets.CMUARCTIC(root='dataset', download=True)\n",
    "test_dataset = torchaudio.datasets.CMUARCTIC(root='dataset', download=True)\n",
    "\n",
    "# Example preprocessing\n",
    "def preprocess_data(dataset):\n",
    "    audio_data = []\n",
    "    text_data = []\n",
    "    for waveform, sample_rate, transcript, _ in dataset:\n",
    "        # Convert waveform to mel spectrogram\n",
    "        mel_spectrogram = T.MelSpectrogram()(waveform)\n",
    "        audio_data.append(mel_spectrogram)\n",
    "        \n",
    "        # Encode text data (simple character encoding for demo)\n",
    "        text_data.append(transcript)\n",
    "    \n",
    "    return audio_data, text_data\n",
    "\n",
    "train_audio_data, train_text_data = preprocess_data(train_dataset)\n",
    "test_audio_data, test_text_data = preprocess_data(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTTSModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleTTSModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=80, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(256, 80)  # Output should match mel spectrogram dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "model = SimpleTTSModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_sequence(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if seq.size(1) < max_length:\n",
    "            # Pad the sequence with zeros\n",
    "            padding_size = max_length - seq.size(1)\n",
    "            padded_seq = F.pad(seq, (0, padding_size), mode='constant', value=0)\n",
    "        else:\n",
    "            # Truncate the sequence if it's too long\n",
    "            padded_seq = seq[:, :max_length]\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return torch.stack(padded_sequences)\n",
    "\n",
    "# Determine max length\n",
    "max_length = max([s.size(1) for s in train_audio_data])\n",
    "\n",
    "# Modify your DataLoader to pad sequences\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_data, text_data, max_length):\n",
    "        self.audio_data = audio_data\n",
    "        self.text_data = text_data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel_spectrogram = self.audio_data[idx]\n",
    "        text = self.text_data[idx]\n",
    "        mel_spectrogram = pad_sequence([mel_spectrogram], self.max_length)[0]  # Pad the spectrogram\n",
    "        return mel_spectrogram, text\n",
    "\n",
    "train_dataset = CustomDataset(train_audio_data, train_text_data, max_length)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 128, 161] at entry 0 and [1, 128, 274] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)  \u001b[38;5;66;03m# Define optimizer\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmel_spectrograms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_spectrograms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codec/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/codec/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/codec/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 43\u001b[0m, in \u001b[0;36mCollateFn.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent tensor sizes after padding.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Stack the list of padded tensors into a single tensor\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m mel_spectrograms \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_mel_spectrograms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_spectrograms, texts\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 128, 161] at entry 0 and [1, 128, 274] at entry 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Define a custom collate function for dynamic padding\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CollateFn:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        mel_spectrograms, texts = zip(*batch)\n",
    "\n",
    "        # Ensure all mel spectrograms have the same number of features\n",
    "        num_features = mel_spectrograms[0].size(2)  # Get number of features from the first mel spectrogram\n",
    "\n",
    "        # Find the maximum length in the batch\n",
    "        max_length = max(mel.size(1) for mel in mel_spectrograms)\n",
    "\n",
    "        # Pad mel spectrograms\n",
    "        padded_mel_spectrograms = []\n",
    "        for mel in mel_spectrograms:\n",
    "            current_length = mel.size(1)\n",
    "            if current_length < max_length:\n",
    "                # Create a tensor of zeros with the desired shape\n",
    "                padded_mel = torch.zeros((mel.size(0), max_length, num_features))\n",
    "                # Copy the original mel spectrogram into the padded tensor\n",
    "                padded_mel[:, :current_length, :] = mel\n",
    "            else:\n",
    "                # Use original tensor if itâ€™s already at max_length\n",
    "                padded_mel = mel\n",
    "            \n",
    "            padded_mel_spectrograms.append(padded_mel)\n",
    "        \n",
    "        # Ensure all tensors in padded_mel_spectrograms are of the same shape before stacking\n",
    "        if not all(padded_mel.size(1) == max_length for padded_mel in padded_mel_spectrograms):\n",
    "            raise RuntimeError(\"Inconsistent tensor sizes after padding.\")\n",
    "\n",
    "        # Stack the list of padded tensors into a single tensor\n",
    "        mel_spectrograms = torch.stack(padded_mel_spectrograms)\n",
    "        \n",
    "        return mel_spectrograms, texts\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for mel_spectrograms, texts in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(mel_spectrograms)\n",
    "            \n",
    "            # Ensure the outputs and targets are the same shape\n",
    "            targets = mel_spectrograms\n",
    "            if outputs.shape != targets.shape:\n",
    "                # Truncate or pad the outputs to match the targets (for this example)\n",
    "                outputs = outputs[:, :targets.size(1), :]\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataLoader with the custom collate function\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=CollateFn())\n",
    "\n",
    "# Example model, criterion, and optimizer setup\n",
    "model = SimpleTTSModel()  # Define your model\n",
    "criterion = nn.MSELoss()  # Define loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Define optimizer\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     total_correct = 0\n",
    "#     total_samples = 0\n",
    "#     with torch.no_grad():\n",
    "#         for mel_spectrograms, transcripts in test_loader:\n",
    "#             outputs = model(mel_spectrograms)\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total_samples += transcripts.size(0)\n",
    "#             total_correct += (predicted == transcripts).sum().item()\n",
    "    \n",
    "#     accuracy = total_correct / total_samples\n",
    "#     print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchaudio.transforms as T\n",
    "\n",
    "# def generate_speech(model, text):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Simple example, assume `text` has been preprocessed to a suitable tensor\n",
    "#         mel_spectrogram = model(text)\n",
    "#         # Convert mel spectrogram back to waveform (placeholder for inverse operation)\n",
    "#         waveform = T.InverseMelScale(n_stft=512)(mel_spectrogram)\n",
    "#         return waveform\n",
    "\n",
    "# # Generate speech for a new text example\n",
    "# text_example = torch.randn(1, 50, 80)  # Replace with actual text preprocessing\n",
    "# generated_waveform = generate_speech(model, text_example)\n",
    "# torchaudio.save('generated_speech.wav', generated_waveform, sample_rate=22050)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
